<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Open Data Hub RSS Feed]]></title><description><![CDATA[Open Data Hub RSS Feed]]></description><link>https://www.opendatahub.io</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 22 May 2023 13:49:38 GMT</lastBuildDate><item><title><![CDATA[Additional Resources]]></title><description><![CDATA[Community Slack  - Community discussions for the Open Data Hub Special Interest Groups and Working Groups occur in the odh-io workspace…]]></description><link>https://www.opendatahub.io/docs/additional/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/additional/</guid><content:encoded>&lt;h2&gt;&lt;a href=&quot;/community.html&quot;&gt;Community&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://odh-io.slack.com&quot;&gt;Slack&lt;/a&gt;  - Community discussions for the Open Data Hub Special Interest Groups and Working Groups occur in the &lt;a href=&quot;https://odh-io.slack.com&quot;&gt;odh-io&lt;/a&gt; workspace.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://opendatahub.io/community.html&quot;&gt;Meetings&lt;/a&gt; -- Community meetings for Open Data Hub are conducted regularly.  Get the meeting information and find out more on the Open Data Hub Community Repo.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/opendatahub-io&quot;&gt;GitHub&lt;/a&gt; -- All Open Data Hub projects are open source.  Browse the source code.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lists.opendatahub.io/admin/lists/&quot;&gt;Mailing List&lt;/a&gt; -- Stay up to date with the latest announcements and discussion about the Open Data Hub.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Videos&lt;/h2&gt;
&lt;p&gt;Additional videos will be available on the OpenShift youtube channel &lt;a class=&quot;external-link&quot; href=&quot;https://www.youtube.com/playlist?list=PLaR6Rq6Z4Iqcg2znnClv-xbj93Q_wcY8L&quot; target=&quot;_blank&quot;&gt;&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt;AI/ML on OpenShift &lt;/a&gt; playlist.&lt;/p&gt;
&lt;h5&gt;Tutorials&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/d6X1xvDXewM&quot;&gt;Uploading data to Ceph via command line&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Presentations&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MD1x2IT7rdg&quot;&gt;AI on OpenShift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/IcQ2bhsw_kQ&quot;&gt;Fraud Detection using the Open Data Hub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Conference Talks&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0HIelZ3qMLE&quot;&gt;Kubecon 2020: How to Use Kubernetes to Build a Data Lake for AI Workloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=9XhbXtPKttM&amp;#x26;feature=youtu.be&quot;&gt;Kubecon 2020: Is There a Place For Distributed Storage For AI/ML on Kubernetes?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/NZOky2Gm0iA?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;ML Pipelines with Kubeflow, Argo and Open Data Hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/og_Abr9jZJU?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;Scalable Kafka Deployment on OpenShift for ML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/WgEKfAj7PLc?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;MLFlow: Experiment Tracking on OpenShift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/dkuTaxWUrfE?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;Scaling your Open Data Hub for Fun and Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/tpDV8nUv45c?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;An Introduction to Unsupervised Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/Dt81qwza-zA?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;Unsupervised NLP for Log Anomaly Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/2QJ367chSS0?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;Sentiment Analysis Service in a DevOps Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/K8G_0z5jbcA?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;Machine Learning with Open Source Infrastructure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/5lT-GajT_Wo?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;AIOps: Anomaly Detection with Prometheus and Istio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/73VZaP3Mh-M?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;Presto: Cloud Native SQL-on-Anything&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/KWDUkm1ZeKY?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln&quot;&gt;Data Science in the Open Cloud Exchange Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=n2IW3VIZmg4&quot;&gt;Ceph Object Storage for AI and ML Workloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=by0l3b55i7g&quot;&gt;Data Exploration with JupyterHub on OpenShift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=B6E7SyxOB2M&quot;&gt;Building AI with Ceph and OpenShift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=iUJ6RGfY0JQ&quot;&gt;Using the Massachusetts Open Cloud Data Hub to perform Data Science Experiments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/CZwUCgkKIc4&quot;&gt;Using the Mass Open Cloud to perform Data Science Experiments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=RbJurxB4RSo&amp;#x26;feature=youtu.be&quot;&gt;ML Workloads with GPUs on Openshift 4&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Audio&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://grhpodcasts.s3.amazonaws.com/opendatahub1908.mp3&quot;&gt;Innovate @Open podcast&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Architecture]]></title><description><![CDATA[High Level Architecture A complete end-to-end AI platform requires services for each step of the AI workflow. In general, an AI workflow…]]></description><link>https://www.opendatahub.io/docs/architecture/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/architecture/</guid><content:encoded>&lt;h2&gt;High Level Architecture&lt;/h2&gt;
&lt;p&gt;A complete end-to-end AI platform requires services for each step of the AI workflow. In general, an AI workflow includes most of the steps shown in Figure 1  and is used by multiple AI engineering personas such as Data Engineers, Data Scientists and DevOps.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1: AI Workflow&lt;/strong&gt;
&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/feb9f3bca92955f3b7cfcf25ccdd8742/7a3d6/figure-1.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 36.50000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAACA0lEQVR42gXB+UtTAQDA8fcP9WMGERQWlTSGHYZhYWCkYCiGCmqLjdJMTSNU8ljm5pTmPLaotjxS5/aeV7rTyaZrRxgzfTseZQR9+3yERmOS+qE4jYYENfo4xR173Onao3E4zmNTAt1ogpIXe4wtpiDeSsZTiRKoRfE+QNkoQvFWoPgqUbbKOI4NINTqw9jFHaxLQUbnItx7FaFOH0Hy7bIZCrMT3UVritFv2+RPuJVoYJFowMn3kB3ZryEemCQWeM83n4W0pwbh4WCKpuGfPDIcUN69zxVdkuLnP2h6u49mOIluJMmN1hQDtiD/PCoO3SrSUiFpSUXGdQ7ZfQnZr+NIuk5uvQhhyGlBZ+2hy2Gk026i025EO92NdmqQpzYDzTYjWqseMfiB4zU1h4t5yMtnkJdOkVk4QXY5j4yvgYx4kdyaGqFm4iqV5stMr+qZ2RhnYu01LY5yTK52JP8nxMBHLKv9OLZ6ySS/EvJ+YdszS2zbQdpbTzI4SSLsJB6aRfY2IFSZiygbKaBirJCeuWb65tvpmX2GWRqib6ENzfR9qswljC5r+OuvJiWWcCCVcrRyi7TzJLIrH9ldwJHzLNkVNULv0hNezmtospXR9rmOyRUDZvcb3kkDNFjvUjd1m2rLTWa2x2G3hdz6NX5tlaKsq8mK+eTE8+TEC2Rdp/kdaec/dBqoSpjbWeQAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;AI Workflow&quot;
        title=&quot;&quot;
        src=&quot;/static/feb9f3bca92955f3b7cfcf25ccdd8742/5a190/figure-1.png&quot;
        srcset=&quot;/static/feb9f3bca92955f3b7cfcf25ccdd8742/772e8/figure-1.png 200w,
/static/feb9f3bca92955f3b7cfcf25ccdd8742/e17e5/figure-1.png 400w,
/static/feb9f3bca92955f3b7cfcf25ccdd8742/5a190/figure-1.png 800w,
/static/feb9f3bca92955f3b7cfcf25ccdd8742/7a3d6/figure-1.png 990w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The first phase of an AI workflow is initiated by Data Engineers that acquire the data from different sources and perform the required transformations. Data Engineers are also responsible to store and provide access to the transformed data to Data Scientist or Data Analysts that work on the second phase in the AI workflow. In the second phase, Data Scientists perform analysis on the transformed data and create the appropriate ML models. Once the models are trained and validated accordingly, they are ready to be served on the production platform in the last phase of the AI end-to-end workflow. In the production phase, ML models are served as services on the cluster and DevOps engineers are tasked with constantly monitoring and optimizing the services. The process does not end at this last step, Data Scientists should monitor and keep validating models based on incoming and trained data sets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Red Hat® OpenShift®  Container Platform&lt;/strong&gt;  is the leading Kubernetes based container platform providing multiple functionalities for successfully running distributed AI workloads. Functionalities such as high availability and self-healing, scaling, security, resource management and operator framework are essential to successfully providing AI/ML services. OpenShift also also supports specialized hardware such as GPUs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Data Hub&lt;/strong&gt;(ODH) currently provides services on OpenShift for AI data services such as data storage and ingestion/transformation. For data storage and availability, ODH integrates seamlessly with Object Storage provider (&lt;a href=&quot;https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation&quot;&gt;OpenShift Data Foundation&lt;/a&gt;, &lt;a href=&quot;https://min.io&quot;&gt;Minio&lt;/a&gt;, AWS S3, ...) that exists in the &lt;a href=&quot;https://www.redhat.com/en/topics/cloud-computing/what-is-hybrid-cloud&quot;&gt;Hybrid Cloud&lt;/a&gt; or on-premise.&lt;/p&gt;
&lt;p&gt;Open Data Hub also provides services for model creation, training and validation. For the Data Scientist development environment, ODH provides Jupyter Notebook images running natively distributed on OpenShift. ODH roadmap includes tools for monitoring services as discussed in the section below. These tools will include the ability for natively monitoring AI services and served models on OpenShift using Prometheus and Grafana.&lt;/p&gt;
&lt;h2&gt;Open Data Hub Platform&lt;/h2&gt;
&lt;p&gt;Open Data Hub platform is a centralized self-service solution for analytic and data science distributed workloads. It is a collection of open source tools and services natively running on OpenShift.&lt;/p&gt;
&lt;h3&gt;Components and Considerations&lt;/h3&gt;
&lt;h4&gt;End-to-End Considerations&lt;/h4&gt;
&lt;p&gt;ODH project’s main goal is to provide an open source end-to-end AI platform on OpenShift Container Platform that is equipped to run large AI/ML distributed workloads. As discussed earlier an end-to-end AI platform includes all phases of AI processing starting from data ingestion all the way to production AI/ML hosting and monitoring. There are multiple user personas for this platform that work on different phases.&lt;/p&gt;
&lt;p&gt;All the tools and components listed below are currently being used as part of Red Hat’s internal ODH platform cluster. This internal cluster is utilized by multiple internal teams of data scientists running AI/ML workloads for functions such as Anomaly Detection and Natural Language Processing. A subset of these components and tools are included in the ODH release available today and the rest are scheduled to be integrated in future releases as described in the roadmap section below. Support for each component is provided by the source entity, for example Red Hat supports Red Hat components such as OpenShift Container Platform and Ceph while open source communities support Seldon, Jupyterhub, Prometheus and so on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data in Motion&lt;/strong&gt; is essential in today&apos;s enterprise backend networks where data resides in multiple locations, especially to support data stored in legacy systems. Hybrid Cloud architectures also require sharing data between different cloud systems. Tools such as Red Hat AMQ Streams, Kafka and Logstash provide robust and scalable data transfer capabilities native to the OpenShift platform. Data Engineers can use these tools to transfer required data from multiple sources.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Storage: Data Lake/Databases/In-Memory&lt;/strong&gt; includes  tools for distributed file, block and object storage at scale. We include tools for both relational databases and document-oriented databases. Big data storage requires the freedom of no schema constraints while data access requires some form of ordered schema definition. Data ingestion can be easily performed using Red Hat Data Grid into distributed object storage provided by Ceph. High performance in-memory datastore solutions such as Red Hat Data Grid which is based on Infinispan are essential for fast data access needed for analysis or model training.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Metadata Management&lt;/strong&gt; tools basically add informational metadata to the stored data such as databases, tables, columns, partitions, schemas and location. Currently, we have investigated Hive Metastore as a solution that provides an SQL interface to access the metadata information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Analysis: Big Data Processing&lt;/strong&gt; tools are needed for running large distributed AI workloads. Apache Spark™is installed as an operator on OCP providing cluster wide custom resource to launch distributed AI workloads on distributed spark clusters. These spark clusters are not shared among users, they are specific to each user providing isolation of resource usage and management. Spark clusters are also ephemeral and are deleted once the user shuts down the notebook providing efficient resource management.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Analysis: Data Exploration&lt;/strong&gt; tools provide the  query and visualization functions for data scientists to perform initial exploration of the data. Hue provides an SQL interface to query the data and basic visualization. Kibana is also a data visualization tool for Elasticsearch indexed data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Analysis: Streaming&lt;/strong&gt; tools such as Kafka and Elasticsearch allow for distributed and scalable message distribution native to OpenShift.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artificial Intelligence and Machine Learning: Model Lifecycle&lt;/strong&gt; tools provide functionalities to serve the model and collect essential metrics needed to monitor the lifecycle of the model. It allows constant evaluation of model performance which can lead to the need for retraining or re-validation. &lt;a href=&quot;https://www.seldon.io/&quot;&gt;Seldon&lt;/a&gt; is a tool that provides model hosting and metric collection from both the model itself and the component serving the model. MLflow provides parameter tracking for models and deployment functionalities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artificial Intelligence and Machine Learning: Interactive Notebooks&lt;/strong&gt; provide a development workspace for data scientists and business analysts to conduct their analysis work. &lt;a href=&quot;https://jupyter.org/hub&quot;&gt;JupyterHub&lt;/a&gt; is a tool that provides a multi-user notebook environment that allows users to use notebooks running in their own workspace. This allows for resource management isolation. Hue is also a multiuser data analysis platform that allows querying and plotting of data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Artificial Intelligence and Machine Learning: Business Intelligence&lt;/strong&gt; tools such as &lt;a href=&quot;https://superset.incubator.apache.org/&quot;&gt;Apache Superset&lt;/a&gt; provide a rich set of data visualization tools and come enterprise-ready with authentication, multi-user and security integrated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Security and Governance&lt;/strong&gt; include tools for providing services, data and API security and governance. Data in storage and in motion require security for both access and encryption. The Ceph Object Gateway provides encryption of uploaded objects and options for the management of encryption keys. The Ceph Object Gateway stores that data in the Ceph Storage Cluster in encrypted form. Red Hat Single Sign-On (&lt;a href=&quot;https://www.keycloak.org/&quot;&gt;Keycloak&lt;/a&gt;) and OpenShift provide user authentication while Red Hat 3Scale provides an API gateway for REST Interfaces.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Monitoring and Orchestration&lt;/strong&gt; provide tools for monitoring all aspects of the end-to-end AI platform. This includes but is not limited to data, messaging, API, resources availability and utilization, etc. Prometheus and Grafana offer an interface for collecting and displaying metrics. For orchestration tools we included Jenkins and Argo Workflows that provide the functionality to create and manage workflows for build and release automation. Argo is OpenShift native workflow tools that can run pods in a directed acyclic graph (DAG) workflow.&lt;/p&gt;
&lt;h4&gt;Current Included Components&lt;/h4&gt;
&lt;p&gt;The ODH platform is installed on OpenShift as a native operator and is available on the OperatorHub.io. The operator framework (&lt;a href=&quot;https://operatorhub.io/getting-started&quot;&gt;https://operatorhub.io/getting-started&lt;/a&gt;) is an open source toolkit that provides effective, scalable and automated native application management. Operators manage custom resources that provide specific cluster wide functionalities. The ODH operator manages the ODH platform AI/ML services cluster-wide. Some of the components within the ODH platform are also operators such as Apache Spark™. Currently ,when installing the ODH operator it includes the following components: Ceph, Apache Spark, Jupyterhub, Prometheus and Grafana.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jupyter Notebook w/ JupyterLab UI&lt;/strong&gt; (&lt;a href=&quot;https://jupyter.org&quot;&gt;https://jupyter.org/hub&lt;/a&gt;) is an open source notebook platform that ODH provides with multiple notebook image streams that incorporate embedded AI/ML libraries. Jupyter Notebooks also provide many features such as for data scientists allowing them to run notebooks in their own workspaces in a multi-tenant environment. Authentication can also be customized as a pluggable component to support authentication protocols such as OAuth. Data scientists can use familiar tools such as Jupyter notebooks for developing complex algorithms and models. Frameworks such as Tensorflow, PyTorch and more are available for use.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prometheus&lt;/strong&gt; (&lt;a href=&quot;https://prometheus.io/&quot;&gt;https://prometheus.io/&lt;/a&gt;) is an open source monitoring and alerting tool that is widely adopted across many enterprises. Prometheus can be configured to monitor targets by scraping or pulling metrics from the target’s HTTP endpoint and storing the metric name and a set of key-value pairs in a time series database. For graphing or querying this data, Prometheus provides a web portal with rudimentary options to list and graph the data. It also provides an endpoint for more powerful visualization tools such as Grafana to query the data and create graphs. An Alert Manager is also available to create alert rules to produce alerts on specific metric conditions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Grafana&lt;/strong&gt; (&lt;a href=&quot;https://grafana.com/&quot;&gt;https://grafana.com/&lt;/a&gt;) is an open source tool for data visualization and monitoring. Data sources such as Prometheus can be added to Grafana for metrics collection. Users create Dashboards that include comprehensive  graphs or plots of specific metrics. It includes powerful visualization capabilities for graphs, tables, and heatmaps. Ready-made dashboards for different data types and sources are also available giving Grafana users a head start. It also has support for a wide variety of plugins so that users can incorporate community-powered visualisation tools for things such as scatter plots or pie charts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Seldon&lt;/strong&gt; (&lt;a href=&quot;https://www.seldon.io&quot;&gt;https://www.seldon.io&lt;/a&gt;)  is an open source framework that makes it easier to deploy AI/ML models on Kubernetes and OpenShift. The model can be created and trained using many tools such as Apache Spark, scikit-learn and TensorFlow. Seldon also provides metric for Prometheus scraping. Metrics can be custom model metrics or Seldon core system metrics.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Release Notes]]></title><description><![CDATA[Open Data Hub version v1.4.1 - January 2023 Technology Version Category Open Data Hub Dashboard v2.5.2 Dashboard Open Data Hub Notebook…]]></description><link>https://www.opendatahub.io/docs/release-notes/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/release-notes/</guid><content:encoded>&lt;h3&gt;Open Data Hub version v1.4.1 - January 2023&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.5.2&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/notebooks&quot;&gt;Open Data Hub Notebook Images&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;Jupyter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/modelmesh-serving&quot;&gt;Model Mesh Serving&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.9.0&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/data-science-pipelines&quot;&gt;Data Science Pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;Data Science Tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v1.4.0 - October 2022&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.2.1&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/modelmesh-serving&quot;&gt;Model Mesh Serving&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.9.0&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/data-science-pipelines&quot;&gt;Data Science Pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;Data Science Tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v1.3.0 - July 2022&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.0&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra Notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;3.9.1&lt;/td&gt;
&lt;td&gt;Data Science Tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Support for Kubeflow 1.5 on &lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/manifests/v1.5-branch-openshift/openshift/kfdef/kfctl_openshift_v1.5.0_odh.yaml&quot;&gt;OpenShift&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v1.2.0 - April 2022&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://elyra.readthedocs.io/en/latest/getting_started/changelog.html#release-3-6-0-02-08-2022&quot;&gt;Elyra Notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;3.6.0&lt;/td&gt;
&lt;td&gt;Data Science Tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/apache/superset/blob/1.4.1/CHANGELOG.md&quot;&gt;Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.4.1&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service Mesh with Kubeflow v1.4 on OpenShift (&lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/manifests/v1.4.0-rc.2-openshift/openshift/kfdef/kfctl_openshift_v1.4.0_servicemesh.yaml&quot;&gt;kfdef&lt;/a&gt;)&lt;/td&gt;
&lt;td&gt;1.4&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v1.1.2 - February 2022&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Support for Kubeflow 1.4 on &lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/manifests/v1.4.0-rc.2-openshift/openshift/kfdef/kfctl_openshift_v1.4.0_odh.yaml&quot;&gt;OpenShift&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.4&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Update addtional components affected by the deprecation of kubernetes apis in OpenShift 4.9 (k8s v1.22)&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v1.1.1 - January 2022&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Update components affected by the deprecation of kubernetes apis in OpenShift 4.9 (k8s v1.22)&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v1.1.0 - July 2021&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Support for Kubeflow 1.3 on &lt;a href=&quot;https://raw.githubusercontent.com/kubeflow/manifests/v1.3-branch/distributions/kfdef/kfctl_openshift_v1.3.0.yaml&quot;&gt;OpenShift&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.3&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v1.0.0 - February 2021&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Support for Kubeflow 1.2 on &lt;a href=&quot;https://github.com/kubeflow/manifests/blob/v1.2-branch/kfdef/kfctl_openshift.v1.2.0.yaml&quot;&gt;OpenShift&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.2&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.9.0 - November 2020&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Support for Kubeflow on &lt;a href=&quot;https://github.com/opendatahub-io/manifests/tree/v1.0-branch-openshift&quot;&gt;OpenShift&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.9&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra Notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.0.5&lt;/td&gt;
&lt;td&gt;Data Science Tools&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard&quot;&gt;Ceph Nano Object Storage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.7&lt;/td&gt;
&lt;td&gt;Object Storage&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.8.0 - September 2020&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Support for Kubeflow on &lt;a href=&quot;https://github.com/opendatahub-io/manifests/tree/v1.0-branch-openshift&quot;&gt;OpenShift&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra Notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.0.2&lt;/td&gt;
&lt;td&gt;Data Science Tools&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.7.0 - July 2020&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Support for Kubeflow on &lt;a href=&quot;https://github.com/opendatahub-io/manifests/tree/v1.0-branch-openshift&quot;&gt;OpenShift&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;AI/ML&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.2.0&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.6.1 - May 2020&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.1.0&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.6.0 - May 2020 (redesign on top of Kubeflow)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Alpha&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.0.0&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.17.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.7.0&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.32.0&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio&quot;&gt;Radanalytics Spark Operator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.0.7&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.5.1 - February 2020&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;JupyterHub&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Update kubespawner to version 0.11.1 and enable user customization of the jupyterhub config&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Radanalytics &lt;a href=&quot;https://github.com/radanalyticsio/spark-operator&quot;&gt;Spark Operator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.0.5&lt;/td&gt;
&lt;td&gt;Adds support for customizing the Spark cluster resource requests and limits for cpu/memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Apache &lt;a href=&quot;https://github.com/apache/incubator-superset&quot;&gt;Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.34.0&lt;/td&gt;
&lt;td&gt;Resolve issues related to connecting to the Data Catalog Thrift Server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.5.0 - December 2019&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;JupyterHub CUDA GPU Images and Notebooks&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Support for building CUDA GPU Images and GPU Notebook&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Apache &lt;a href=&quot;https://github.com/apache/incubator-superset&quot;&gt;Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Data Catalog (&lt;a href=&quot;https://gethue.com/&quot;&gt;Hue&lt;/a&gt;, &lt;a href=&quot;https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html&quot;&gt;Spark Thrift Server&lt;/a&gt;, Hive Metastore)&lt;/td&gt;
&lt;td&gt;Hue 4.4.1 &amp;#x26; Spark 2.4.3 &amp;#x26; Spark Thrift Server 2.4 &amp;#x26; Hive Metastore 1.2.1&lt;/td&gt;
&lt;td&gt;Deployment of Hue, Spark Thrift Server and Hive Metastore to simplify querying data lakes using Spark SQL language&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://argoproj.github.io/argo/&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.4.2&lt;/td&gt;
&lt;td&gt;Container native workflow engine&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.4.0 - September 2019&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://argoproj.github.io/argo/&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.3.0&lt;/td&gt;
&lt;td&gt;Container native workflow engine&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://strimzi.io/&quot;&gt;Strimzi Kafka Operator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.11.1&lt;/td&gt;
&lt;td&gt;Distributed streaming platform&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Open Data Hub AI-Library&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;Machine learning as a service&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.3.0 - June 2019&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Seldon&lt;/td&gt;
&lt;td&gt;0.2.7&lt;/td&gt;
&lt;td&gt;Model Serving and Metrics Tool&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;JupyterHub with GPU Support&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Apache Spark&lt;/td&gt;
&lt;td&gt;2.2.3&lt;/td&gt;
&lt;td&gt;Query &amp;#x26; ETL frameworks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TwoSigma BeakerX Integration&lt;/td&gt;
&lt;td&gt;1.4.0&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prometheus&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;System monitoring tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Grafana&lt;/td&gt;
&lt;td&gt;4.7&lt;/td&gt;
&lt;td&gt;System monitoring tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Open Data Hub version v0.2.0 - May 2019&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;JupyterHub with GPU Support&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Apache Spark&lt;/td&gt;
&lt;td&gt;2.2.3&lt;/td&gt;
&lt;td&gt;Query &amp;#x26; ETL frameworks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TwoSigma BeakerX Integration&lt;/td&gt;
&lt;td&gt;1.4.0&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;{:class=&quot;table table-bordered&quot;}&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content:encoded></item><item><title><![CDATA[Tiered Components]]></title><description><![CDATA[Open Data Hub Components As the AI/ML landscape has matured, Open Data Hub is refocusing on providing an opinionated deployment of…]]></description><link>https://www.opendatahub.io/docs/tiered-components/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/tiered-components/</guid><content:encoded>&lt;h2&gt;Open Data Hub Components&lt;/h2&gt;
&lt;p&gt;As the AI/ML landscape has matured, Open Data Hub is refocusing on providing an opinionated deployment of components to focus on providing a core part of the AI/ML workflow while also supporting an open community model where ODH maintainers will officially support a well defined integrated ODH Core functionality with integration points for additional components that have a designated maintainers and clearly defined levels of support.&lt;/p&gt;
&lt;p&gt;Under the new model ODH will support a Tiered component structure that clearly defines the requirements for a component integrating with Open Data Hub.&lt;/p&gt;
&lt;h3&gt;Tiered Architecture&lt;/h3&gt;
&lt;p&gt;All components/projects will be required to designate dedicated maintainers/owners that will be responsible for meeting the requirements or support of the respective tiers&lt;/p&gt;
&lt;h4&gt;ODH Core (Tier 0) - opendatahub-io&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Available under &lt;a href=&quot;https://github.com/opendatahub-io&quot;&gt;https://github.com/opendatahub-io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;These are integrated components that will provide functionality that the ODH Community decides is a core part of the Data Science workflow with an opinionated deployment that provides a seamless user experience.&lt;/li&gt;
&lt;li&gt;These components will be included as part of the downstream offering provided by Red Hat&lt;/li&gt;
&lt;li&gt;Requirements:
&lt;ul&gt;
&lt;li&gt;Long term support by ODH Maintainers for the life of the component&lt;/li&gt;
&lt;li&gt;Regular updates&lt;/li&gt;
&lt;li&gt;Full testsuite - Unit, Functional, Integration, Regression&lt;/li&gt;
&lt;li&gt;Security scanning for all container images&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Component list based on components deployed downstream
&lt;ul&gt;
&lt;li&gt;Open Data Hub Operator&lt;/li&gt;
&lt;li&gt;Notebook Controller&lt;/li&gt;
&lt;li&gt;Default ODH notebook images&lt;/li&gt;
&lt;li&gt;ODH Dashboard&lt;/li&gt;
&lt;li&gt;Data Science Pipelines&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://opendatahub.io&quot;&gt;https://opendatahub.io&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;ODH Incubating (Tier 1) - opendatahub-io&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Available under &lt;a href=&quot;https://github.com/opendatahub-io&quot;&gt;https://github.com/opendatahub-io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;These components have the same requirements as ODH Core components but they will not be included as part of the default core deployment.  The focus of Tier 1 is to incubate projects/features that will be owned by the Open Data Hub community and a candidate for ODH Core OR provide advanced functionality that is of strong interest to the community at large. These incubated features could&lt;/li&gt;
&lt;li&gt;Requirements:
&lt;ul&gt;
&lt;li&gt;Same requirements as ODH Core components so that upon promotion no additional changes are required.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;ODH Contrib (Tier 2) - opendatahub-io-contrib&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Available under &lt;a href=&quot;https://github.com/opendatahub-io-contrib&quot;&gt;https://github.com/opendatahub-io-contrib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;These are components that want to integrate with Open Data Hub but are not candidates to provide ODH Core functionality –OR– the maintainer is unable to commit to the requirements of an ODH Incubating component to provide long term support&lt;/li&gt;
&lt;li&gt;Requirements:
&lt;ul&gt;
&lt;li&gt;A basic testsuite will be required as part of Tier 2 acceptance as part of a simple smoke test to verify that the application is still compatible with each ODH release.  Components that fail to validate successfully after &lt;X&gt; number of ODH releases with no updates/fixes from the designated maintainer(s) will be archived/deprecated&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Proposed List
&lt;ul&gt;
&lt;li&gt;Apache Spark on OpenShift&lt;/li&gt;
&lt;li&gt;Apache Airflow on OpenShift&lt;/li&gt;
&lt;li&gt;Ray.io&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Data Exploration]]></title><description><![CDATA[Pre-requisites This tutorial requires that you followed the basic tutorial and added the following components to your ODH deployment using…]]></description><link>https://www.opendatahub.io/docs/advanced-tutorials/data-exploration/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/advanced-tutorials/data-exploration/</guid><content:encoded>&lt;h2&gt;Pre-requisites&lt;/h2&gt;
&lt;p&gt;This tutorial requires that you followed the &lt;a href=&quot;/docs/getting-started/basic-tutorial&quot;&gt;basic tutorial&lt;/a&gt; and added the following components to your ODH deployment using the KfDef shown below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spark-operator&lt;/li&gt;
&lt;li&gt;hue&lt;/li&gt;
&lt;li&gt;thrift-server&lt;/li&gt;
&lt;li&gt;ceph-nano&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;# ODH uses the KfDef manifest format to specify what components will be included in the deployment
apiVersion: kfdef.apps.kubeflow.org/v1
kind: KfDef
metadata:
  name: opendatahub
spec:
  applications:
    - kustomizeConfig:
        repoRef:
          name: manifests
          path: odh-common
      name: odh-common
    # Create the SecurityContextConstraint to grant the ceph-nano service account anyuid permissions
    - kustomizeConfig:
        repoRef:
          name: manifests
          path: ceph/object-storage/scc
      name: ceph-nano-scc
    # Deploy ceph-nano for minimal object storage running in a pod
    - kustomizeConfig:
        repoRef:
          name: manifests
          path: ceph/object-storage/nano
      name: ceph-nano
    # Deploy Radanalytics Spark Operator
    - kustomizeConfig:
        repoRef:
          name: manifests
          path: radanalyticsio/spark/cluster
      name: radanalyticsio-spark-cluster
    # Deploy Open Data Hub JupyterHub
    - kustomizeConfig:
        parameters:
          - name: s3_endpoint_url
            value: s3.odh.com
        repoRef:
          name: manifests
          path: jupyterhub/jupyterhub
      name: jupyterhub
    # Deploy addtional Open Data Hub Jupyter notebooks
    - kustomizeConfig:
        overlays:
          - additional
        repoRef:
          name: manifests
          path: jupyterhub/notebook-images
      name: notebook-images
    # Deploy Hue with configuration to access the ceph-nano object store
    - kustomizeConfig:
        # These parameters are required to allow access to object storage provided by ceph-nano
        parameters:
          - name: s3_is_secure
            value: &quot;false&quot;
          - name: s3_endpoint_url
            value: &quot;ceph-nano-0&quot;
          - name: s3_credentials_secret
            value: ceph-nano-credentials
        repoRef:
          name: manifests
          path: hue/hue
      name: hue
    # Deploy Thriftserver with configuration to access the ceph-nano object store
    - kustomizeConfig:
        overlays:
          - create-spark-cluster
        # These parameters are required to allow access to object storage provided by ceph-nano
        parameters:
          - name: spark_url
            value: &quot;spark://spark-cluster-thriftserver&quot;
          - name: s3_endpoint_url
            value: &quot;http://ceph-nano-0&quot;
          - name: s3_credentials_secret
            value: ceph-nano-credentials
        repoRef:
          name: manifests
          path: thriftserver/thriftserver
      name: thriftserver
  repos:
    - name: manifests
      uri: &apos;https://github.com/opendatahub-io/odh-manifests/tarball/v1.0.0&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All screenshots and instructions are from OpenShift 4.6.&lt;/p&gt;
&lt;h2&gt;Exploring Data Catalog&lt;/h2&gt;
&lt;p&gt;The Data Catalog is a set of components with which you can
read data stored in Data Lakes, create tables and query them in a SQL-like style. You can find
below a picture of the simplified architecture of Data Catalog:&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/60e9aa6c842cc77de50b3edd58e23da9/b97f6/architecture.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 55.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACW0lEQVR42p1TX2hSYRT/nBaK0AbRU616qFUjYlG9Loigl1aD4UuNWOjYiNXTeukPF4KGUdjYw2KlczTFde838yXCUbjKdIWJuZZuc05zbTpd6tXrnF7vieu0yYqgfvDj43c43/l+3zkchEogSVLIEwAE6xEQlljFq5e2KdkLm+fGe0+geyEXuc7T+S3SbfWEb74JMudDzfXt4ZbDR9DfQBDrxcbsvv0TM7H0VJgB8weaI+4D3FZyMO5muVAW4PNilnEO9J8GhITFixaCEM/29soCfX1y74O7BxECgWQr0yyRQAdC6QZ/0nPgUyCd9SUynAbT+S2iHCtAOXbAQOd9iTTr8Cfzdm94wx2t0ez4oVZH4RkJSW3/NYQsovpDjL/xJEBNTfwRwPPaiZk46wym4LUzymnwCqipGLxyxjhnkIaPc3HW7g5tFJzXasWzSuUZ/52ec5EnPfv4GCYbVLYJ8fzQUF1rBmDP5SuJXO3uHDTJkqw3ShemV5KFlos0u3PXWqFVQefXoOSQIIhin3QjI406jG+VH6Eo1XGMH2ssBBItrXr2yq+mU0dPAFyQ5zjPcgG8URbaOvNcwzGAS50pJglzdb+my58Gna6NIsmxcsHBwaenSJKyEgSxjdfjrmCTxb3UZfWGFfbp5fZ3XxY7rF/DCov7e9fbyYWzfxqqBCFUXaGlCKHtpfhm8L8Sof9FuS3YaFTi0VGGwpjGGNMUxgzGRlVlTiUEJf6my8lGo/GewWBY1ev1qeHh4ZRer8+YTKaHxTUAqPpnp/wWmc1mqcPhqObpcrmkm539BMXiWIb6C6vEAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Data Catalog Architecture&quot;
        title=&quot;Data Catalog Architecture&quot;
        src=&quot;/static/60e9aa6c842cc77de50b3edd58e23da9/5a190/architecture.png&quot;
        srcset=&quot;/static/60e9aa6c842cc77de50b3edd58e23da9/772e8/architecture.png 200w,
/static/60e9aa6c842cc77de50b3edd58e23da9/e17e5/architecture.png 400w,
/static/60e9aa6c842cc77de50b3edd58e23da9/5a190/architecture.png 800w,
/static/60e9aa6c842cc77de50b3edd58e23da9/b97f6/architecture.png 958w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These are the components that are part of Data Catalog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark SQL Thrift server to enable an endpoint where clients can connect using an ODBC/JDBC connection&lt;/li&gt;
&lt;li&gt;Cloudera Hue as a Data Exploration tool to explore the Data Lake, create tables and query them. You can&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;also create dashboards using the tables managed by Hive Metastore&lt;/p&gt;
&lt;h2&gt;Using Data Catalog&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Find the route to Hue. Within your Open Data Hub Project click on Networking -&gt; Routes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;../assets/img/pages/docs/data-catalog/routes.png&quot; alt=&quot;OpenShift routes&quot; title=&quot;OpenShift routes&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For the route named hue, click on the location to bring up Hue (typically &lt;code&gt;http://hue-project.apps.your-cluster.your-domain.com&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It will open the first-time login page where you can create the superuser for Hue.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/c2ed6ab54f47ebc1fcfb128454a676da/a6ec4/hue-user-creation.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 66%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABAUlEQVR42p2SwUoEMQyG9433FQSfSL2KN8HTeHCXxZOrosi4zsw6M02TtP2lUREUduwUQtKQfM3fduG9xyGjbPTjp+oXUzBhjxgEKQqCsuVmA/PqiLF6bHD1sMNtvbcci4CI5gEbx6i2b7i8f8VdM1hO5gA5S01Az4qGBC2JxRoTVLUMSAZka962I25eOlw/d9jUPXoOiEHhyidkpJQQVBFjRIjJ9tnKJ6RPIGvA7n1AO/ovI5CoHVIEzHIQFJt6j+VZhaPzFY4v1lieVjhZPwFIGJ0rk2z3KIqRGI4ZxIKBGF4EbCpmfBtrIjL/J57zsX8/0n/qJoHOOZso++/4EPADUs/wz2wi1fMAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Hue user creation&quot;
        title=&quot;Hue user creation&quot;
        src=&quot;/static/c2ed6ab54f47ebc1fcfb128454a676da/5a190/hue-user-creation.png&quot;
        srcset=&quot;/static/c2ed6ab54f47ebc1fcfb128454a676da/772e8/hue-user-creation.png 200w,
/static/c2ed6ab54f47ebc1fcfb128454a676da/e17e5/hue-user-creation.png 400w,
/static/c2ed6ab54f47ebc1fcfb128454a676da/5a190/hue-user-creation.png 800w,
/static/c2ed6ab54f47ebc1fcfb128454a676da/c1b63/hue-user-creation.png 1200w,
/static/c2ed6ab54f47ebc1fcfb128454a676da/a6ec4/hue-user-creation.png 1438w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As the first login, Hue will show a tutorial about the interface. You can skip the tutorial by closing the window.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/35fb29f1f58e40f088d20d960ffb0cd5/33c15/tutorial.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 53.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+ElEQVR42pWQsWoCQRCG9x1SxuZAAtqYQkECNunSp0gaizR5gjxCQDSCkNJGsAjkcUzORjCSNwh3atyZf0bczZ1nyIL5GZYddr5/ZtasLauq6E5E7IKImBlOUgwvV6siYr6SpU8gIAIxPOwdgDxln1sidjaqapartbNRiEDckXnnHYr3Ymo2G5uPnT8A4L2gAWVwZud3/dU8DNs9LCK+5/jltfM06A2eH7v94Whsf2okABfGBnB92y6Vq1GldnIaNVuXSZoeC4vITfsuqtTOGxdn9dbV/UOaJAH4cGev+cdi8vYex9NJPJ0tPgEc9WH/kvn+C8ahQvAWWhNvRombVgEAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Hue tutorial&quot;
        title=&quot;Hue tutorial&quot;
        src=&quot;/static/35fb29f1f58e40f088d20d960ffb0cd5/5a190/tutorial.png&quot;
        srcset=&quot;/static/35fb29f1f58e40f088d20d960ffb0cd5/772e8/tutorial.png 200w,
/static/35fb29f1f58e40f088d20d960ffb0cd5/e17e5/tutorial.png 400w,
/static/35fb29f1f58e40f088d20d960ffb0cd5/5a190/tutorial.png 800w,
/static/35fb29f1f58e40f088d20d960ffb0cd5/c1b63/tutorial.png 1200w,
/static/35fb29f1f58e40f088d20d960ffb0cd5/33c15/tutorial.png 1463w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Hue editor will appear in a blank textarea.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/9f66e80181b54a96e1ee02974ec10a9e/e4ba2/editor.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 44.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA0UlEQVR42qWOSUoEQRBF66Ai6NZNCyUuGlwV2itP4TG8kqhVkTEPUtnDQrAF/RBB/k+8yBg2L68WaR7mQWLvC8zQPmZYGjZiJCVWEkVWlPVB4qKuHmwx3D7s3j6XyvQIEV2QiRmgNSSAJiJIDA2ReG7ILL1zRFTVsNlOIpKZ7m7mahYRmblW1952VWaqWh013E/PVZXdxPq/18867DjBN9vdMV29qrIIIAKRrbfYOfh6nA5p92bOLMTs7vv8HHx193ga+jb6+9mX4z/gi/Hpz/AXmwYHp5ms4YEAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Hue editor&quot;
        title=&quot;Hue editor&quot;
        src=&quot;/static/9f66e80181b54a96e1ee02974ec10a9e/5a190/editor.png&quot;
        srcset=&quot;/static/9f66e80181b54a96e1ee02974ec10a9e/772e8/editor.png 200w,
/static/9f66e80181b54a96e1ee02974ec10a9e/e17e5/editor.png 400w,
/static/9f66e80181b54a96e1ee02974ec10a9e/5a190/editor.png 800w,
/static/9f66e80181b54a96e1ee02974ec10a9e/c1b63/editor.png 1200w,
/static/9f66e80181b54a96e1ee02974ec10a9e/e4ba2/editor.png 1468w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can create a table from a file inside the Data Lake.&lt;/p&gt;
&lt;h2&gt;Creating and querying tables&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Let&apos;s create first a database with the following command (You can run the query by either clicking on the play button in the left or type Ctrl+Enter):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE opendatahub;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;In the explorer, click on the refresh button. The new database will appear:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 291px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/01004fce214f56313030ad9154f5269d/78805/new-database.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 135.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAbCAYAAAB836/YAAAACXBIWXMAAAsTAAALEwEAmpwYAAABSklEQVR42u2T626CQBCFefO+Sh+kr+AvG5KCEWIQA8hlCwiI7GnOVIwlVsH0V8Mkw7KZ5dszF4yu6wCtobUG32UP/NiPcZ5vmgZGpzVe3t6x8BJkUYBMKTiOgzAML+B7xjjPpmmK4/H4DXxdOPgIFcpPhThJYFkWttvtKCDVEZYkyRl4TvH6xmdSptV1DaMoCqEvl0uYpgnf96GUGqVuKEJqSGAcx9jtdgiCAPv9Hnme4xkTIPNmzVzXxWazged5An5kTHO9XosA1rAXYpxOJ9lce5/yvVq2bSsgZng4HFBVlbjBQJZl4gwQ1t/IlYd68NCHNRQgFfKFMKbPldLpvJ2d+62z/JbeD7YAy7K81I4Dzebc6uIjvzSFjyiKJGVOPDtOVb2yyV2+VsiVnVutVrBtG4xNmUcB3vpDhkWfpBB/aDNwBs7A/wP8AvdFUK30Z8M3AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;New database created&quot;
        title=&quot;New database created&quot;
        src=&quot;/static/01004fce214f56313030ad9154f5269d/78805/new-database.png&quot;
        srcset=&quot;/static/01004fce214f56313030ad9154f5269d/772e8/new-database.png 200w,
/static/01004fce214f56313030ad9154f5269d/78805/new-database.png 291w&quot;
        sizes=&quot;(max-width: 291px) 100vw, 291px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;{:class=&quot;img-auto&quot;}&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Now let&apos;s select the database with the command:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;USE opendatahub;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;We will create a table from the &lt;code&gt;sample_data.csv&lt;/code&gt; file used in the &lt;code&gt;Basic Tutorial&lt;/code&gt; section:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;CREATE EXTERNAL TABLE opendatahub.sample(
    timestamp TIMESTAMP,
    name STRING,
    field STRING,
    primary_audience STRING,
    key_people STRING,
    outcome STRING,
    full_notes STRING,
    email STRING,
    sample_date DATE,
    notes STRING,
    lowlights STRING,
    learnings STRING,
    trip_region STRING,
    number_of_days INT,
    estimated_cost FLOAT,
    product_mix STRING
)
ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.OpenCSVSerde&apos;
WITH SERDEPROPERTIES (
&quot;separatorChar&quot; = &quot;,&quot;,
&quot;quoteChar&quot; = &quot;\&quot;&quot;,
&quot;escapeChar&quot; = &quot;\\&quot; 
)
TBLPROPERTIES(&quot;skip.header.line.count&quot;=&quot;1&quot;)
LOCATION &apos;s3a://&amp;#x3C;bucket-name&gt;/&amp;#x3C;csv-file-rootdir&gt;&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; The &lt;code&gt;LOCATION&lt;/code&gt; statement needs a path to the directory where the file is stored, not the file path.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You will see the result of table creation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/00f20b5be77fdba3a93ae7f210dbe103/4e814/table-creation.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 60.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABSklEQVR42qWTi2rkMAxF5/+/smx3OhMnthMnjh+Sc5Z4utBSWKZdwUVGNkfIki65Ctua2VwihkRKD7XW+JepKqUURIRaa38fQuCypoKZPL9f3nAmUItwHI3jOHjW/r5d15XLlgqj87y93ggu8hP7BAwxd+D19YofAhz/CVy2HTPOmJvBGY9IobWKNkHbeZbvAX2I/Lq+YM2NGANVd6rsiGREM9rq94DzumOtJ6f9AdDS1Y72s5LP9qsoTbW3/vQ91t79kzqtj42oEtaNPeU+T6LSYXImaUfP/lmtx3vyd30BuiUQ4s6eEtY43DAzDAOjGbDThB1P2cfdOOOswzuPt57Fz6zrhlRlOYHaGjlnSsl98s39jrkbnJ2YjMG6O5O74v2INR43OuZlYA6GuG/EuJL6/yvLsnA5y+grpNpLKrmSU6FWeXpbPjblD0SwrEkGXFplAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;New table created&quot;
        title=&quot;New table created&quot;
        src=&quot;/static/00f20b5be77fdba3a93ae7f210dbe103/5a190/table-creation.png&quot;
        srcset=&quot;/static/00f20b5be77fdba3a93ae7f210dbe103/772e8/table-creation.png 200w,
/static/00f20b5be77fdba3a93ae7f210dbe103/e17e5/table-creation.png 400w,
/static/00f20b5be77fdba3a93ae7f210dbe103/5a190/table-creation.png 800w,
/static/00f20b5be77fdba3a93ae7f210dbe103/c1b63/table-creation.png 1200w,
/static/00f20b5be77fdba3a93ae7f210dbe103/4e814/table-creation.png 1335w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can now query the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;select * from opendatahub.sample limit 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;Check the query results in Hue.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/398d325391341fd4350446f52080da6e/d4377/query-results.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 66%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA60lEQVR42u2T2WrFMAxE8/9fmn2PHceLyhnwhfahUMhjDY4saTQaE7kJIVqKyWKMdt+3Pc8j30qx31bOWTUppc8upVhTD2+tf8IXCMMT7c3V8PtzTp9RYKMY5dX/25VzMee9XdelOYQAC+F+OuunxS5/m3dOMe+9CrH4dRYRoTkMKYkshGDHcWif52n7vuu8bZvy67oqjk+cPDHOWDCI0ZWdc+q0LIuN4yhA3/cq7rpORfjTNCk/DIPN86wcxG3bCiuFfCDkyUFYwYAA4/8kxIKFsDYF+40QhSgDTJJCiKqPJU8zyMiBqU2rwi/8l/zyXSqJwQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Query results&quot;
        title=&quot;Query results&quot;
        src=&quot;/static/398d325391341fd4350446f52080da6e/5a190/query-results.png&quot;
        srcset=&quot;/static/398d325391341fd4350446f52080da6e/772e8/query-results.png 200w,
/static/398d325391341fd4350446f52080da6e/e17e5/query-results.png 400w,
/static/398d325391341fd4350446f52080da6e/5a190/query-results.png 800w,
/static/398d325391341fd4350446f52080da6e/c1b63/query-results.png 1200w,
/static/398d325391341fd4350446f52080da6e/d4377/query-results.png 1294w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[GPU Enablement]]></title><description><![CDATA[Open Data Hub has support for accessing NVIDIA GPUs from Jupyter notebooks. Currently,
the Open Data Hub team has verified that ODH Core…]]></description><link>https://www.opendatahub.io/docs/administration/advanced-installation/gpu/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/administration/advanced-installation/gpu/</guid><content:encoded>&lt;p&gt;Open Data Hub has support for accessing NVIDIA GPUs from Jupyter notebooks. Currently,
the Open Data Hub team has verified that ODH Core Jupyter notebooks can successfully access the GPU in OpenShift 4.8+ clusters.&lt;/p&gt;
&lt;h3&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OpenShift cluster with GPU(s) enabled&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Enabling GPUs in OpenShift 4.8+ can be achieved by deploying the NVIDIA GPU Operator from the OpenShift OperatorHub. Please follow NVIDIA GPU OpenShift installation instructions &lt;a href=&quot;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html&quot;&gt;here&lt;/a&gt;. If you encounter any issue related to this operator, please check your issue against the list open issues in NVIDIA &lt;a href=&quot;https://github.com/NVIDIA/gpu-operator/issues&quot;&gt;gpu-operator&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/openshift/cluster-nfd-operator&quot;&gt;Node Feature Discovery&lt;/a&gt; operator is responsible for discovering and labeling hardware (GPU(s) in this case) features available on each node.
The &lt;a href=&quot;https://github.com/NVIDIA/gpu-operator&quot;&gt;NVIDIA GPU Operator&lt;/a&gt; will setup and install the necessary drivers to enable the use of GPU(s) as compute resource.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Verifying GPU Availability&lt;/h3&gt;
&lt;p&gt;To spawn a Jupyter Notebook with GPU support, set the &lt;code&gt;GPU&lt;/code&gt; field to a number greater than 0.  From inside the notebook, run the following command to verify it&apos;s availability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import tensorflow as tf
tf.test.is_gpu_available()
&lt;/code&gt;&lt;/pre&gt;</content:encoded></item><item><title><![CDATA[Installing Object Storage]]></title><description><![CDATA[Deploying Ceph Nano Object Storage with Open Data Hub The ceph-nano component of Open Data Hub can provide minimal object storage for use in…]]></description><link>https://www.opendatahub.io/docs/administration/advanced-installation/object-storage/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/administration/advanced-installation/object-storage/</guid><content:encoded>&lt;h4&gt;Deploying Ceph Nano Object Storage with Open Data Hub&lt;/h4&gt;
&lt;p&gt;The ceph-nano component of Open Data Hub can provide minimal object storage for use in your development environment.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt; If you need to deploy persistent, large scale object storage for use in a production environment, we recommend &lt;a href=&quot;https://rook.io/&quot;&gt;Rook Ceph&lt;/a&gt; or &lt;a href=&quot;https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage&quot;&gt;OpenShift Container Storage&lt;/a&gt; for scaleable, high availability storage running inside of your OpenShift cluster.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;When deploying Open Data Hub, you will need to include the &lt;code&gt;ceph-nano&lt;/code&gt; component in your kfdef file.&lt;/p&gt;
&lt;p&gt;Here is an example kfdef used in the basic tutorial that we have modified to deploy the &lt;code&gt;ceph nano&lt;/code&gt; component&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;# ODH uses the KfDef manifest format to specify what components will be included in the deployment
apiVersion: kfdef.apps.kubeflow.org/v1
kind: KfDef
metadata:
  # The name of your deployment
  name: opendatahub
# only the components listed in the `KFDef` resource will be deployed:
spec:
  applications:
    # REQUIRED: This contains all of the common options used by all ODH components
    - kustomizeConfig:
        repoRef:
          name: manifests
          path: odh-common
      name: odh-common
    # Create the SecurityContextConstraint to grant the ceph-nano service account anyuid permissions
    - kustomizeConfig:
        repoRef:
          name: manifests
          path: ceph/object-storage/scc
      name: ceph-nano-scc
    # Deploy ceph-nano for minimal object storage running in a pod
    - kustomizeConfig:
        repoRef:
          name: manifests
          path: ceph/object-storage/nano
      name: ceph-nano
    # Deploy Radanalytics Spark Operator
    - kustomizeConfig:
        repoRef:
          name: manifests
          path: radanalyticsio/spark/cluster
      name: radanalyticsio-spark-cluster
    # Deploy Open Data Hub JupyterHub
    - kustomizeConfig:
        parameters:
          - name: s3_endpoint_url
            value: &quot;http://ceph-nano-0&quot;
        repoRef:
          name: manifests
          path: jupyterhub/jupyterhub
      name: jupyterhub
    # Deploy addtional Open Data Hub Jupyter notebooks
    - kustomizeConfig:
        overlays:
          - additional
        repoRef:
          name: manifests
          path: jupyterhub/notebook-images
      name: notebook-images
  # Reference to all of the git repo archives that contain component kustomize manifests
  repos:
    # Official Open Data Hub v1.0.0 component manifests repo
    # This shows that we will be deploying components from an archive of the odh-manifests repo tagged for v1.0.0
    - name: manifests
      uri: &apos;https://github.com/opendatahub-io/odh-manifests/tarball/v1.0.0&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once the &lt;code&gt;ceph-nano-0 pod&lt;/code&gt; is running, you can access the object storage from any pod inside the namespace use the endpoint &lt;code&gt;http://ceph-nano-0&lt;/code&gt;.
S3 credentials to access object storage will be saved in the secret name &lt;code&gt;ceph-nano-credentials&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt; This object storage is for DEVELOPMENT PURPOSES ONLY. The access credentials and stored objects DO NOT persist across pod restarts and will be changed/deleted when a new pod is spawned.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Pre-requisites for Optional Components]]></title><description><![CDATA[Before installing Argo, Seldon, or Kafka, there are pre-requisites that must be installed before the Open Data Hub operator.  You must…]]></description><link>https://www.opendatahub.io/docs/administration/advanced-installation/optional/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/administration/advanced-installation/optional/</guid><content:encoded>&lt;p&gt;Before installing Argo, Seldon, or Kafka, there are pre-requisites that must be installed before the Open Data Hub operator.  You must install several &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions&quot;&gt;Custom Resource Definitions (CRDs)&lt;/a&gt; either through command line, or through the console.&lt;/p&gt;
&lt;h3&gt;CLI Installation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;From the command line, use the &lt;code&gt;oc&lt;/code&gt; command line tool to log in as a user with &lt;code&gt;cluster-admin&lt;/code&gt; privileges.  For a developer installation from &lt;a href=&quot;https://try.openshift.com/&quot;&gt;try.openshift.com&lt;/a&gt; including AWS and CRC, the &lt;code&gt;kubeadmin&lt;/code&gt; user will work.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ oc login https://api.crc.testing:6443 -u kubeadmin -p ***********
Login successful.

You have access to 53 projects, the list has been suppressed. You can list all projects with &apos;oc projects&apos;

Using project &quot;default&quot;.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To prepare to install &lt;strong&gt;Argo&lt;/strong&gt;, install the &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/raw/v0.5.1/deploy/crds/argo-crd.yaml&quot;&gt;workflow CRD&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ oc apply -f https://gitlab.com/opendatahub/opendatahub-operator/raw/v0.5.1/deploy/crds/argo-crd.yaml
customresourcedefinition.apiextensions.k8s.io/workflows.argoproj.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To prepare to install &lt;strong&gt;Seldon&lt;/strong&gt;, install the &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/raw/v0.5.1/deploy/crds/seldon-deployment-crd.json&quot;&gt;SeldonDeployment CRD&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ oc apply -f https://gitlab.com/opendatahub/opendatahub-operator/raw/v0.5.1/deploy/crds/seldon-deployment-crd.json
customresourcedefinition.apiextensions.k8s.io/seldondeployments.machinelearning.seldon.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In preparation to deploy a &lt;strong&gt;Kafka&lt;/strong&gt; cluster, install &lt;a href=&quot;https://strimzi.io/&quot;&gt;Strimzi&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Web Console Installation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;From the OpenShift console, log in as a user with &lt;code&gt;cluster-admin&lt;/code&gt; privileges.  For a developer installation from &lt;a href=&quot;https://try.openshift.com/&quot;&gt;try.openshift.com&lt;/a&gt; including AWS and CRC, the &lt;code&gt;kubeadmin&lt;/code&gt; user will work.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/e8ea7e8056b11c38e25cfcac7d9469a5/c2d13/login.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 51%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABmUlEQVR42mWSvU7jQBSFZ+zYjs3Yif9i+x2g4GkQiHdAQlGa1RYrGoSEaCARART8AHSICATp9g22oaakDj8x4azu3bUVJyMdXc2M9M2Ze4+I4xil0jRFlmU10ZlpWZCaBk3Xa6LzIAjQ6XQqhlgEmqYJIcSKpJQMWIYKTYNlWVhkVMAwDNHr9TAajdDv9zEYDDAcDpHnOXZ2dxls23YFLuF6o4EoilaBZP1pMgGtoigwn8+50hpeXsITApsb64iTBMp10Wq30TAMBrd9v/p2zeF4PMZsVuD945PrdPrGwOs8h2EYDCHYmlJo2nbl1Gu1VoHk8P7hHsA3itkHMP/CF1WAv62UQppl7MZ1XTiOA9tx/u09D1azuTSUMMDN3QSPL8DNnxlun7+5/n4FTs+vuIe+73O/yE0p2tNghJT8SAWMohB7+10cnFzgx+Epfh6dofvrmOvW9g4DaQAkwzTZEYF0GtL/qdNdLTaeUvDWmmgpm6UcE7oU/HrZr2VVEZKSH6kBkyRBkqSVyqBHccw9pCAvh3sx5GEQ4C+OWT0+ie33xwAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Log in to OpenShift&quot;
        title=&quot;Log in to OpenShift&quot;
        src=&quot;/static/e8ea7e8056b11c38e25cfcac7d9469a5/5a190/login.png&quot;
        srcset=&quot;/static/e8ea7e8056b11c38e25cfcac7d9469a5/772e8/login.png 200w,
/static/e8ea7e8056b11c38e25cfcac7d9469a5/e17e5/login.png 400w,
/static/e8ea7e8056b11c38e25cfcac7d9469a5/5a190/login.png 800w,
/static/e8ea7e8056b11c38e25cfcac7d9469a5/c1b63/login.png 1200w,
/static/e8ea7e8056b11c38e25cfcac7d9469a5/29007/login.png 1600w,
/static/e8ea7e8056b11c38e25cfcac7d9469a5/c2d13/login.png 2560w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To install CRDs, find the list of CRDs under &lt;code&gt;Administration&lt;/code&gt; -&gt; &lt;code&gt;Custom Resource Definitions&lt;/code&gt; and click &lt;code&gt;Create Custom Resource Definitions&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/56b67af03551edcf4774cfd9145f3ba4/3955b/admin-crd.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 58.50000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABy0lEQVR42o2SW08TQRTH90uISgxRhJJIt3a5iL1YpAnFkgKNhAcIPvD1iF/BF+OtKiUhAXxSwd0YExPbWbszOzszfzOz6bIlQTrJP3NyzsxvzmWsBxMZ2FkbY/cnccd5gtF82WjELuJGtoCbdnFAt3KxtD1iF8y5sfkqbucrGHUqsJyFMjLTDpbqTey3vmG//RMvDzy8/voHLZfgk0vw2fVjeT4OPB9tL7Y//iD4cE7w7nsH78+6RtbDuQLGp2zUG028PfmFV8cEb750EXAJvRSGWEr2DVj5uSLGp3KorW7A/U1it1IGFFAKxkJIpYxCziGkNNIx7WM8BAtDKAUIIWHlZh7jXsZGbXUdR+cdECoRRcKAKaXwfT9+QEOlTHYh4jOMs8SvfZbzqGRKrtXXceJ20OPxhT6QMZYAoygysauAerfsVIYaSFPAIAjQ6/USIOd8OODdySyW62sG6DOJSAyWnC7pfyUbYOnpMqadBaw1t3DqdvGXKdPcPjCd4XU9NMDqswbmS0vY2nmBU69rhhKmhpIGDtXD6koDs8VFbG7vGmAo9Le5yJAQkgxmqJJrjecoV1ewvbuHw7MOIjkI1D1Mgy4DKacQ8iL+D2qiLvCQisgVAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Admin CRDs&quot;
        title=&quot;Admin CRDs&quot;
        src=&quot;/static/56b67af03551edcf4774cfd9145f3ba4/5a190/admin-crd.png&quot;
        srcset=&quot;/static/56b67af03551edcf4774cfd9145f3ba4/772e8/admin-crd.png 200w,
/static/56b67af03551edcf4774cfd9145f3ba4/e17e5/admin-crd.png 400w,
/static/56b67af03551edcf4774cfd9145f3ba4/5a190/admin-crd.png 800w,
/static/56b67af03551edcf4774cfd9145f3ba4/c1b63/admin-crd.png 1200w,
/static/56b67af03551edcf4774cfd9145f3ba4/29007/admin-crd.png 1600w,
/static/56b67af03551edcf4774cfd9145f3ba4/3955b/admin-crd.png 1865w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To prepare to install &lt;strong&gt;Argo&lt;/strong&gt;, install the workflow CRD.  Click &lt;code&gt;Create Custom Resource Definitions&lt;/code&gt; copy the &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/v0.5.1/deploy/crds/argo-crd.yaml&quot;&gt;argo-crd.yaml&lt;/a&gt; into the editing window, and click &lt;code&gt;Create&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/81ef3ce77e02da20d2aae8db4e8ff90a/fb1ef/create-argo-crd.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 58.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB3UlEQVR42qXTzWsTQRjH8cUEi0KC2WQ3u832jZS0cfNmktIkxFpbWqog/hUeLKSa0KCSEP8AGyImWjUH68WTeBYvKnoQkxbBg2h9qTdBPAiVSA/9ym4CKZ5SevgwzwMzv5lhGCESDOEdHaVfn0AMpBD1JE49gagnkALJLqPX9/ZJRP8kcihNf2zaHN2RKYRQPIXveJj6k2d82oaNHy3e/dzhw+9dNrfhc8fXFmy14Ptf+NZq95t/MNcYjPr9rx0EfyiOYXX1Pg8frFG/XWbtbpl7tTK1mytUKwajLlPtqP3nzq0K1coNXr18jjAWjBGdTHPhYgaX5kU8dgTbUStWqxWLpeuQxdKxt2473NeHIAhkczmEsUCUcDzFUjbPeDCKf3yEAY+KW1FR1d5omobD4aBUKrWvbARmLucZ9vpQFRlFcSPLcs+MULvdTrFYbJ8wMpFicSmHZ2AIp+hEkmQkSeqZoijdQJ9+gnA8aQZqgyO4nE4k+QCBseQUiZMzZLJ5tMFhXK4DBqZPzzOzcI5Ly1dRtSHzhPK+A1VsNhuFQgEhNT3HqfmzLF+5Zv4Yt1s2Jxi79srj8SCKDq4brzx75jyzcwus1B/z6GmDRuMtzWZzXzbW13nx+g0fv2zxD4jftwDO0gqWAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Argo CRD&quot;
        title=&quot;Argo CRD&quot;
        src=&quot;/static/81ef3ce77e02da20d2aae8db4e8ff90a/5a190/create-argo-crd.png&quot;
        srcset=&quot;/static/81ef3ce77e02da20d2aae8db4e8ff90a/772e8/create-argo-crd.png 200w,
/static/81ef3ce77e02da20d2aae8db4e8ff90a/e17e5/create-argo-crd.png 400w,
/static/81ef3ce77e02da20d2aae8db4e8ff90a/5a190/create-argo-crd.png 800w,
/static/81ef3ce77e02da20d2aae8db4e8ff90a/c1b63/create-argo-crd.png 1200w,
/static/81ef3ce77e02da20d2aae8db4e8ff90a/29007/create-argo-crd.png 1600w,
/static/81ef3ce77e02da20d2aae8db4e8ff90a/fb1ef/create-argo-crd.png 1879w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To prepare to install &lt;strong&gt;Seldon&lt;/strong&gt;, install the SeldonDeployment CRD.  Click &lt;code&gt;Create Custom Resource Definitions&lt;/code&gt; copy the &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/v0.5.1/deploy/crds/seldon-deployment-crd.json&quot;&gt;seldon-deployment-crd.json&lt;/a&gt; into the editing window, and click &lt;code&gt;Create&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/246e39a5b1773935da79105aa57f4991/85bc8/create-seldon-crd.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 55.49999999999999%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACKklEQVR42oXP30tTcRjH8RPo5oU4d3bO2fket+bc3DKbqeXUJpLQyTFN588QoR/UbRA5f8yW/hdB1HXdJUjUNi0IuvFfEC8KNbqIKMhSK96xnYEJYhcvHnh4+PB5pHAwRORMhFBDI/5YAuWciXbexN12GRHtxdsRx9sZx9Nh8f5DtPUi2hP4ugYwOhJ4Yn1IzdEYgYazXDT7WP+8w8a3X6x/2Wfj628+7MBmwQ/Y3oWPe/Bp37K9Z+22dmHzp+X99z9IzW0XqAs3Ee8fYun5ErPTKTLpaR7Mp7g/lyI9m2JueupY6ZkU6Zl7PH70EKkl2kXgVBMDw1e5c3eKE/ZKFFXGLdtwVVdQVWmnosJOebnNYispP2Cz2SkrK8M0zVLDUIT+5BjzmQVcqk4gFKYhaBCsFQihI3Qd/RhCGOhulZGRYaRISzu19Y3FwHRmEUe1jM8fwOfV8QgVVf0/TXOjKjJDyUGr4aFApwt/MIw/EEDTNBRFRVEPHB2oWYFDSaTW9i7qTzeTHJ0oNXQihKDmZB1efxCPoWHoKkKoGEItvnZUw8J+dCSJZCYGicZ6mLxxm7nMIlXFQB1R48VX6y+GaaUWlqMD3W6N8bEkUk9vH62d3Uxcu8XCwiIOhwNVVdB0A+HxFV+RZScu2VmcBU7nYbJcuKlmfHQQKX5lmO5LCSav3+TZqzWePF1mJZ8jl8uTX33Daj7H6kq26HVpZrOH5XJZll+85O27Nf4Ciu2UhimGJsEAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Seldon CRD&quot;
        title=&quot;Seldon CRD&quot;
        src=&quot;/static/246e39a5b1773935da79105aa57f4991/5a190/create-seldon-crd.png&quot;
        srcset=&quot;/static/246e39a5b1773935da79105aa57f4991/772e8/create-seldon-crd.png 200w,
/static/246e39a5b1773935da79105aa57f4991/e17e5/create-seldon-crd.png 400w,
/static/246e39a5b1773935da79105aa57f4991/5a190/create-seldon-crd.png 800w,
/static/246e39a5b1773935da79105aa57f4991/c1b63/create-seldon-crd.png 1200w,
/static/246e39a5b1773935da79105aa57f4991/29007/create-seldon-crd.png 1600w,
/static/246e39a5b1773935da79105aa57f4991/85bc8/create-seldon-crd.png 1875w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In preparation to deploy a &lt;strong&gt;Kafka&lt;/strong&gt; cluster, install &lt;a href=&quot;https://strimzi.io/&quot;&gt;Strimzi&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/7d7e04629f5b058cf1ab757be4b5d05f/4c917/install-strimzi.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 55.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABa0lEQVR42qWR2U7CQBSG+xombiguKAoWo1hKrQoqtsTlwuc23rgQLzSGi85M04K2nW6/mcHlUpGTfDmzfjlzRlleKKK8VsZyaR1LRhdTux1Ma5ZkRrMwr1so6LZkXrex8ImYzzYszDRsFA+uMNfsomBeQKlsayiuVdE8OsXNs4Pb/gD3NMQDi/DAOHquIEKPRTI/uVwixo8swj2NcEdCeUegbKh1LK5uQjNa6L++wKcOfJfBd10MXAqfEbwPPARfDP2f+XCUw6H/nRWj1YFab+LEvoRDGZI0RRhxvAccPMmRpBmyTJD/Co9jKO2zc2hmG9blNSilyLNMbjD/DWEUI89z/DXEWaUqeliqQDOOQAgRy4jjBJQxJEmCv+tGIXsohA2zLYVpmsonep6HIAgwbiiV2t5IuN8CIVTKROlfeWyhuqNjpaxCN49BKJUV/kf0LdySwpoUOg6RfZsklFrdkBU2D0/kL3POJxJ+ALws7NBo9LuMAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Strimzi Install&quot;
        title=&quot;Strimzi Install&quot;
        src=&quot;/static/7d7e04629f5b058cf1ab757be4b5d05f/5a190/install-strimzi.png&quot;
        srcset=&quot;/static/7d7e04629f5b058cf1ab757be4b5d05f/772e8/install-strimzi.png 200w,
/static/7d7e04629f5b058cf1ab757be4b5d05f/e17e5/install-strimzi.png 400w,
/static/7d7e04629f5b058cf1ab757be4b5d05f/5a190/install-strimzi.png 800w,
/static/7d7e04629f5b058cf1ab757be4b5d05f/c1b63/install-strimzi.png 1200w,
/static/7d7e04629f5b058cf1ab757be4b5d05f/29007/install-strimzi.png 1600w,
/static/7d7e04629f5b058cf1ab757be4b5d05f/4c917/install-strimzi.png 1859w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Adding custom notebook images]]></title><description><![CDATA[This document describes how to add custom notebook images to use in the JupyterHub Spawner UI How to add a custom notebook image The…]]></description><link>https://www.opendatahub.io/docs/administration/installation-customization/add-custom-image/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/administration/installation-customization/add-custom-image/</guid><content:encoded>&lt;p&gt;This document describes how to add custom notebook images to use in the JupyterHub Spawner UI&lt;/p&gt;
&lt;h2&gt;How to add a custom notebook image&lt;/h2&gt;
&lt;p&gt;The JupyterHub Spawner UI provides a list of available images, which are suitable to be run as a singleuser notebook server.
Each of these images provide a set of different preinstalled dependencies and each is bound to a different profile. This means each also uses different configurations and compute resources.&lt;/p&gt;
&lt;p&gt;Each image is listed in a dropdown list in the Spawner UI.&lt;/p&gt;
&lt;h3&gt;Adding an existing image&lt;/h3&gt;
&lt;p&gt;If an image exists in a registry (like quay.io) and you wish to add it to ODH, you can create an &lt;code&gt;ImageStream&lt;/code&gt;, which points to the image.&lt;/p&gt;
&lt;p&gt;Then choose to add a label and enter the:
&lt;code&gt;opendatahub.io/notebook-image=true&lt;/code&gt; label to enable it in the Spawner UI&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;ImageStream&lt;/code&gt; YAML should look similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  labels:
    opendatahub.io/notebook-image: &quot;true&quot;
  annotations:
    opendatahub.io/notebook-image-url: &quot;https://github.com/thoth-station/s2i-minimal-notebook&quot;
    opendatahub.io/notebook-image-name: &quot;Minimal Notebook Image&quot;
    opendatahub.io/notebook-image-desc: &quot;Jupyter notebook image with minimal dependency set to start experimenting with Jupyter environment.&quot;
  name: s2i-minimal-notebook
spec:
  lookupPolicy:
    local: true
  tags:
  - annotations:
      openshift.io/imported-from: quay.io/thoth-station/s2i-minimal-notebook
    from:
      kind: DockerImage
      name: quay.io/thoth-station/s2i-minimal-notebook:v0.0.4
    name: &quot;v0.0.4&quot;
    referencePolicy:
      type: Source
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Creating a custom image&lt;/h3&gt;
&lt;p&gt;To create a custom image, you can use  &lt;a href=&quot;https://quay.io/repository/thoth-station/s2i-minimal-notebook&quot;&gt;https://quay.io/repository/thoth-station/s2i-minimal-notebook&lt;/a&gt; as your base image and create a &lt;code&gt;BuildConfig&lt;/code&gt;, which has your own repository with your desired customisations as its source.&lt;/p&gt;
&lt;p&gt;For example, you could take the &lt;a href=&quot;https://github.com/thoth-station/s2i-scipy-notebook/&quot;&gt;https://github.com/thoth-station/s2i-scipy-notebook/&lt;/a&gt; repo, and your &lt;code&gt;BuildConfig&lt;/code&gt; would look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: build.openshift.io/v1
kind: BuildConfig
metadata:
  labels:
    build: s2i-scipy-notebook
  name: s2i-scipy-notebook
spec:
  output:
    to:
      kind: ImageStreamTag
      name: s2i-scipy-notebook:local-build
  source:
    git:
      uri: https://github.com/thoth-station/s2i-scipy-notebook
    type: Git
  strategy:
    sourceStrategy:
      from:
        kind: ImageStreamTag
        name: quay.io/thoth-station/s2i-minimal-notebook:latest
    type: Source
  triggers:
  - type: ConfigChange
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now create an &lt;code&gt;ImageStream&lt;/code&gt; the same way as in the &lt;strong&gt;Adding an existing image&lt;/strong&gt; section and push the build into it.&lt;/p&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;p&gt;This feature is not dependent on the version of the UI&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Customizing the Installation]]></title><description><![CDATA[This document explains the preferred way of customizing Open Data Hub and Kubeflow deployments. How to Customize the Installation There are…]]></description><link>https://www.opendatahub.io/docs/administration/installation-customization/customization/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/administration/installation-customization/customization/</guid><content:encoded>&lt;p&gt;This document explains the preferred way of customizing Open Data Hub and Kubeflow deployments.&lt;/p&gt;
&lt;h2&gt;How to Customize the Installation&lt;/h2&gt;
&lt;p&gt;There are essentially three ways to customize the deployment. All of them will involve working with &lt;a href=&quot;https://kustomize.io/&quot;&gt;kustomize&lt;/a&gt;, the &lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests&quot;&gt;odh-manifests&lt;/a&gt; repository and a &lt;a href=&quot;https://www.redhat.com/en/topics/devops/what-is-gitops&quot;&gt;GitOps&lt;/a&gt; workflow&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fork odh-manifests and maintain changes there&lt;/li&gt;
&lt;li&gt;Create your own manifests repository to overwrite the component resources&lt;/li&gt;
&lt;li&gt;Fork odh-manifests repository and extend it with overlays for components&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of the above requires you to have a git repository with some Kustomize and OpenShift resources which you will reference from the KFDef resource, but each of them requires different level of effort to create and maintain.&lt;/p&gt;
&lt;p&gt;We will only discuss the last one (overlays) here, since that is the one we promote and recommend for maintaining the customizations. If you would like to learn more about the other approaches, you can read &lt;a href=&quot;https://developers.redhat.com/blog/2020/07/23/open-data-hub-and-kubeflow-installation-customization/&quot;&gt;the blog post&lt;/a&gt; we published on this topic.&lt;/p&gt;
&lt;h3&gt;Customizing a Deployment Using Overlays&lt;/h3&gt;
&lt;p&gt;Overlays provide a great way to offer an optional customization to a component. It allows you to modify or delete/exclude existing resources or add new ones. You can simply enable and disable overlays in KFDef custom resource by adding to or removing them from overlays list at the component level.&lt;/p&gt;
&lt;p&gt;The way overlays generally work in the context of Open Data Hub manifests is that they add another layer on top of the &lt;code&gt;base&lt;/code&gt; resources for the component. This layer can leverage any of the Kustomize functionality to extend the &lt;code&gt;base&lt;/code&gt;. We provide many overlays across the odh-manifests repository. Some of them add resources (e.g. &lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/data-science-pipelines/overlays/metadata-store-mariadb&quot;&gt;deploying custom database storage&lt;/a&gt;) and others can modify resources to customize the deployment for a specific environment (e.g. &lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard/overlays/authentication&quot;&gt;enabling oauth authentication in the dashboard&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Overlays add a bit of complexity and it takes a bit of learning to become productive when using them, but it is well worth due to the flexibility and long term maintainablity you gain by leveraging them for your customizations.&lt;/p&gt;
&lt;p&gt;If you need additional examples of kustomize overlays, please read the kubernetes documentation on kustomize manifests &lt;a href=&quot;https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/#bases-and-overlays&quot;&gt;Bases and Overlays&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[No title]]></title><description><![CDATA[TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. "License" shall mean the terms and conditions for use…]]></description><link>https://www.opendatahub.io/docs/LICENSE/</link><guid isPermaLink="false">https://www.opendatahub.io/docs/LICENSE/</guid><content:encoded>&lt;pre&gt;&lt;code&gt;                             Apache License
                       Version 2.0, January 2004
                    http://www.apache.org/licenses/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Definitions.&lt;/p&gt;
&lt;p&gt;&quot;License&quot; shall mean the terms and conditions for use, reproduction,
and distribution as defined by Sections 1 through 9 of this document.&lt;/p&gt;
&lt;p&gt;&quot;Licensor&quot; shall mean the copyright owner or entity authorized by
the copyright owner that is granting the License.&lt;/p&gt;
&lt;p&gt;&quot;Legal Entity&quot; shall mean the union of the acting entity and all
other entities that control, are controlled by, or are under common
control with that entity. For the purposes of this definition,
&quot;control&quot; means (i) the power, direct or indirect, to cause the
direction or management of such entity, whether by contract or
otherwise, or (ii) ownership of fifty percent (50%) or more of the
outstanding shares, or (iii) beneficial ownership of such entity.&lt;/p&gt;
&lt;p&gt;&quot;You&quot; (or &quot;Your&quot;) shall mean an individual or Legal Entity
exercising permissions granted by this License.&lt;/p&gt;
&lt;p&gt;&quot;Source&quot; form shall mean the preferred form for making modifications,
including but not limited to software source code, documentation
source, and configuration files.&lt;/p&gt;
&lt;p&gt;&quot;Object&quot; form shall mean any form resulting from mechanical
transformation or translation of a Source form, including but
not limited to compiled object code, generated documentation,
and conversions to other media types.&lt;/p&gt;
&lt;p&gt;&quot;Work&quot; shall mean the work of authorship, whether in Source or
Object form, made available under the License, as indicated by a
copyright notice that is included in or attached to the work
(an example is provided in the Appendix below).&lt;/p&gt;
&lt;p&gt;&quot;Derivative Works&quot; shall mean any work, whether in Source or Object
form, that is based on (or derived from) the Work and for which the
editorial revisions, annotations, elaborations, or other modifications
represent, as a whole, an original work of authorship. For the purposes
of this License, Derivative Works shall not include works that remain
separable from, or merely link (or bind by name) to the interfaces of,
the Work and Derivative Works thereof.&lt;/p&gt;
&lt;p&gt;&quot;Contribution&quot; shall mean any work of authorship, including
the original version of the Work and any modifications or additions
to that Work or Derivative Works thereof, that is intentionally
submitted to Licensor for inclusion in the Work by the copyright owner
or by an individual or Legal Entity authorized to submit on behalf of
the copyright owner. For the purposes of this definition, &quot;submitted&quot;
means any form of electronic, verbal, or written communication sent
to the Licensor or its representatives, including but not limited to
communication on electronic mailing lists, source code control systems,
and issue tracking systems that are managed by, or on behalf of, the
Licensor for the purpose of discussing and improving the Work, but
excluding communication that is conspicuously marked or otherwise
designated in writing by the copyright owner as &quot;Not a Contribution.&quot;&lt;/p&gt;
&lt;p&gt;&quot;Contributor&quot; shall mean Licensor and any individual or Legal Entity
on behalf of whom a Contribution has been received by Licensor and
subsequently incorporated within the Work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grant of Copyright License. Subject to the terms and conditions of
this License, each Contributor hereby grants to You a perpetual,
worldwide, non-exclusive, no-charge, royalty-free, irrevocable
copyright license to reproduce, prepare Derivative Works of,
publicly display, publicly perform, sublicense, and distribute the
Work and such Derivative Works in Source or Object form.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Grant of Patent License. Subject to the terms and conditions of
this License, each Contributor hereby grants to You a perpetual,
worldwide, non-exclusive, no-charge, royalty-free, irrevocable
(except as stated in this section) patent license to make, have made,
use, offer to sell, sell, import, and otherwise transfer the Work,
where such license applies only to those patent claims licensable
by such Contributor that are necessarily infringed by their
Contribution(s) alone or by combination of their Contribution(s)
with the Work to which such Contribution(s) was submitted. If You
institute patent litigation against any entity (including a
cross-claim or counterclaim in a lawsuit) alleging that the Work
or a Contribution incorporated within the Work constitutes direct
or contributory patent infringement, then any patent licenses
granted to You under this License for that Work shall terminate
as of the date such litigation is filed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Redistribution. You may reproduce and distribute copies of the
Work or Derivative Works thereof in any medium, with or without
modifications, and in Source or Object form, provided that You
meet the following conditions:&lt;/p&gt;
&lt;p&gt;(a) You must give any other recipients of the Work or
Derivative Works a copy of this License; and&lt;/p&gt;
&lt;p&gt;(b) You must cause any modified files to carry prominent notices
stating that You changed the files; and&lt;/p&gt;
&lt;p&gt;(c) You must retain, in the Source form of any Derivative Works
that You distribute, all copyright, patent, trademark, and
attribution notices from the Source form of the Work,
excluding those notices that do not pertain to any part of
the Derivative Works; and&lt;/p&gt;
&lt;p&gt;(d) If the Work includes a &quot;NOTICE&quot; text file as part of its
distribution, then any Derivative Works that You distribute must
include a readable copy of the attribution notices contained
within such NOTICE file, excluding those notices that do not
pertain to any part of the Derivative Works, in at least one
of the following places: within a NOTICE text file distributed
as part of the Derivative Works; within the Source form or
documentation, if provided along with the Derivative Works; or,
within a display generated by the Derivative Works, if and
wherever such third-party notices normally appear. The contents
of the NOTICE file are for informational purposes only and
do not modify the License. You may add Your own attribution
notices within Derivative Works that You distribute, alongside
or as an addendum to the NOTICE text from the Work, provided
that such additional attribution notices cannot be construed
as modifying the License.&lt;/p&gt;
&lt;p&gt;You may add Your own copyright statement to Your modifications and
may provide additional or different license terms and conditions
for use, reproduction, or distribution of Your modifications, or
for any such Derivative Works as a whole, provided Your use,
reproduction, and distribution of the Work otherwise complies with
the conditions stated in this License.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Submission of Contributions. Unless You explicitly state otherwise,
any Contribution intentionally submitted for inclusion in the Work
by You to the Licensor shall be under the terms and conditions of
this License, without any additional terms or conditions.
Notwithstanding the above, nothing herein shall supersede or modify
the terms of any separate license agreement you may have executed
with Licensor regarding such Contributions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trademarks. This License does not grant permission to use the trade
names, trademarks, service marks, or product names of the Licensor,
except as required for reasonable and customary use in describing the
origin of the Work and reproducing the content of the NOTICE file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disclaimer of Warranty. Unless required by applicable law or
agreed to in writing, Licensor provides the Work (and each
Contributor provides its Contributions) on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
implied, including, without limitation, any warranties or conditions
of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
PARTICULAR PURPOSE. You are solely responsible for determining the
appropriateness of using or redistributing the Work and assume any
risks associated with Your exercise of permissions under this License.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Limitation of Liability. In no event and under no legal theory,
whether in tort (including negligence), contract, or otherwise,
unless required by applicable law (such as deliberate and grossly
negligent acts) or agreed to in writing, shall any Contributor be
liable to You for damages, including any direct, indirect, special,
incidental, or consequential damages of any character arising as a
result of this License or out of the use or inability to use the
Work (including but not limited to damages for loss of goodwill,
work stoppage, computer failure or malfunction, or any and all
other commercial damages or losses), even if such Contributor
has been advised of the possibility of such damages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accepting Warranty or Additional Liability. While redistributing
the Work or Derivative Works thereof, You may choose to offer,
and charge a fee for, acceptance of support, warranty, indemnity,
or other liability obligations and/or rights consistent with this
License. However, in accepting such obligations, You may act only
on Your own behalf and on Your sole responsibility, not on behalf
of any other Contributor, and only if You agree to indemnify,
defend, and hold each Contributor harmless for any liability
incurred by, or claims asserted against, such Contributor by reason
of your accepting any such warranty or additional liability.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;END OF TERMS AND CONDITIONS&lt;/p&gt;
&lt;p&gt;APPENDIX: How to apply the Apache License to your work.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  To apply the Apache License to your work, attach the following
  boilerplate notice, with the fields enclosed by brackets &quot;[]&quot;
  replaced with your own identifying information. (Don&apos;t include
  the brackets!)  The text should be enclosed in the appropriate
  comment syntax for the file format. We also recommend that a
  file or class name and description of purpose be included on the
  same &quot;printed page&quot; as the copyright notice for easier
  identification within third-party archives.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Copyright 2022 Red Hat&lt;/p&gt;
&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   http://www.apache.org/licenses/LICENSE-2.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Scalable Kafka Deployment on OpenShift for ML]]></title><link>https://www.opendatahub.io/videos/scalable-kafka-deployment-on-openshift-for-ml/</link><guid isPermaLink="false">https://www.opendatahub.io/videos/scalable-kafka-deployment-on-openshift-for-ml/</guid><pubDate>Tue, 28 Nov 2023 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Incubation of Distributed Workloads stack in ODH]]></title><description><![CDATA[Starting with ODH 1.6, we have started incubation of the Distributed Workloads stack. It brings the following features: Ease of use with…]]></description><link>https://www.opendatahub.io/blog/2023-05-19-docs-distributedworkloads/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2023-05-19-docs-distributedworkloads/</guid><pubDate>Fri, 19 May 2023 00:00:00 GMT</pubDate><content:encoded>&lt;ul&gt;
&lt;li&gt;Starting with ODH 1.6, we have started incubation of the &lt;a href=&quot;https://github.com/opendatahub-io/distributed-workloads&quot;&gt;Distributed Workloads&lt;/a&gt; stack. It brings the following features:
&lt;ul&gt;
&lt;li&gt;Ease of use with &lt;code&gt;Codeflare SDK&lt;/code&gt;: The Codefare SDK is integrated into the out-of-the-box ODH notebook images and provides an interactive client for data scientists to define resource requirements (GPU, CPU, and memory) and to submit and manage training jobs.&lt;/li&gt;
&lt;li&gt;Batch Management with &lt;code&gt;Multi-Cluster Application Dispatcher (MCAD)&lt;/code&gt;: MCAD manages a queue of training jobs that data scientists submit. MCAD ensures that jobs are not started until all required compute resources are available on the cluster. MCAD ensures that a given team has not requested more aggregate resources than their quota allows, and ensures that highest priority jobs are executed first. Finally, MCAD ensures that all processes necessary to execute a distributed run are scheduled concurrently, meaning that compute cycles aren’t wasted waiting for processes to be scheduled.&lt;/li&gt;
&lt;li&gt;Dynamic scaling with &lt;code&gt;InstaScale&lt;/code&gt;: InstaScale works alongside MCAD to ensure that the OpenShift cluster contains sufficient compute resources to execute a job. InstaScale optimizes compute costs by launching the right-sizes compute instance for a given job, and releasing these instances when they are no longer needed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KubeRay&lt;/code&gt; for management of remote Ray clusters on Kubernetes for running distributed compute workloads&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There is a quickstart for the stack available at &lt;a href=&quot;https://github.com/opendatahub-io/distributed-workloads/blob/main/Quick-Start.md&quot;&gt;https://github.com/opendatahub-io/distributed-workloads/blob/main/Quick-Start.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Refer to &lt;a href=&quot;https://cloud.redhat.com/blog/ai/ml-models-batch-training-at-scale-with-open-data-hub&quot;&gt;https://cloud.redhat.com/blog/ai/ml-models-batch-training-at-scale-with-open-data-hub&lt;/a&gt; for more information!&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[New documentation for Data Science Projects added]]></title><description><![CDATA[New documentation has been added for the new Data Science Projects feature in the ODH dashboard, the documentation covers the following…]]></description><link>https://www.opendatahub.io/blog/2023-02-06-docs-datascienceprojects/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2023-02-06-docs-datascienceprojects/</guid><pubDate>Mon, 06 Feb 2023 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;New documentation has been added for the new &lt;a href=&quot;http://opendatahub.io/docs/working-on-data-science-projects.html&quot;&gt;Data Science Projects&lt;/a&gt; feature in the ODH dashboard, the documentation covers the following items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Creating and importing notebooks&lt;/li&gt;
&lt;li&gt;Collaborating on notebooks using Git&lt;/li&gt;
&lt;li&gt;Working on data science projects&lt;/li&gt;
&lt;li&gt;Model serving on Open Data Hub&lt;/li&gt;
&lt;li&gt;Troubleshooting&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 1.4.1 Release Guide]]></title><description><![CDATA[What is new? Changes included in the Open Data Hub v1.4.1 release: ODH Core Notebook Images - As part of our ODH Core deployment, we are now…]]></description><link>https://www.opendatahub.io/releases/2023-01-24-odh-release-1.4.1-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2023-01-24-odh-release-1.4.1-blog/</guid><pubDate>Tue, 24 Jan 2023 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;Changes included in the Open Data Hub v1.4.1 release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ODH Core Notebook Images - As part of our &lt;a href=&quot;http://opendatahub.io/docs/tiered-components.html&quot;&gt;ODH Core&lt;/a&gt; deployment, we are now providing Jupyter Notebook images with CUDA support by default.  These images will be updated continuously for security and package version updates on a regular cadence&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jupyter Python Notebook&lt;/li&gt;
&lt;li&gt;Jupyter Data Science Notebook - CPU &amp;#x26; CUDA&lt;/li&gt;
&lt;li&gt;Jupyter PyTorch Notebook - CUDA&lt;/li&gt;
&lt;li&gt;Jupyter Tensorflow Notebook - CUDA&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ODH Dashboard v2.5.2  -- &lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard/releases/tag/v2.5.2&quot;&gt;Release Notes&lt;/a&gt; &amp;#x26; &lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard/compare/v2.2.1...v2.5.2&quot;&gt;ChangeLog&lt;/a&gt; since previous ODH release&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://bluejeans.com/s/OSrgirIbbyl&quot;&gt;Demo&lt;/a&gt; from November ODH Community Meeting&lt;/li&gt;
&lt;li&gt;New OdhApplication CustomResources to dynamically add component tiles&lt;/li&gt;
&lt;li&gt;Jupyter/KFNBC now supports opening in a new tab&lt;/li&gt;
&lt;li&gt;Data Science Groupings (&lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard/issues/433&quot;&gt;DSG&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Optional feature -- enabled through the disableProjects (set to false) feature flag found in OdhDashboardConfig&lt;/li&gt;
&lt;li&gt;When enabled it will grant access to users of the Dashboard the ability to create their own projects in OpenShift Console through the Dashboard interface&lt;/li&gt;
&lt;li&gt;Within&apos; projects, you&apos;ll be able to create the same Juypter Notebooks (now called Workbenches within&apos; the DSG as to grow beyond Notebooks in time)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Serving &lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard/issues/626&quot;&gt;UI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following is a list of components that are available in &lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/releases/tag/v1.4.1&quot;&gt;v1.4.1&lt;/a&gt; release of odh-manifests:&lt;/p&gt;
&lt;p&gt;| Technology | Version | Category |
|--|--|--|--|
| &lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt; | v2.5.2 | Dashboard |
| &lt;a href=&quot;https://github.com/opendatahub-io/notebooks&quot;&gt;Open Data Hub Notebook Images&lt;/a&gt; | v1.3.1 | Jupyter |
| &lt;a href=&quot;https://github.com/opendatahub-io/modelmesh-serving&quot;&gt;Model Mesh Serving&lt;/a&gt; | v0.9.0 | AI/ML |
| &lt;a href=&quot;https://github.com/opendatahub-io/data-science-pipelines&quot;&gt;Data Science Pipelines&lt;/a&gt; | v1.2.1 | Data Science Tools |&lt;/p&gt;</content:encoded></item><item><title><![CDATA[ML Pipelines with Kubeflow, Argo and Open Data Hub]]></title><link>https://www.opendatahub.io/videos/ml-pipelines-with-kubeflow-argo-and-open-data-hub/</link><guid isPermaLink="false">https://www.opendatahub.io/videos/ml-pipelines-with-kubeflow-argo-and-open-data-hub/</guid><pubDate>Mon, 28 Nov 2022 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Open Data Hub 1.4.0 Release Guide]]></title><description><![CDATA[What is new? Changes included in the Open Data Hub v1.4.0 release: Refocus on deployment of ODH Core components ODH Dashboard v2.2.1 Lots of…]]></description><link>https://www.opendatahub.io/releases/2022-10-31-odh-release-1.4.0-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2022-10-31-odh-release-1.4.0-blog/</guid><pubDate>Mon, 31 Oct 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;Changes included in the Open Data Hub v1.4.0 release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Refocus on deployment of ODH Core components
&lt;ul&gt;
&lt;li&gt;ODH Dashboard v2.2.1
&lt;ul&gt;
&lt;li&gt;Lots of security fixes -- all end points are secured through the x-forwarded-access-token provided by the oauth container during setup&lt;/li&gt;
&lt;li&gt;Users should not be able to request resources that are not theres (with the exception of admins)&lt;/li&gt;
&lt;li&gt;Various QoL fixes to make the Dashboard smoother&lt;/li&gt;
&lt;li&gt;Allow the admin to view the servers that are running from the Administration view&lt;/li&gt;
&lt;li&gt;Migration to ODH Notebook Controller for lifecycle management of Jupyter notebook servers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Mesh Serving v0.9.0&lt;/li&gt;
&lt;li&gt;Data Science Pipelines - Standalone distribution of Kubeflow Pipelines Tekton v1.3.1 customized for Open Data Hub&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Migration of [Tier 2 components]({{ &apos;/docs/tiered-components.html&apos; | prepend: site.baseurl  }}) to &lt;a href=&quot;https://github.com/opendatahub-io-contrib/odh-contrib-manifests&quot;&gt;odh-contrib-manifests&lt;/a&gt; in the &lt;a href=&quot;https://github.com/opendatahub-io-contrib&quot;&gt;opendatahub-io-contrib&lt;/a&gt; organization&lt;/li&gt;
&lt;li&gt;Deprecation of JupyterHub component - As of ODH 1.4, ODH has replaced JupyterHub multi-user server with ODH Notebook Controller for lifecycle management of Jupyter Notebook servers. JupyterHub is still available for deployment but there will be no further updates and it will be officially deprecated with the release of ODH 1.5&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following is a list of components that are available in v1.4.0 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.0&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/kubeflow/tree/master/components/odh-notebook-controller&quot;&gt;Open Data Hub notebook-controller&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;Notebooks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/modelmesh-serving&quot;&gt;Model Mesh&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.9.0&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/data-science-pipelines&quot;&gt;Data Science Pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;Data Science Tools&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 1.3.0 Release Guide]]></title><description><![CDATA[What is new? Changes included in the Open Data Hub v1.3.0 release: Updated components: ODH Dashboard v2.0 - Major update to the code base…]]></description><link>https://www.opendatahub.io/releases/2022-07-20-odh-release-1.3.0-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2022-07-20-odh-release-1.3.0-blog/</guid><pubDate>Wed, 20 Jul 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;Changes included in the Open Data Hub v1.3.0 release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Updated components:
&lt;ul&gt;
&lt;li&gt;ODH Dashboard v2.0 - Major update to the code base that adds an &lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard/blob/v2.0/docs/admin_dashboard.md&quot;&gt;Admin Dashboard&lt;/a&gt; that supports the administration of:
&lt;ul&gt;
&lt;li&gt;Timeout for culling idle notebooks&lt;/li&gt;
&lt;li&gt;Size of the persistent volume attached to user&apos;s Jupyter notebook pods&lt;/li&gt;
&lt;li&gt;Bring Your Own Notebook(&lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard/blob/v2.0/docs/byon.md&quot;&gt;BYON&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Import custom notebook images and make them available to users in the notebook spawner UI&lt;/li&gt;
&lt;li&gt;Notebook descriptions and the list of major software packages it provides&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;JupyterHub v0.3.7 - Add support to the JH Spawner UI to display the maximum number of available gpus per node&lt;/li&gt;
&lt;li&gt;Elyra v3.9.1 - &lt;a href=&quot;https://elyra.readthedocs.io/en/latest/getting_started/changelog.html#release-3-9-1-06-10-2022&quot;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Support for the integration of Service Mesh with Kubeflow v1.5 on OpenShift (&lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/manifests/v1.5-branch-openshift/openshift/kfdef/kfctl_openshift_v1.5.0_odh.yaml&quot;&gt;kfdef&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following is a list of components that are available in v1.3.0 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.0&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://cloud.redhat.com/blog/introducing-openshift-pipelines&quot;&gt;Openshift Pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;Pipelines&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v4.8.0&lt;/td&gt;
&lt;td&gt;Data Exploration&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.10.11&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v3.9.1&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.8.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.12.5&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.4.1&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/ceph&quot;&gt;Ceph Nano&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.7&lt;/td&gt;
&lt;td&gt;Object Storage&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;NOTE: Any components listed above that do not specify a version will use the default operator version available in OpenShift OperatorHub at the time of deployment&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 1.2.0 Release Guide]]></title><description><![CDATA[What is new? Changes included in the Open Data Hub v1.2.0 release: New components: Pachyderm Model Mesh Serving Updated components: Elyra v…]]></description><link>https://www.opendatahub.io/releases/2022-04-29-odh-release-1.2.0-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2022-04-29-odh-release-1.2.0-blog/</guid><pubDate>Fri, 29 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;Changes included in the Open Data Hub v1.2.0 release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New components:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhpachyderm&quot;&gt;Pachyderm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/model-mesh&quot;&gt;Model Mesh Serving&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Updated components:
&lt;ul&gt;
&lt;li&gt;Elyra v3.6.0 - &lt;a href=&quot;https://elyra.readthedocs.io/en/latest/getting_started/changelog.html#release-3-6-0-02-08-2022&quot;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Superset v1.4.1 - &lt;a href=&quot;https://github.com/apache/superset/blob/1.4.1/CHANGELOG.md&quot;&gt;CHANGELOG&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Components now include the Kubernetes &lt;code&gt;app.kubernetes.io/part-of&lt;/code&gt; label to group applications in the Topology UI view of the OpenShift Developer Console&lt;/li&gt;
&lt;li&gt;Support for the integration of Service Mesh with Kubeflow v1.4 on OpenShift (&lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/manifests/v1.4.0-rc.2-openshift/openshift/kfdef/kfctl_openshift_v1.4.0_servicemesh.yaml&quot;&gt;kfdef&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following is a list of components that are available in v1.2.0 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://cloud.redhat.com/blog/introducing-openshift-pipelines&quot;&gt;Openshift Pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;Pipelines&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v4.8.0&lt;/td&gt;
&lt;td&gt;Data Exploration&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.0&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.10.11&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.3.6&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v3.6.0&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.8.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.12.5&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;*&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.4.1&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/ceph&quot;&gt;Ceph Nano&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.7&lt;/td&gt;
&lt;td&gt;Object Storage&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;NOTE: Any components listed above that do not specify a version will use the default operator version available in OpenShift OperatorHub at the time of deployment&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 1.1.2 Release Guide]]></title><description><![CDATA[What is new? In Open Data Hub v1.1.2 includes updates to: Additional components to support the deprecation of kubernetes apis in OpenShift…]]></description><link>https://www.opendatahub.io/releases/2022-02-09-odh-release-1.1.2-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2022-02-09-odh-release-1.1.2-blog/</guid><pubDate>Wed, 09 Feb 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;In Open Data Hub v1.1.2 includes updates to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Additional components to support the deprecation of kubernetes apis in OpenShift 4.9 (k8s v1.22).
&lt;ul&gt;
&lt;li&gt;ODH components on OCP 4.9+ &lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/odh-manifests/master/kfdef/kfctl_openshift4.9_odh.yaml&quot;&gt;KFDef&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Support for the deployment of Kubeflow v1.4.0-rc2 on OpenShift &lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/manifests/v1.4.0-rc.2-openshift/openshift/kfdef/kfctl_openshift_v1.4.0_odh.yaml&quot;&gt;KFDef&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;For detailed information on Kubeflow v1.4 components please visit the official Kubeflow 1.4 release &lt;a href=&quot;https://blog.kubeflow.org/kubeflow-1.4-release/&quot;&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following is a list of components that are available in v1.1.2 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.openshift.com/learn/topics/ci-cd&quot;&gt;Openshift Pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;Pipelines&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://trino.io/&quot;&gt;Trino&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v355&lt;/td&gt;
&lt;td&gt;SQL Query Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/thriftserver&quot;&gt;Spark Thrift Server&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;SQL Connect&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v4.8.0&lt;/td&gt;
&lt;td&gt;Data Exploration&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.0&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.10.11&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v7.1.1&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.3.5&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.2.4&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.8.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.12.5&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.16.0&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.4.5&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.2&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.2.2&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/ceph/cn-core&quot;&gt;Ceph Nano&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.7&lt;/td&gt;
&lt;td&gt;Object Storage&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 1.1.1 Release Guide]]></title><description><![CDATA[What is new? In Open Data Hub version 1.1.1, the community focused on updating components to support the deprecation of kubernetes apis in…]]></description><link>https://www.opendatahub.io/releases/2022-01-10-odh-release-1.1.1-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2022-01-10-odh-release-1.1.1-blog/</guid><pubDate>Mon, 10 Jan 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;In Open Data Hub version 1.1.1, the community focused on updating components to support the deprecation of kubernetes apis in OpenShift 4.9 (k8s v1.22).&lt;/p&gt;
&lt;p&gt;The following is a list of components that are available in v1.1.1 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.openshift.com/learn/topics/ci-cd&quot;&gt;Openshift Pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;Pipelines&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://trino.io/&quot;&gt;Trino&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v355&lt;/td&gt;
&lt;td&gt;SQL Query Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/thriftserver&quot;&gt;Spark Thrift Server&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;SQL Connect&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v4.8.0&lt;/td&gt;
&lt;td&gt;Data Exploration&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.0&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.10.11&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v7.1.1&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.3.5&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.2.4&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.8.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.12.5&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.16.0&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.4.5&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.2&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.2.2&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/ceph/cn-core&quot;&gt;Ceph Nano&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.7&lt;/td&gt;
&lt;td&gt;Object Storage&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 1.1.0 Release Guide]]></title><description><![CDATA[What is new? In Open Data Hub version 1.1.0 the community focused on hardening the JupyterHub deployment, providing new and improved…]]></description><link>https://www.opendatahub.io/releases/2021-07-13-odh-release-1.1.0-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2021-07-13-odh-release-1.1.0-blog/</guid><pubDate>Tue, 13 Jul 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;In Open Data Hub version 1.1.0 the community focused on hardening the JupyterHub deployment, providing new and improved JupyterHub spawner UI, integrating Open Data Hub dashboard with Openshift oauth, adding &lt;a href=&quot;https://github.com/kubeflow/manifests/tree/v1.3.0/distributions/stacks/openshift&quot;&gt;Kubeflow 1.3 Openshift distribution stack&lt;/a&gt; along with adding new components such as Trino and Openshift Pipelines.  ODH version 1.1.0 also comes with an Operator Level 4 verification indicating &quot;Deep Insight&quot; status after enabling more monitoring and logging.&lt;/p&gt;
&lt;p&gt;An important note to users, ODH 1.1.0 includes two example KFDefs for installation on OpenShift 4.3 (or later) :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ODH components &lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/odh-manifests/v1.1.0/kfdef/kfctl_openshift.yaml&quot;&gt;KFDef&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kubeflow 1.3 &lt;a href=&quot;https://raw.githubusercontent.com/kubeflow/manifests/v1.3-branch/distributions/kfdef/kfctl_openshift_v1.3.0.yaml&quot;&gt;KFDef&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For detailed information on Kubeflow v1.3 components please visit the official Kubeflow 1.3 release &lt;a href=&quot;https://blog.kubeflow.org/kubeflow-1.3-release/&quot;&gt;blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The following is a list of components that are available in v1.1.0 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.openshift.com/learn/topics/ci-cd&quot;&gt;Openshift Pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;Pipelines&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://trino.io/&quot;&gt;Trino&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v355&lt;/td&gt;
&lt;td&gt;SQL Query Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/thriftserver&quot;&gt;Spark Thrift Server&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;SQL Connect&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v4.8.0&lt;/td&gt;
&lt;td&gt;Data Exploration&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.9&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.10.11&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v7.1.1&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.1.5&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.0.5&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.4.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.12.5&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.16.0&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.4.5&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.2.2&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/ceph/cn-core&quot;&gt;Ceph Nano&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.7&lt;/td&gt;
&lt;td&gt;Object Storage&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 1.0.0 Release Guide]]></title><description><![CDATA[What is new? Open Data Hub 1.0.0 brings in many new features and components. Components from Data Catalog such as Hue and Spark ThirftServer…]]></description><link>https://www.opendatahub.io/releases/2021-02-02-odh-release-1.0.0-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2021-02-02-odh-release-1.0.0-blog/</guid><pubDate>Tue, 02 Feb 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;Open Data Hub 1.0.0 brings in many new features and components. Components from Data Catalog such as Hue and Spark ThirftServer are now migrated and included in ODH beta. We added two new Kubeflow components: KF Serving and KF Pipeline on Tekton. JupyterHub now has a new Spawner User Interface that includes more customization options. ODH Argo has also been upgraded to the latest release, v2.12.5.
Legacy option for installing ODH has been removed as the team focuses on ODH Beta where all the new features and bug fixes will be included moving forward.
An important note to users, ODH 1.0 includes three example KFDefs for installation on OpenShift 4.3 (or later) :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ODH components &lt;a href=&quot;https://raw.githubusercontent.com/opendatahub-io/odh-manifests/master/kfdef/kfctl_openshift.yaml&quot;&gt;KFDef&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kubeflow 1.2 &lt;a href=&quot;https://raw.githubusercontent.com/kubeflow/manifests/master/kfdef/kfctl_openshift.v1.2.0.yaml&quot;&gt;KFDef&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kubeflow from master that included KF Serving and KF Pipeline on Tekton &lt;a href=&quot;https://raw.githubusercontent.com/kubeflow/manifests/master/kfdef/kfctl_openshift.master.kfptekton.yaml&quot;&gt;KFDef&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For detailed information on Kubeflow v1.2 components please visit the official Kubeflow 1.2 release &lt;a href=&quot;https://blog.kubeflow.org/release/official/2020/11/18/kubeflow-1.2-blog-post.html&quot;&gt;blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The following is a list of components that are available in v1.0.0 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/thriftserver&quot;&gt;Spark Thrift Server&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;SQL Connect&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/hue&quot;&gt;Hue&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v4.8.0&lt;/td&gt;
&lt;td&gt;Data Exploration&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.9&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.10.11&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v7.1.1&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.1.5&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai/elyra&quot;&gt;Elyra&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.3.3&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.4.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.12.5&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.16.0&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v2.4.5&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v1.2.2&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/ceph/cn-core&quot;&gt;Ceph Nano&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.7&lt;/td&gt;
&lt;td&gt;Object Storage&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 0.9.0 Release Guide]]></title><description><![CDATA[What is new? The Open Data Hub 0.9.0 release provides updates for successfully deploying v1.2 of upstream Kubeflow on OpenShift 4.3 (or…]]></description><link>https://www.opendatahub.io/releases/2020-11-09-odh-release-0.9.0-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2020-11-09-odh-release-0.9.0-blog/</guid><pubDate>Mon, 09 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;The Open Data Hub 0.9.0 release provides updates for successfully deploying v1.2 of upstream Kubeflow on OpenShift 4.3 (or later) and a new Open Data Hub &lt;a href=&quot;https://github.com/opendatahub-io/odh-dashboard&quot;&gt;Dashboard&lt;/a&gt; that lists ODH components that are currently installed and provides links to application dashboards and documentation.&lt;/p&gt;
&lt;p&gt;You can find out more about the components available in the upstream deployment of Kubeflow v1.2 &lt;a href=&quot;https://blog.kubeflow.org/release/official/2020/11/18/kubeflow-1.2-blog-post.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Additionally, Prometheus will now scrape metrics for Kafka and Jupyterhub components and we added the ability to deploy a single [ceph-container] to provide all-n-one object storage for minimal deployments.&lt;/p&gt;
&lt;p&gt;The following is a list of components that are available in v0.9.0 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odh-dashboard&quot;&gt;Open Data Hub Dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.9&lt;/td&gt;
&lt;td&gt;Dashboard&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Alpha&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;7.1.1&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/elyra-ai&quot;&gt;Elyra&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.0.5&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.19.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.7.0&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.37.0&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio&quot;&gt;Radanalytics Spark Operator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.1.0&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.2.2&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/ceph/cn-core&quot;&gt;Ceph Nano&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;v0.7&lt;/td&gt;
&lt;td&gt;Object Storage&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 0.7.0 Release Guide]]></title><description><![CDATA[What is new? The Open Data Hub 0.7.0 release provides updates for successfully deploying v1.0 of upstream Kubeflow on OpenShift 4.2 (or…]]></description><link>https://www.opendatahub.io/releases/2020-07-06-odh-release-0.7.0-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2020-07-06-odh-release-0.7.0-blog/</guid><pubDate>Mon, 06 Jul 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;The Open Data Hub 0.7.0 release provides updates for successfully deploying v1.0 of upstream Kubeflow on OpenShift 4.2 (or later) in the &lt;a href=&quot;https://github.com/opendatahub-io/manifests/tree/v1.0-branch-openshift&quot;&gt;v1.0-branch-openshift&lt;/a&gt; branch of opendatahub-io/manifests.&lt;/p&gt;
&lt;p&gt;Additionally, we have added functionality tests to the CI/CD workflow that will smoke test every pull requests submitted to &lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests&quot;&gt;odh-manifests&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can find out more about the components available in the upstream deployment of Kubeflow v1.0 &lt;a href=&quot;https://www.kubeflow.org/docs/components/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The following is a list of tools that are available in v0.7.0 release of odh-manifests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Alpha&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.0.0&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.17.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.7.0&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.32.0&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio&quot;&gt;Radanalytics Spark Operator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.0.7&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.2.0&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Read More&lt;/h2&gt;
&lt;p&gt;You can read more in a blog post we have published on &lt;a href=&quot;https://developers.redhat.com/blog/2020/08/13/open-data-hub-0-7-adds-support-for-kubeflow-1-0&quot;&gt;developers.redhat.com&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 0.6.1 Release Guide]]></title><description><![CDATA[What is new? Open Data Hub 0.6.1 is now available in OpenShift OperatorHub. It comes with many important bug fixes in the operator, some new…]]></description><link>https://www.opendatahub.io/releases/2020-05-28-odh-release-0.6.1-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2020-05-28-odh-release-0.6.1-blog/</guid><pubDate>Thu, 28 May 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;Open Data Hub 0.6.1 is now available in OpenShift OperatorHub. It comes with many important bug fixes in the operator, some new components and integration with OpenShift CI.&lt;/p&gt;
&lt;p&gt;We have also added READMEs for all the components to make sure it is easy for users to orient themselves in the new manifests structure.&lt;/p&gt;
&lt;p&gt;The components added or (re)enabled in this version are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.1.0&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We have also enabled OpenShift CI for our odh-manifests repository to help us with PR reviews. We have a basic set of tests and are &lt;a href=&quot;https://github.com/orgs/opendatahub-io/projects/4&quot;&gt;expanding the test set&lt;/a&gt; to make sure that your PRs are reviewed faster in a more automated fashion.&lt;/p&gt;
&lt;h2&gt;Read More&lt;/h2&gt;
&lt;p&gt;You can read more in a blog post we have published on &lt;a href=&quot;https://developers.redhat.com/blog/2020/06/02/open-data-hub-0-6-1-bug-fix-release-to-smooth-out-redesign-regressions/&quot;&gt;developers.redhat.com&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 0.6.0 Release Guide]]></title><description><![CDATA[What is new? Open Data Hub 0.6.0 adopted Kubeflow as upstream which lead to conversion from Ansible Operator to Kustomize for component…]]></description><link>https://www.opendatahub.io/releases/2020-05-07-odh-release-0.6.0-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2020-05-07-odh-release-0.6.0-blog/</guid><pubDate>Thu, 07 May 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is new?&lt;/h2&gt;
&lt;p&gt;Open Data Hub 0.6.0 adopted Kubeflow as upstream which lead to conversion from Ansible Operator to Kustomize for component deployment and management.&lt;/p&gt;
&lt;p&gt;All key project repositories moved to Github &lt;a href=&quot;https://github.com/opendatahub-io/&quot;&gt;opendatahub-io organization&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/opendatahub-io/opendatahub-operator/&quot;&gt;https://github.com/opendatahub-io/opendatahub-operator/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/&quot;&gt;https://github.com/opendatahub-io/odh-manifests/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of the components were converted to Kustomize and moved to the above mentioned repositories.&lt;/p&gt;
&lt;p&gt;Another important change is that ODH 0.6 and forward relies on Operator Lifecycle Manager to install and manage some of the components to avoid duplication of operator deployment manifests which are already available through OLM.&lt;/p&gt;
&lt;p&gt;The following is a list of tools are available in ODH 0.6.0:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Deployment Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/airflow&quot;&gt;Airflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Alpha&lt;/td&gt;
&lt;td&gt;Workflow Management&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.0.0&lt;/td&gt;
&lt;td&gt;Monitoring Dashboards&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/kafka&quot;&gt;Kafka Strimzi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.17.0&lt;/td&gt;
&lt;td&gt;Distributed Streaming&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.7.0&lt;/td&gt;
&lt;td&gt;Workflow Engine&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.32.0&lt;/td&gt;
&lt;td&gt;Monitoring&lt;/td&gt;
&lt;td&gt;OLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio&quot;&gt;Radanalytics Spark Operator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.0.7&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/opendatahub-io/odh-manifests/tree/master/superset&quot;&gt;Apache Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;td&gt;Manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Read More&lt;/h2&gt;
&lt;p&gt;You can read more in a blog post we have published on &lt;a href=&quot;https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/&quot;&gt;developers.redhat.com&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[GPU-enabled notebooks in Open Data Hub]]></title><description><![CDATA[GPU-enabled notebooks in Open Data Hub Open Data Hub 0.5.0 introduced support for utilizing NVIDIA GPUs from Jupyter notebooks. GPU are…]]></description><link>https://www.opendatahub.io/blog/2020-02-20-gpu-notebooks-3.11-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2020-02-20-gpu-notebooks-3.11-blog/</guid><pubDate>Thu, 20 Feb 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;GPU-enabled notebooks in Open Data Hub&lt;/h2&gt;
&lt;p&gt;Open Data Hub 0.5.0 &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/-/blob/v0.5.0/docs/enabling-gpu-aicoe-jupyterhub.adoc&quot;&gt;introduced support for utilizing NVIDIA GPUs&lt;/a&gt; from Jupyter notebooks. GPU are hardware accelerators that dramatically increase the performance of model training and inference for deep learning applications such as image classification, medical healthcare diagnosis, and autonomous vehicles, to name a few. These hardware accelerators can efficiently perform complex matrix multiplication operations at a rate that far exceeds standard CPU-based operations. The Open Data Hub operator and its GPU notebook support is designed to work seamlessly across OpenShift 3 and OpenShift 4 deployments.&lt;/p&gt;
&lt;h2&gt;Enablement by the Operator&lt;/h2&gt;
&lt;p&gt;There has been an evolution to GPU enablement in OpenShift that has progressed from privileged pods to special SELinux security policies to finally a model that shields the pod from any particular security context considerations. The Open Data Hub operator has &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/-/blob/v0.5.1/deploy/crds/opendatahub_v1alpha1_opendatahub_cr.yaml#L27&quot;&gt;configuration settings&lt;/a&gt; that instruct the JupyterHub notebook spawner as to which pod specification should be applied.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;none (current default for OpenShift 4 deployments)&lt;/li&gt;
&lt;li&gt;selinux (3.11 mode)&lt;/li&gt;
&lt;li&gt;privileged (legacy mode for 3.10 deployments)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;selinux&lt;/code&gt; mode is the most detailed and is dependent on specific installation of SELinux policies as described &lt;a href=&quot;https://github.com/zvonkok/origin-ci-gpu/blob/release-3.11/doc/How%20to%20use%20GPUs%20with%20DevicePlugin%20in%20OpenShift%203.11%20.pdf&quot;&gt;here&lt;/a&gt;. The &lt;a href=&quot;https://blog.openshift.com/how-to-use-gpus-with-deviceplugin-in-openshift-3-10/&quot;&gt;3.10 GPU enablement&lt;/a&gt; is similar but relies on the GPU pods running privileged containers.&lt;/p&gt;
&lt;h2&gt;3.11 issue&lt;/h2&gt;
&lt;p&gt;Data scientists sometimes include various Python packages in their notebook images and pods, some of which include process and task distribution libraries. Those types of libraries typically are designed to run in non-Kubernetes platforms and may rely on access to IPC constructs that are typically not used outside container communication within a pod. However, if a notebook pod has been scheduled to a node that has multiple GPU devices available, there is certainly nothing precluding using such a distribution mechanism locally.&lt;/p&gt;
&lt;p&gt;The community recently discovered an issue with the &lt;code&gt;selinux&lt;/code&gt; mode of GPU notebook enablement in OpenShift 3.11. In the spawned Jupyter notebook pod, there are two containers, nbviewer and the notebook container. When a Open Data Hub user tried to initialize the dask.distributed library in their GPU notebook, they would encounter the following error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  File &quot;/opt/app-root/src/miniconda/envs/pytorch/lib/python3.7/multiprocessing/synchronize.py&quot;, line 59, in __init__
    unlink_now)
FileNotFoundError: [Errno 2] No such file or directory
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It&apos;s not apparent from the stacktrace but the underlying issue is that two key directories for IPC, &lt;code&gt;/dev/shm&lt;/code&gt; and &lt;code&gt;/dev/mqueue&lt;/code&gt;, receive SELinux labels from Docker that are inconsistent with the rest of the container file system. Thus, they are not writable by the dask.distributed IPC library at initialization.&lt;/p&gt;
&lt;p&gt;Workarounds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;a href=&quot;https://docs.openshift.com/container-platform/3.11/crio/crio_runtime.html&quot;&gt;CRI-O instead of Docker&lt;/a&gt; for the container runtime. Ad-hoc testing indicated that the problem didn&apos;t occur with CRI-O.&lt;/li&gt;
&lt;li&gt;Use privileged GPU pods instead. Note that this provides elevated capabilities for a pod and should only be done after serious consideration of your own security requirements.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, there is a workaround that could be applied in the &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/-/tree/v0.5.1/roles/aicoe-jupyterhub#modifying-jupyterhub-server-behavior&quot;&gt;JupyterHub custom ConfigMap&lt;/a&gt;. The goal is to modify the security context for the GPU pod spec such that we ensure that a common MCS category is applied to all the containers in the pod.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  jupyterhub_config.py: |
    from kubernetes.client import V1Capabilities, V1SELinuxOptions
    spawner = c.OpenShiftSpawner
    def mcs_selinux_profile(spawner, pod):
      # Apply profile from singleuser-profiles
      apply_pod_profile(spawner, pod)
      if spawner.gpu_mode and spawner.gpu_mode == &quot;selinux&quot; and \
           spawner.extra_resource_limits and &quot;nvidia.com/gpu&quot; in spawner.extra_resource_limits:
        # Currently a bug in RHEL Docker 1.13 whereby /dev IPC dirs get inconsistent MCS
        pod.spec.security_context.se_linux_options = V1SELinuxOptions(type=&apos;nvidia_container_t&apos;,level=&apos;s0&apos;)
      return pod
    spawner.modify_pod_hook = mcs_selinux_profile
kind: ConfigMap
metadata:
  name: jupyterhub-mcs
&lt;/code&gt;&lt;/pre&gt;</content:encoded></item><item><title><![CDATA[Is There a Place For Distributed Storage For AI/ML on Kubernetes?]]></title><link>https://www.opendatahub.io/videos/is-there-a-place-for-distributed-storage-for-aiml-on-kubernetes/</link><guid isPermaLink="false">https://www.opendatahub.io/videos/is-there-a-place-for-distributed-storage-for-aiml-on-kubernetes/</guid><pubDate>Sun, 16 Feb 2020 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Open Data Hub 0.5.1 Release Guide]]></title><description><![CDATA[What is included? Open Data Hub 0.5.1 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub…]]></description><link>https://www.opendatahub.io/releases/2020-02-16-odh-release-0.5.1-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2020-02-16-odh-release-0.5.1-blog/</guid><pubDate>Sun, 16 Feb 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is included?&lt;/h2&gt;
&lt;p&gt;Open Data Hub 0.5.1 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub is a meta-operator that can be installed on Openshift Container Platform 3.11 and 4.x.&lt;/p&gt;
&lt;p&gt;The following is a list of tools added to Open Data Hub in this release:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;JupyterHub&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Radanalytics &lt;a href=&quot;https://github.com/radanalyticsio/spark-operator&quot;&gt;Spark Operator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.0.5&lt;/td&gt;
&lt;td&gt;Operator for managing Spark cluster on OpenShift&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Apache &lt;a href=&quot;https://github.com/apache/incubator-superset&quot;&gt;Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can review the release notes for components added in the previous v0.5.0 release &lt;a href=&quot;https://opendatahub.io/news/2019-12-17/odh-release-0.5-blog.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AICoE-JupyterHub&lt;/h2&gt;
&lt;p&gt;AICoE-JupyterHub now has support for greater customization of the JupyterHub deployment and Spark cluster resources. In this release, we added&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support for modifying the &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/-/tree/master/roles/aicoe-jupyterhub&quot;&gt;JupyterHub server behavior&lt;/a&gt; via a custom JupyterHub config&lt;/li&gt;
&lt;li&gt;Update the kubespawner library to version 0.11.1&lt;/li&gt;
&lt;li&gt;Support for specifying the resource requests and limits of the cpu/memory allocated to the Jupyter notebook user spark cluster nodes using the default JupyterHub &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/-/tree/master/roles/aicoe-jupyterhub#additional-information&quot;&gt;Singleuser profile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Radanalytics Spark Operator&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Support for specifying the resource requests and limits of the cpu/memory allocated to the Spark cluster master and worker nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Apache Superset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Resolve issues related to connecting to the Data Catalog Thrift Server&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 0.5.0 Release Guide]]></title><description><![CDATA[What is included? Open Data Hub 0.5.0 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub…]]></description><link>https://www.opendatahub.io/releases/2019-12-16-odh-release-0.5-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2019-12-16-odh-release-0.5-blog/</guid><pubDate>Tue, 17 Dec 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is included?&lt;/h2&gt;
&lt;p&gt;Open Data Hub 0.5.0 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub is a meta-operator that can be installed on Openshift Container Platform 3.11 and 4.x.&lt;/p&gt;
&lt;p&gt;The following is a list of tools added to Open Data Hub in this release:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;JupyterHub CUDA GPU Images and Notebooks&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Support for building CUDA GPU Images and GPU Notebook&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Apache &lt;a href=&quot;https://github.com/apache/incubator-superset&quot;&gt;Superset&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.34.0&lt;/td&gt;
&lt;td&gt;Data Exploration and Visualization Tool&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Data Catalog (&lt;a href=&quot;https://gethue.com/&quot;&gt;Hue&lt;/a&gt;, &lt;a href=&quot;https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html&quot;&gt;Spark Thrift Server&lt;/a&gt;, Hive Metastore)&lt;/td&gt;
&lt;td&gt;Hue 4.4.1 &amp;#x26; Spark Thrift Server 2.4 &amp;#x26; Hive Metastore 1.2.1&lt;/td&gt;
&lt;td&gt;Deployment of Hue, Spark Thrift Server and Hive Metastore to simplify querying data lakes using Spark SQL language&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://argoproj.github.io/argo/&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.4.2&lt;/td&gt;
&lt;td&gt;Container native workflow engine&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can review the release notes for components added in the previous v0.4.0 release &lt;a href=&quot;https://opendatahub.io/news/2019-09-16/odh-release-0.4-blog.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;AICoE-JupyterHub CUDA GPU Images and Notebooks&lt;/h2&gt;
&lt;p&gt;AICoE-JupyterHub now has support for accessing NVIDIA GPUs from Jupyter notebooks. In this release, we added&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Documentation on how to &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/v0.5.0/docs/enabling-gpu-aicoe-jupyterhub.adoc&quot;&gt;enable GPUs nodes&lt;/a&gt; in your OpenShift cluster&lt;/li&gt;
&lt;li&gt;Support for building CUDA GPU images and notebooks as part of the component deployment process&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can test these new features by following the Data Engineering and Machine Learning &lt;a href=&quot;https://gitlab.com/opendatahub/data-engineering-and-machine-learning-workshop&quot;&gt;workshop&lt;/a&gt;. The &lt;a href=&quot;https://gitlab.com/opendatahub/data-engineering-and-machine-learning-workshop/blob/master/source/notebooks/tf-training-serving.ipynb&quot;&gt;tf-training-serving&lt;/a&gt; contains a demonstration of how you can create Openshift Jobs to access a cluster GPU.&lt;/p&gt;
&lt;h2&gt;Apache Superset&lt;/h2&gt;
&lt;p&gt;Apache &lt;a href=&quot;https://github.com/apache/incubator-superset&quot;&gt;Superset&lt;/a&gt; is a data exploration and visualization tool. Instructions for deploying and creating an example database &amp;#x26; chart are available in &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/v0.5.0/docs/deploying-superset.adoc&quot;&gt;Deploy Superset Setup&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Data Catalog (Tech Preview)&lt;/h2&gt;
&lt;p&gt;The Data Catalog is a set of components with which you can run Data Exploration on your Data Lake. These components are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hive Metastore&lt;/em&gt; to store metadada information about the Hive tables&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Spark SQL Thrift server&lt;/em&gt; to expose a ODBC/JDBC endpoint to interact with the Hive Tables&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Hue&lt;/em&gt; to view S3 object store, connect to Spark SQL Thrift server and run queries, as well as create dashboards.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on the Data Catalog, please review the Data Catalog &lt;a href=&quot;https://opendatahub.io/news/2019-12-15/data-catalog-in-odh.html&quot;&gt;announcement&lt;/a&gt; and &lt;a href=&quot;https://opendatahub.io/docs/advanced-tutorials/data-exploration.html&quot;&gt;tutorial&lt;/a&gt;. The Data Catalog is currently designated as &quot;Tech Preview&quot; as we enable support for additional features available in Hue.&lt;/p&gt;
&lt;h2&gt;Argo&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://argoproj.github.io/&quot;&gt;Argo&lt;/a&gt; has been updated to version 2.4.2. &lt;a href=&quot;https://argoproj.github.io/&quot;&gt;Argo&lt;/a&gt; is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.  It is useful for defining workflows using containers, running computer intensive jobs, and orchestrating DAG container pipelines natively on Kubernetes.&lt;/p&gt;
&lt;p&gt;To learn more about deploying Argo in Open Data Hub, please visit &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-argo.adoc&quot;&gt;link&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Data Catalog in Open Data Hub]]></title><description><![CDATA[Data Catalog In the Open Data Hub v0.5.0 release, we introduced Data Catalog. It is a set of components to
read data stored in Data Lakes…]]></description><link>https://www.opendatahub.io/blog/2019-12-15-data-catalog-in-odh/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2019-12-15-data-catalog-in-odh/</guid><pubDate>Sun, 15 Dec 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Data Catalog&lt;/h2&gt;
&lt;p&gt;In the Open Data Hub v0.5.0 release, we introduced Data Catalog. It is a set of components to
read data stored in Data Lakes, create tables and query them in a SQL-like style. You can find
below a picture of the simplified architecture of Data Catalog:&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/60e9aa6c842cc77de50b3edd58e23da9/b97f6/Data_Catalog_Simplified_Diagram.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 55.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACW0lEQVR42p1TX2hSYRT/nBaK0AbRU616qFUjYlG9Loigl1aD4UuNWOjYiNXTeukPF4KGUdjYw2KlczTFde838yXCUbjKdIWJuZZuc05zbTpd6tXrnF7vieu0yYqgfvDj43c43/l+3zkchEogSVLIEwAE6xEQlljFq5e2KdkLm+fGe0+geyEXuc7T+S3SbfWEb74JMudDzfXt4ZbDR9DfQBDrxcbsvv0TM7H0VJgB8weaI+4D3FZyMO5muVAW4PNilnEO9J8GhITFixaCEM/29soCfX1y74O7BxECgWQr0yyRQAdC6QZ/0nPgUyCd9SUynAbT+S2iHCtAOXbAQOd9iTTr8Cfzdm94wx2t0ez4oVZH4RkJSW3/NYQsovpDjL/xJEBNTfwRwPPaiZk46wym4LUzymnwCqipGLxyxjhnkIaPc3HW7g5tFJzXasWzSuUZ/52ec5EnPfv4GCYbVLYJ8fzQUF1rBmDP5SuJXO3uHDTJkqw3ShemV5KFlos0u3PXWqFVQefXoOSQIIhin3QjI406jG+VH6Eo1XGMH2ssBBItrXr2yq+mU0dPAFyQ5zjPcgG8URbaOvNcwzGAS50pJglzdb+my58Gna6NIsmxcsHBwaenSJKyEgSxjdfjrmCTxb3UZfWGFfbp5fZ3XxY7rF/DCov7e9fbyYWzfxqqBCFUXaGlCKHtpfhm8L8Sof9FuS3YaFTi0VGGwpjGGNMUxgzGRlVlTiUEJf6my8lGo/GewWBY1ev1qeHh4ZRer8+YTKaHxTUAqPpnp/wWmc1mqcPhqObpcrmkm539BMXiWIb6C6vEAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Data Catalog architecture&quot;
        src=&quot;/static/60e9aa6c842cc77de50b3edd58e23da9/5a190/Data_Catalog_Simplified_Diagram.png&quot;
        srcset=&quot;/static/60e9aa6c842cc77de50b3edd58e23da9/772e8/Data_Catalog_Simplified_Diagram.png 200w,
/static/60e9aa6c842cc77de50b3edd58e23da9/e17e5/Data_Catalog_Simplified_Diagram.png 400w,
/static/60e9aa6c842cc77de50b3edd58e23da9/5a190/Data_Catalog_Simplified_Diagram.png 800w,
/static/60e9aa6c842cc77de50b3edd58e23da9/b97f6/Data_Catalog_Simplified_Diagram.png 958w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These are the components that are part of Data Catalog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hive Metastore, responsible for maintaining the table metadata created by the user to query the data stored in Ceph/S3&lt;/li&gt;
&lt;li&gt;Spark SQL Thrift server to enable an endpoint where clients can connect using an ODBC/JDBC connection&lt;/li&gt;
&lt;li&gt;Cloudera Hue as a Data Exploration tool to explore the Data Lake, create tables and query them. You can&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;also create dashboards using the tables managed by Hive Metastore&lt;/p&gt;
&lt;p&gt;In the next section we&apos;ll cover each component in details.&lt;/p&gt;
&lt;h2&gt;Hive Metastore&lt;/h2&gt;
&lt;p&gt;The Hive Metastore component is part of Apache Hive and stores all metadata related to Hive tables. A Hive table
is a special structure where you can expose your data, whether they are unstructred, semi-structured or
structured data, as a table. That way you can use SQL syntax to query this data directly from where the data
is stored without the need to use ETL workflows to insert your data into a relational or NoSQL Database.&lt;/p&gt;
&lt;p&gt;One of the advantages of using Hive tables is you don&apos;t need to write a full ETL (Extract-Transform-Load) workflow
to have your data available to read from traditional SQL clients. It is possible to partition the data according to
your needs too.&lt;/p&gt;
&lt;p&gt;With Hive Metastore, you can catalog your data in order to extract the best from your Data Lake.&lt;/p&gt;
&lt;h2&gt;Spark SQL Thrift Server&lt;/h2&gt;
&lt;p&gt;The main feature in Spark SQL Thrift Server is to use the power of Spark SQL and Dataframes to query the data from a
Data Lake, by creating a query plan and run in a Spark cluster to extract all information needed.&lt;/p&gt;
&lt;p&gt;In order to use the Spark SQL features, Thrift Server exposes a ODBC/JDBC endpoint so clients like SquirrelSQL, Tableau,
Superset or any SQL client can connect to it and issue SQL statements using raw data stored in your Data Lake.&lt;/p&gt;
&lt;h2&gt;Cloudera Hue&lt;/h2&gt;
&lt;p&gt;Cloudera Hue is a Data Exploration tool where you can explore your Data Lake for the files stored and issue SQL statements
in a Hive instance. Data Catalog combines Hive Metastore and Spark SQL Thrift Server to have all Hive features.&lt;/p&gt;
&lt;p&gt;With Hue, you can explore the raw data from the Data Lake, create a set of hive tables and expose them to Data Scientists
to create models based on that data.&lt;/p&gt;
&lt;h2&gt;Sample Use Cases&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Data Exploration: You can explore your Data Lake to look at the data structure, as well as creating a table structure to&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;query them.
2) Data Catalogging: After exploring the data and creating tables based on them, you can document the data by creating metadata
against it. With that, you can share with Data Scientists the tables so they can understand what are the features they need
to create their models.&lt;/p&gt;
&lt;h2&gt;Further improvements&lt;/h2&gt;
&lt;p&gt;With Cloudera Hue in the Data Catalog architecture, it is possible to create visualizations with the data too. As for now, this
feature is disabled as some other components are required to make it work. The Open Data Hub team will evalute the option to include
this Hue feature in Data Catalog. Superset can be a good replacement for Data Visualization tasks, or Jupyterhub.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub videos available on the OpenShift YouTube channel]]></title><description><![CDATA[Open Data Hub video tutorials are now available on the  OpenShift youtube channel under the  AI/ML on OpenShift  playlist.]]></description><link>https://www.opendatahub.io/blog/2019-11-04-openshift-youtube-announcement/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2019-11-04-openshift-youtube-announcement/</guid><pubDate>Mon, 04 Nov 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Open Data Hub video tutorials are now available on the &lt;a class=&quot;external-link&quot; href=&quot;https://www.youtube.com/user/rhopenshift&quot; target=&quot;_blank&quot;&gt;&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt; OpenShift&lt;/a&gt; youtube channel under the &lt;a class=&quot;external-link&quot; href=&quot;https://www.youtube.com/playlist?list=PLaR6Rq6Z4Iqcg2znnClv-xbj93Q_wcY8L&quot; target=&quot;_blank&quot;&gt;&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt; AI/ML on OpenShift &lt;/a&gt; playlist.&lt;/p&gt;
&lt;ul&gt;
&lt;li/&gt;&lt;a class=&quot;external-link&quot; href=&quot;https://www.youtube.com/watch?v=-T6ypF7LoKk&amp;t=2s&quot; target=&quot;_blank&quot;&gt;&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt; Installing Open Data Hub on OpenShift 4.1&lt;/a&gt;
&lt;li/&gt;&lt;a class=&quot;external-link&quot; href=&quot;https://youtu.be/d6X1xvDXewM&quot; target=&quot;_blank&quot;&gt;&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt; Uploading data to Ceph via command line&lt;/a&gt;
&lt;li/&gt;&lt;a class=&quot;external-link&quot; href=&quot;https://youtu.be/662FccIWeOE&quot; target=&quot;_blank&quot;&gt;&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt; Fraud Detection Using Open Data Hub on Openshift&lt;/a&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[OpenShift Commons Gathering at ODSC West]]></title><description><![CDATA[Want to learn more about open source AI, Open Data Hub, and other data and AI workloads on Kubernetes and OpenShift? Curious about AI DevOps…]]></description><link>https://www.opendatahub.io/blog/2019-11-15-openshift-commons-announcement/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2019-11-15-openshift-commons-announcement/</guid><pubDate>Tue, 15 Oct 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Want to learn more about open source AI, Open Data Hub, and other data and AI workloads on Kubernetes and OpenShift? Curious about AI DevOps? Join us October 28 at the &lt;a class=&quot;external-link&quot; href=&quot;https://commons.openshift.org/gatherings/San_Francisco_2019.html&quot; target=&quot;_blank&quot;&gt;&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt; OpenShift Commons Gathering&lt;/a&gt; co-located with ODSC/West.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 0.4.0 Release Guide]]></title><description><![CDATA[What is included? Open Data Hub 0.4.0 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub…]]></description><link>https://www.opendatahub.io/releases/2019-09-16-odh-release-0.4-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2019-09-16-odh-release-0.4-blog/</guid><pubDate>Mon, 16 Sep 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is included?&lt;/h2&gt;
&lt;p&gt;Open Data Hub 0.4.0 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub is a meta-operator that can be installed on Openshift Container Platform 3.11 and 4.&lt;/p&gt;
&lt;p&gt;The following is a list of tools added to the Open Data Hub in this release:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Open Data Hub Operator&lt;/td&gt;
&lt;td&gt;0.4.0&lt;/td&gt;
&lt;td&gt;Meta Operator Application management&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://argoproj.github.io/argo/&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.3.0&lt;/td&gt;
&lt;td&gt;Container native workflow engine&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://strimzi.io/&quot;&gt;Strimzi Kafka Operator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.11.1&lt;/td&gt;
&lt;td&gt;Distributed streaming platform&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Open Data Hub &lt;a href=&quot;https://opendatahub.io&quot;&gt;AI-Library&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;Machine learning as a service&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can review the release notes for components added in the v0.3.0 release &lt;a href=&quot;https://opendatahub.io/news/2019-06-27/odh-release-0.3-blog.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Argo&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://argoproj.github.io/&quot;&gt;Argo&lt;/a&gt; is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.  It is useful for defining workflows using containers, running computer intensive jobs, and running CI/CD pipelines natively on Kubernetes.&lt;/p&gt;
&lt;p&gt;To learn more about deploying Argo in the Open Data Hub, please visit &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-argo.adoc&quot;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Strimzi Kafka Operator&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://strimzi.io&quot;&gt;Strimzi&lt;/a&gt; provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations. &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; is a distributed streaming platform for publishing and subscribing records as well as storing and processing streams of records.&lt;/p&gt;
&lt;p&gt;Strimzi is based on Apache Kafka 2.0.1 and consists of three main components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Cluster Operator
Responsible for deploying and managing Apache Kafka clusters within OpenShift or Kubernetes cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Topic Operator
Responsible for managing Kafka topics within a Kafka cluster running within OpenShift or Kubernetes cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;User Operator
Responsible for managing Kafka users within a Kafka cluster running within OpenShift or Kubernetes cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To learn more about using the Strimzi operator to deploy an Apache Kafka cluster in the Open Data Hub, please visit &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-kafka.adoc&quot;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Open Data Hub AI-Library&lt;/h2&gt;
&lt;p&gt;AI-Library is an open source collection of AI components that allows for rapid prototyping of ideas. AI-Library enables users to work with machine learning models without worrying about infrastructure issues, model complexity or any data science expertise.&lt;/p&gt;
&lt;p&gt;To learn more about deploying AI-Library models in the Open Data Hub, please visit &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-ai-library.adoc&quot;&gt;link&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Source Sentiment Analysis Modeling - An advanced approach]]></title><description><![CDATA[Table of Contents Introduction What is Sentiment analysis Our Approaches Open-source NLP tools for sentiment analysis Deep Learning for…]]></description><link>https://www.opendatahub.io/blog/2019-08-21-sentiment-analysis-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2019-08-21-sentiment-analysis-blog/</guid><pubDate>Wed, 04 Sep 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#what-is-sentiment-analysis&quot;&gt;What is Sentiment analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#our-approaches&quot;&gt;Our Approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#open-source-nlp-tools-for-sentiment-analysis&quot;&gt;Open-source NLP tools for sentiment analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#deep-learning-for-sentiment-analysis&quot;&gt;Deep Learning for sentiment analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#bert-for-sentiment-analysis&quot;&gt;BERT for sentiment analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#infrastructure-requirements-for-building-the-service&quot;&gt;Infrastructure requirements for building the service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#continuous-improvement-and-incorporating-feedback&quot;&gt;Continuous improvement and incorporating feedback&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Traditionally, sentiment analysis and opinion mining are techniques used by organizations to ascertain customer sentiments about their products, brand and services. With growing availability of opinion-rich resources like customer service surveys, reviews, blogs and engagement metrics there is an ever increasing opportunity for actively using the available resources in order to understand customer sentiments and opinions.&lt;/p&gt;
&lt;p&gt;This also poses a challenge when it comes to narrowing down to a system which can generalize to a wide variety of use cases. Also much of the Cognitive and Artificial Intelligence (AI) systems need infrastructures to support their training, development and maintenance.&lt;/p&gt;
&lt;p&gt;This blog outlines our approach to improving the sentiment analysis service at Red Hat, through continuous learning and discusses how we evolved the system to learn, adapt and produce desirable outcomes for a multitude of use cases. Through this blog, we also demonstrate the approaches we took and our results on comparative analyses of the approaches.&lt;/p&gt;
&lt;h2&gt;What is Sentiment analysis&lt;/h2&gt;
&lt;p&gt;Sentiment Analysis is a technique to identify emotional states and polarity from human language. These tasks, pertaining to extracting sentiments from a piece of text, often about a certain topic fall under the field of Natural language processing (NLP). NLP is a range of computational techniques for the automatic analysis and representation of human language. It is closely linked to the fields of artificial intelligence (AI) and computational linguistics.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/8713e36eb27a077e25dca160be786e2c/ecf19/sentiment_analysis_example.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 57.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB0UlEQVR42k2R64vUMBTF5///CxwExcesooiPwijrh1VWBF3XkZGdFGdqm6S5SZtX213TRzqDTMPC/j6dXDice3Nm4zhaa401xpqjMKaua2NNXdXGHCdhGLQxBuosVYhWu863s67rtNbW2mJCCCGl3O22q9UqTVMpZVmWACAKYY1VSv5IXn+4vHeOntT/ylnf924CADDGhBBKCWMMY8wY45wDsCzLCiG6rmudMx+jbHFfv3k2aDkbJsqJEG6tVUqFZ3mHoiz8MFx/fs9ePqyWL7xRRzNjLEkSAOCcE0JCoBCCTeQTAEAIEUJcpNHyYv4JLSp3u7b3XmuVpn+TJMnzPNgIIRjjPM8pJZW13vvWOfIt+hnNk7OTtprWds5hnBFCsiyjlBJCJgMNgWmWYowppQDgnPsdvzs7n39fPW1upmTvvTFGKaW1llKGm7XW5g5KqbquBz98EdHb3YPT/MS05TG5aRoAYIwJITjnobCiKDg/1hY+L2zRNM1us/z19dHm8vnNdTnz3iulgmG9XiOE4jjebv8ghK6uruI43mw2CCEpJXCupOxOo/Lx3L1ajKqcjROHw2G/3w/D0Pd9KC+I/pZxHPeHwzj6DkiPk56m+679D4y6UaWWJKmAAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;An example of sentence classification of tweets into positive and negative sentiments.&quot;
        src=&quot;/static/8713e36eb27a077e25dca160be786e2c/5a190/sentiment_analysis_example.png&quot;
        srcset=&quot;/static/8713e36eb27a077e25dca160be786e2c/772e8/sentiment_analysis_example.png 200w,
/static/8713e36eb27a077e25dca160be786e2c/e17e5/sentiment_analysis_example.png 400w,
/static/8713e36eb27a077e25dca160be786e2c/5a190/sentiment_analysis_example.png 800w,
/static/8713e36eb27a077e25dca160be786e2c/ecf19/sentiment_analysis_example.png 948w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are various approaches that can be taken to build a sentiment analysis service. One of the most intuitive approaches is that of building of a textual analysis system. Such a system comprises of analyzing a piece of text based on the terms appearing in the text and building a rule-based or fact-based system around categorizing pieces of text into various sentiment classes. The other approach, which is mainly how most of the state-of-the-art systems work is a machine learning and deep learning based approach. In this approach, rather than teaching a system to make a decision based on a set of rules, we expose the system to a variety of examples and train it to learn from the examples and then draw predictions on the basis of those.&lt;/p&gt;
&lt;p&gt;In the following sections, we go over details of the techniques mentioned above and explain how we leveraged each of the above techniques, into iteratively improving our system at Red Hat and narrowing down to a deep learning based approach in the end, and how it fits our use-case.&lt;/p&gt;
&lt;h2&gt;Our Approaches&lt;/h2&gt;
&lt;p&gt;In this section, we describe in detail, the approaches we took for sentiment analysis and the results we got at each step, and how we improved the sentiment analysis service iteratively.&lt;/p&gt;
&lt;h4&gt;Open-source NLP tools for Sentiment Analysis&lt;/h4&gt;
&lt;p&gt;Our initial approach to sentiment analysis was building a service which can detect sentiments from  customer reviews using three open-source NLP tools, Stanford CoreNLP,  Vader Sentiment Processor and TextBlob.&lt;/p&gt;
&lt;p&gt;The CoreNLP model is built using a Recurrent Neural Network trained on a tree based corpus called &apos;Stanford Sentiment Treebank&apos; which is a fully labeled parse tree that allows for a complete analysis of the compositional effects of sentiment in language. On the other hand Vader is a lexicon based and rule based approach at sentiment analysis mainly targeted towards social media text. TextBlob is a classifier based approach, where a Naive Bayes classifier is used for a multiclass classification.&lt;/p&gt;
&lt;p&gt;The initial approach involved sentiment calculation using the CoreNLP Annotator with an additional validation step performed on the annotated results by passing sentences which are classified as negative by CoreNLP, through Vader and Textblob for negative sentiment validation(nsv).&lt;/p&gt;
&lt;p&gt;The results of the above approach are shown in the figure below. We are displaying F1-scores, which is calculated from Precision and Recall values for each sentiment. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Recall is the ratio of correctly predicted positive observations to the all observations in actual class.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 407px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/815f5017dd6dfe7dfe2e366daf297e08/0ff56/metrics-for-initial-approach.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 71.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA90lEQVR42r2SsYqEMBRF/f9fCBYKYmcrCFZBYxPEwkJBEQujotGQGBlZzTLrwjLF7DoM7Cledx73XZ6mlKKU1nVNCJFSMsaEEPu+qwtoSqk0TX3fRwhhjBFCSZKs63r7leM4vuUwDA3DsG3bsizTNF3XZYxNT6CUzvN8RtPUG9zlfd8/vjjDnPOq3Pd9EARVVTVNUxRFWZbLslzZcpchhAAAXdcxxpzz86pnTNPEGPsprG3bPM+zLBuGYdu2P6t+aPutwjjnXdeN4yil5JwLIV6Q4zh2HMfzvCiKkiQhhLwQG0Ko67plWQCAMAyllJTSf3mS45Hr8iezmR/q6UaTPAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;F-1 scores of initially developed service using open source NLP tools&quot;
        src=&quot;/static/815f5017dd6dfe7dfe2e366daf297e08/0ff56/metrics-for-initial-approach.png&quot;
        srcset=&quot;/static/815f5017dd6dfe7dfe2e366daf297e08/772e8/metrics-for-initial-approach.png 200w,
/static/815f5017dd6dfe7dfe2e366daf297e08/e17e5/metrics-for-initial-approach.png 400w,
/static/815f5017dd6dfe7dfe2e366daf297e08/0ff56/metrics-for-initial-approach.png 407w&quot;
        sizes=&quot;(max-width: 407px) 100vw, 407px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.&lt;/p&gt;
&lt;p&gt;A core limitation of this method was, that it ignores the fact that we are in fact, treating the 3 models as competitive or alternate NLP models to treat a sentence, whereas, there is a significant difference between the 3 models in terms of their accuracy at sentence classification and their ability  to truly identify a certain label positive, negative, or neutral.
In order to improve this service, we created a service, which is an ensemble of the 3 models, Stanford CoreNLP, Vader and TextBlob and created an ensemble model by giving weighted scores to the 3 services. So, we  give scores to the three services, based on the precision values of the 3 services when they are used independently to calculate sentiment on a training dataset.&lt;/p&gt;
&lt;p&gt;The results of this approach are shown in the figure below. We are displaying the F1-scores of the ensembled model when tested on a test dataset.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 424px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/2d811eb1c3af2fe7ef69f62c59e203ee/1cfa9/metrics-for-weighted-approach.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 68.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAABfUlEQVR42p2SS0vDQBSF+xMVn+CiKxcuxRYsVuhSFKog+FgLgtS6sYuCtnUhWLqogqhV6zO1bdKZTDLJZDLpJCPxlYpagoePWc053Hu4ESGEzZiiQoOYtuMQSi1me54nQigihIBP0sl+/q5SlS9vn6rn0tmF0+pwGfq0IVdU1+m6PfqK9s3a8nZnIKaMz8kjs8pYQhlNNIfiPsPx5uBMcyIJrupQ1yAAEEIAgGmagdnzPM55EOy6IvzY/td33M+Xu4LzgD+y3sY2cLVeq7caMkYNBFpYo10ndGFbuZvJ1MP0YiOWfpxekmJpObEKkms+8+ud+Ip2eoWZjTUdY6zrOqU0MNPyublzQPZKZrZIskUzWzAyhx/sFoztPHl4sbrMIsSyLEIIY6xn5//KNzOHtUEHYUwdx7IpNcy3wnroUxi6vqtmcrV86fmofH9caUhSt68454FZ38jCaApOLSjRFExtYoSQhpCqot+kqioh5NvOvWOFPOwfR/JO6MJeAfV/DwdcCeoxAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;F-1 scores of service  using model stacking approach with weighted precision scores&quot;
        src=&quot;/static/2d811eb1c3af2fe7ef69f62c59e203ee/1cfa9/metrics-for-weighted-approach.png&quot;
        srcset=&quot;/static/2d811eb1c3af2fe7ef69f62c59e203ee/772e8/metrics-for-weighted-approach.png 200w,
/static/2d811eb1c3af2fe7ef69f62c59e203ee/e17e5/metrics-for-weighted-approach.png 400w,
/static/2d811eb1c3af2fe7ef69f62c59e203ee/1cfa9/metrics-for-weighted-approach.png 424w&quot;
        sizes=&quot;(max-width: 424px) 100vw, 424px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the results in the above figure, as compared to the results of the primary approach as shown in the previous figure, there were improvements in the service by using a model stacking method with weighted average.&lt;/p&gt;
&lt;h4&gt;Deep Learning for Sentiment Analysis&lt;/h4&gt;
&lt;p&gt;As Young et al. point out in, for decades NLP problems were tackled using common machine learning approaches like SVMs, and logistic regression trained on very high dimensional and sparse features. However, recently, studies by Mikolov and Socher show, neural network based approaches are showing superior results in NLP using dense vectors and word embeddings. Deep learning is an application of artificial neural networks that allows computational models that are composed of multiple layers to learn representations of data.&lt;/p&gt;
&lt;p&gt;Andrew Ng., a pioneer in the field of machine learning points out -
“for most flavors of the old generations of learning algorithms … performance will plateau. … deep learning … is the first class of algorithms … that is scalable. … performance just keeps getting better as you feed them more data.”&lt;/p&gt;
&lt;p&gt;Much after neural networks were introduced in 1958, in the late 1990s, the research community started to lose interest in neural networks mainly because they were regarded as only practical for “shallow” neural networks ( with one or two layers), since training a deep neural network is computationally very  expensive. However, recently, deep learning has produced state-of-the-art results in many application domains, starting from computer vision, speech recognition and most recently, natural language processing.&lt;/p&gt;
&lt;p&gt;The renaissance of neural networks as pointed out by Lei Zhang, can be attributed to factors like availability of computing power due to advances in hardware eg. GPUs, availability of huge amounts of training data and introduction of learning intermediate representations.&lt;/p&gt;
&lt;p&gt;And today, these very factors help us develop better systems and more advanced models to tackle NLP tasks.&lt;/p&gt;
&lt;p&gt;In order to understand how deep learning can be applied to customer reviews, we need to first think about the kind of data we are feeding into the system, which in our case is text, or an input string. But in order to apply mathematical operations like dot products, matrix multiplications etc, instead of a string input, we need to convert each word in the string into a vector. These vectors can be created in a way such as to represent the context, meaning and semantics. And, in order to create these vectors, which are called, word embeddings, we use word vector generation models like Word2Vec and GloVe. This gives us an embedding matrix, that contains vectors for each distinct word in the training corpus. We started with building a Recurrent Neural Network model (RNN) with Long short term memory units for sentiment analysis. A recurrent neural network is a bit different from a traditional feedforward neural network. The main difference is the temporality of an RNN and thus they are ideal for sequential data like sentences and text. LSTM ( Long short term memory ) units are modules that we can place inside of RNNs. At a high level, they make sure we are able to encapsulate long term dependencies in the text.&lt;/p&gt;
&lt;h4&gt;BERT for Sentiment Analysis&lt;/h4&gt;
&lt;p&gt;A big challenge in NLP is the shortage of training data. Most modern deep learning techniques benefit from large amounts of training data, that is, in hundreds of thousands and millions. However, since NLP is a very diversified field with many distinct tasks, there is a shortage of task specific datasets. This includes customer service review datasets, survey datasets, operational data etc.&lt;/p&gt;
&lt;p&gt;To close this gap, a technique called Bidirectional Encoder Representation from Transformers (BERT) was developed for training general purpose language representation models using enormous amount of unannotated text on the web ( known as pre-training) by researchers at Google.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/216f578c98816a091fda8482f66c3846/f43e4/bert-architecture.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 49.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABrUlEQVR42lXQzWvbMBgG8Pzluw4GY4dtUMooDFborRuDwQ6DscMOoXUgru3IsfVtWXL8JTkmWEui4jgp7Hd8Xx49kmbu4ng87i8Oh4P7336/t9b+O3kZzqaYc05rjTFmlBJCtNYv80lVVesTSqm1dtqOYWOMlFIplQvBVcFyyYuNkLLrOudc3/dCCCklYzRNU5nLBK/XCGitZ9OpcRwTjJmU4uf36vpd+eUaPvnNqb/VmhDCOZdSbpQiEP76fffn77dCbcZwXddhGGKEuFTyx9fi/evq5gN68htjpudwzrMsk0ollK0gDDwfPC6LzebcHAYBQijP8zXCQRwDiAhjSklrrRAijmOMMWUsu/1UX72t7m8JwWVZnpuTJKGUCiE4YxghRkd1XTvnmqZZrVYQwkwI9vmKv3nFbj6iJCmragyXZQkAoJRmWRZFURiMIIRt207XHn/kJErhIgh9AAil52al1Hw+j6IoTVPPW3iet1h4QRA0zdjctq3v+wCANE395dJfLh8fHqIwLIpiDA/D0Pf9drs1xvQXxphhGJxz1tqu66Ztc6K1bppmt9s9A5eQHePvKab0AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Pre - training general purpose language representation models on huge unannotated text and Fine tuning on customer review dataset&quot;
        src=&quot;/static/216f578c98816a091fda8482f66c3846/5a190/bert-architecture.png&quot;
        srcset=&quot;/static/216f578c98816a091fda8482f66c3846/772e8/bert-architecture.png 200w,
/static/216f578c98816a091fda8482f66c3846/e17e5/bert-architecture.png 400w,
/static/216f578c98816a091fda8482f66c3846/5a190/bert-architecture.png 800w,
/static/216f578c98816a091fda8482f66c3846/f43e4/bert-architecture.png 1120w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The pre-trained model can then be fined tuned on smaller datasets for performing sentiment analysis.&lt;/p&gt;
&lt;p&gt;The advantage that this gives us is that, by using the principles of transfer learning, the universal properties that a language model possesses when exposed to a huge amount of data could be used in our case where there is a lack of annotated datasets.&lt;/p&gt;
&lt;p&gt;We use pre-trained BERT models and fine-tune it on our comparatively smaller dataset. This allows us to take advantage of feature extraction that happens in the front layers of the network without developing that feature extraction network from scratch.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 520px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/b0fdf78f029ace461ffb53c746289734/69902/bert-results.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABTElEQVR42qXSz06DMBzA8b2vJzUuanTxGTwYM2YCzOHMHsCDiQefQFnGdGwwYPJvG7S0RSgVzBibOxr95Zumh34ObVor/jG1oihSSmEUAQhBBEO4ahEEEKEQwmUYYkJ2AaXUMIwkSSocQ+iqY1MZGgPFVlV7pE5eZWv4ZgwUrd/3NT0LwToGIPH84UDBGFc4fHpW9w714wvt6HydXm+s13Jzph6cVu2fTOqNUJ+mjFU4p5RhnGGyjSKc4W2Ekfgr/ollWZ7nG1zOLx9p93B55zTxALDmc9P3vCBwguVoZumuawfLif1hLeafabpiJWCMybIMIawwUt4tvjNtd3WhY97em+3u+EbUBckQ7zS+MxUlR+otHh4JQoQQgrHv+3Ecb/CLPLu8dlqiwwl2k7ebvMsJDsc7zTJOmF21bKkXAYAQiqIIY0wprfCfP8k3a1gf3gz3maEAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;F-1 Scores of Positive and Negative sentiments run on BERT based model&quot;
        src=&quot;/static/b0fdf78f029ace461ffb53c746289734/69902/bert-results.png&quot;
        srcset=&quot;/static/b0fdf78f029ace461ffb53c746289734/772e8/bert-results.png 200w,
/static/b0fdf78f029ace461ffb53c746289734/e17e5/bert-results.png 400w,
/static/b0fdf78f029ace461ffb53c746289734/69902/bert-results.png 520w&quot;
        sizes=&quot;(max-width: 520px) 100vw, 520px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;Results using the BERT model&lt;/h5&gt;
&lt;p&gt;The BERT based model performs better than our RNN based approach and predicts positive sentiments with a 0.92 F1 Score and 0.73 F1 Score. This showed significant improvements as compared to all our previous approaches. Another advantage of this approach is that, it lets us get a single sentiment annotation for an entire customer review, by encapsulating very long term dependencies in text.  Moreover, the process of building the service highly improves for the BERT based deep learning model by using GPUs for training the model. We discuss this in more detail in the next section.&lt;/p&gt;
&lt;h2&gt;Infrastructure requirements for building the service&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/9b0f4dce45608e7884779d0a477ef85e/c5bb3/cpu-vs-gpu.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 66.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAACCUlEQVR42k2STW/TQBCG+///BBIg9VCVcqRwQFBKKyrogaZpmzhxvN7NOuv12vv9aUPs0PJqTjPzrOad2aNhGEJw3hvvTZgi2H14k6KNwfopM5bif5lhGI5SSgqfJ3ISyVkkp4m8c/hEguOhOaueXpPlm4G9l+DY4ZOensHZK5q9Heipxh9SintY0hlD32pwwdD3Bl5q+pNXPxi6JMVFtflCwFexu6nBRfF4DhaftqvPcPGxLm+GoR9hZbYVhVtCaItwrbTXxtKGk7rFFd1iqrTthG5a+bTI1xvIpW0Y7/sJlrLvk9aKd90w9M7ZLFuu1ysACliWRbFZrTLOu7u737e3vxZPjw8PcylFSmkPG2NSSm3bEkJijCmloiju7+/zPF8ul/P5HELonLsadX19NZvNrLUHWEoZY2SMUUq999ZapVTTNBjjqqratpVSOudSSn7UfsdKvcBSSgjher0WQhhjpJRd1xFCdrvdBPN/6kZxzg+wEKIcBUcJIaZZQgiTC6UUxnhqAADkeU4IOdxZa22MgRBmWdZ1nfdeCDHBEy+EcM5hjBGCZQmapjHGxDjeWSklpUQIlWXJGHPOSSm11pTSuq6nqrWWcz7ySGv94tkYI4QAACCEGGPee63183Occ2OM934aHiEUQtBa7+Hxb++VUoqjvPfOuWfDMUbnnPd+6pl2HkL46/kP9zrW9Tk61mwAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;CPU vs GPU and comparison of training time&quot;
        src=&quot;/static/9b0f4dce45608e7884779d0a477ef85e/c5bb3/cpu-vs-gpu.png&quot;
        srcset=&quot;/static/9b0f4dce45608e7884779d0a477ef85e/772e8/cpu-vs-gpu.png 200w,
/static/9b0f4dce45608e7884779d0a477ef85e/e17e5/cpu-vs-gpu.png 400w,
/static/9b0f4dce45608e7884779d0a477ef85e/c5bb3/cpu-vs-gpu.png 680w&quot;
        sizes=&quot;(max-width: 680px) 100vw, 680px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, as we fine-tune a sentiment analysis model, with pre-trained BERT parameters by training it on a large annotated dataset, we introduce large computational operations in terms of memory. To compute the data efficiently, we need infrastructures that can handle the computation processes in minimum time.&lt;/p&gt;
&lt;p&gt;We need GPUs to solve these problems since neural networks heavily rely on floating point matrix multiplication. Also, deep learning algorithms require a lot of data to train, thus they need large memory and high speed networks to complete in a reasonable amount of time.&lt;/p&gt;
&lt;p&gt;Although, less expensive as compared to the pre-training procedure, the fine-tuning step when run on a CPU with 4 cores and 8 Gigabytes of RAM takes about 3 hours to train. The same model when trained using an NVIDIA P100 GPU with 3584 cores and 16 Gigabytes of Memory, trains the model in only about 6 minutes.&lt;/p&gt;
&lt;p&gt;This speeds up the overall workflow of training, testing , hyperparameter tuning and thus building of the service.&lt;/p&gt;
&lt;p&gt;Also, for training the models and running predictions we use the Jupyterhub installation of the Open Data Hub which have GPU support. Jupyter notebooks allow running code, documenting, visualization in the same environment which makes the process of training and prototyping more flexible.&lt;/p&gt;
&lt;h6&gt;Links to sample code and model&lt;/h6&gt;
&lt;p&gt;The code used for training and testing and the saved model can be found in this &lt;a href=&quot;https://gitlab.com/opendatahub/sample-models/tree/master/sentiment-analysis&quot;&gt;repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Continuous improvement and incorporating feedback&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/34038dda5305b38ddc6798a4296dd591/07a9c/feedback.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 38.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABiUlEQVR42k2OS2/TQBhF/dNhC+qfQOqiCDZdQCWiLgpUKhJNiKPYHT8mM86M5+FH4vg5tudDaYTEXdzFkY7udQDAWjsMQ991fdfZeYZ/mee5qqrT6dQ0zYVYa+G/OMaYtm0TSnbJnih9aDszDAAwjmNd15RS3/ekUm3XGWOmaSrL8nA4jON4lrXWURRyqdL1Un293T89plIBQFEUhBApZZ4XmMSLx89/3GetszAMoigyxpxlpRTy/UQqvviSX71Jbz+mWgNAWZaUUiEEZewFoefvP+IgmOfZWqu1vvx3hBCe5zHGdoQEUYQJTdN0GAbOmLfdUs73n67zq7fy4Z7rDKydJqt00Q+Tta/LGGPOOcbY8z0cx1JKAMiyLECIMoYWd/jmA/r5QBkbRyPILxbeJ+hbW0uHc+66bpIkm427Wi3X67UQ4iJ73hbH8QahkKcb318tl5nW4uU6eHq3+/2+zrdOXdda6zzPsyy79PF4BICmaYqiOBOtM6WUlGVZWmv7hvfVrqvwOJz+Alkisx3n7H5IAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Feedback system for the sentiment analysis service&quot;
        src=&quot;/static/34038dda5305b38ddc6798a4296dd591/5a190/feedback.png&quot;
        srcset=&quot;/static/34038dda5305b38ddc6798a4296dd591/772e8/feedback.png 200w,
/static/34038dda5305b38ddc6798a4296dd591/e17e5/feedback.png 400w,
/static/34038dda5305b38ddc6798a4296dd591/5a190/feedback.png 800w,
/static/34038dda5305b38ddc6798a4296dd591/c1b63/feedback.png 1200w,
/static/34038dda5305b38ddc6798a4296dd591/07a9c/feedback.png 1440w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The lifecycle of developing AI systems, does not end with building the first iteration of the system. We improve the system and continuously evolve it by making it learn from a feedback system.&lt;/p&gt;
&lt;p&gt;In our Sentiment Analysis project at Red Hat, after deployment of the service, we introduce additional steps of monitoring how our service is performing, capturing feedback from users, and including it back in building our ground truth, our model training and evaluation process.&lt;/p&gt;
&lt;p&gt;For this purpose, we built a feedback system which lets users correctly annotate the captured results from running the service on data for any false predictions that might be observed.&lt;/p&gt;
&lt;p&gt;As we see in the figure above, the results of the feedback annotation are introduced back in the training of the model in the second supervised learning phase, therefore, making our training datasets larger and more robust and enabling us to generate more context specific data which can be introduced in the training phase of the model, thus continually improving the service.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Apache Kafka in Open Data Hub]]></title><description><![CDATA[Apache Kafka  The Open Data Hub is a reference architecture running on OpenShift that incorporates a variety of open source projects to…]]></description><link>https://www.opendatahub.io/blog/2019-08-20-kafka-in-odh/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2019-08-20-kafka-in-odh/</guid><pubDate>Tue, 20 Aug 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Apache Kafka&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 450px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/25c0592eb924c9e89a998334cda60a2a/fc2a6/apache_kafka.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 46.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABiklEQVR42n1STcsBURT2i+ZXmH9gIVKEphQaG5MiU8qUhZQFsx8fySxmQchGVr4XpFixsFAiWTLP2z01E73e99SZe++cc5/7nOccBwBsNhtIkoTlcsmOeL1etpum+eHv/7+Zg31yuRzi8ThkWcblcqEHmP11yTIr/p5HgLvdDjzPYzQaIZVKIRAIENvxeEzgs9kM6/Uah8MBhmGgVqthu90SwPP5/M2QJXg8HsRiMfj9fiQSCUynUyiKgv1+j+PxSACn0wnpdBrVahWtVosAbrcbHo+HLYnjfr8T0Pl8RjAYRKfTIWcmCAIqlQo0TUMmk6EHQqEQCoUCwuEwdF2nfTQatRkTw0ajAbfbTQGv10t6shJdLhcikQixajabSCaTKBaLVDbHccjn81BVldZ2u/2podPpJA1Zt30+H+bzORaLBfr9Pnq9HkqlErrdLiaTCckwHA4xGAxQr9dRLpcpn5Vud1kURWSzWVyvV5u+pct/Hf46NqyTrJzVamXPoTVzrIvvc/ktZq0M8AeHJF8M9xRkggAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Apache Kafka&quot;
        src=&quot;/static/25c0592eb924c9e89a998334cda60a2a/fc2a6/apache_kafka.png&quot;
        srcset=&quot;/static/25c0592eb924c9e89a998334cda60a2a/772e8/apache_kafka.png 200w,
/static/25c0592eb924c9e89a998334cda60a2a/e17e5/apache_kafka.png 400w,
/static/25c0592eb924c9e89a998334cda60a2a/fc2a6/apache_kafka.png 450w&quot;
        sizes=&quot;(max-width: 450px) 100vw, 450px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Open Data Hub is a reference architecture running on OpenShift that incorporates a variety of open source projects to function as an ML-as-a-service platform. Given the huge amounts of data to be ingested and processed, it&apos;s crucial to have a reliable streaming platform. To solve this problem we use Apache Kafka.&lt;/p&gt;
&lt;p&gt;Apache Kafka is an open-source stream-processing software platform. It is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.&lt;/p&gt;
&lt;p&gt;Apache Kafka is a distributed streaming platform. What exactly does that mean?
A streaming platform has three key capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.&lt;/li&gt;
&lt;li&gt;Store streams of records in a fault-tolerant durable way.&lt;/li&gt;
&lt;li&gt;Process streams of records as they occur.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Kafka is generally used for two broad classes of applications:&lt;/p&gt;
&lt;p&gt;Building real-time streaming data pipelines that reliably get data between systems or applications
Building real-time streaming applications that transform or react to the streams of data&lt;/p&gt;
&lt;h2&gt;Strimzi Operator&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 601px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/f046a640ad3345cff5c6538a0cff7bc5/d8f62/strimzi.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 26%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsSAAALEgHS3X78AAABVklEQVR42mMw84jmE9VwbuWUNj/CLGTsykAp4JG32aXrGPO/rGf+H6fIwv9skqaBIHEpLXc/CV1XOwldV08xXQ9nMR1PVxFNF1UJLSctcS03MzFdbyUxHQ89UW03KwkdDy9JbVdrEJ8hv23WhQN3nv/feP7WN4eIgv98yg7hMnre7pI67rcldd3SJHU9zkvqeMyX1PF8KantsUpSx2ONpK7nKSkd952Suh63pfQ86iV13E5J6LjPkdT1mMfQuXTrlhNf/v+fdeb+34SO+f/1PZLcRNScPCW13e6La7knSGi7T5bU9SiQ0Ha/J6ntNkNCx2OFpK77VrCFOm77JXTclkloux2W0PHoktR2bwT5jiO6YWp1yaLteyKbZ9qABGR0PXUldL3spDSchaVB3jT0kpfUcdOQ1HUzBtES2l6a0gYuqhLaLprieq7m4tpujuKaruaiWvYSAPclZFP4fqvoAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Strimzi&quot;
        src=&quot;/static/f046a640ad3345cff5c6538a0cff7bc5/d8f62/strimzi.png&quot;
        srcset=&quot;/static/f046a640ad3345cff5c6538a0cff7bc5/772e8/strimzi.png 200w,
/static/f046a640ad3345cff5c6538a0cff7bc5/e17e5/strimzi.png 400w,
/static/f046a640ad3345cff5c6538a0cff7bc5/d8f62/strimzi.png 601w&quot;
        sizes=&quot;(max-width: 601px) 100vw, 601px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://strimzi.io/&quot;&gt;Strimzi&lt;/a&gt; is based on Apache Kafka, a popular platform for streaming data delivery and processing. Strimzi makes it easy to run Apache Kafka on Kubernetes.&lt;/p&gt;
&lt;p&gt;Strimzi provides three operators:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cluster Operator&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Responsible for deploying and managing Apache Kafka clusters running on a Kubernetes cluster.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Topic Operator&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Responsible for managing Kafka topics within a Kafka cluster running on a Kubernetes cluster.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;User Operator&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Responsible for managing Kafka users within a Kafka cluster running on a Kubernetes cluster.&lt;/p&gt;
&lt;h2&gt;Monitoring&lt;/h2&gt;
&lt;p&gt;Kafka deployed usind ODH Operator comes pre-configured to expose metrics out of the box which are scraped using Prometheus and Visualized using Grafana. This gives us a holistic view of the Kafka cluster&apos;s health and performance.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/a20d16a42c4e4ef06bca0940f4e648b2/3c492/kafka_metrics.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 38.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAC4jAAAuIwF4pT92AAABkklEQVR42nWQS4sTQRRG+1e4mDGpR1dVuqur388EFDLxgT9tEHeDiEKC4GYwE/yHZ+iOi3Hh4nDu/e63ulF5zBiODc2pIP+aEr6klKeM8djSn2qKH55wnzD3+mOz5NUpED4nFN893am6Zj8D+b0nqs+B9lzQXHLKX9lSqi4Z01PPcKkpHj3hIaE6Z9S/525J9RTIv6VUjxnNOdCdS+o/geLBE5mDQu8V8UFh9wr9RjFn8Z1cSD9awocU/8lhD9ebe6cxbyX2bt41ei+JD5r0fUY0pJLKSlon6RNF4ySNlfSpotsoRq/ovV48eUWXXPMuEeTG0IUWfSMRr1bs2i3RrrdsO7MwtTHb1rLrHLvBsm0NU+0WxtoyLjZMnWOoDcFvmKaJNHGsXt8y9B2RWK2ZkeurF9YC8XfXUuOMI1YaJQRaCmIpMErgtMCZGBvHaKUw1hF5K/Du/2QbSTG/JZM0QdIViqHSjJWmCYK6CEzjSOIsG+eIqtxRBkMZ7D9Ui82L2VJmluIFITU4Y7DGsLq5RSvJM68VAUqrDgjJAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Kafka Monitoring&quot;
        src=&quot;/static/a20d16a42c4e4ef06bca0940f4e648b2/5a190/kafka_metrics.png&quot;
        srcset=&quot;/static/a20d16a42c4e4ef06bca0940f4e648b2/772e8/kafka_metrics.png 200w,
/static/a20d16a42c4e4ef06bca0940f4e648b2/e17e5/kafka_metrics.png 400w,
/static/a20d16a42c4e4ef06bca0940f4e648b2/5a190/kafka_metrics.png 800w,
/static/a20d16a42c4e4ef06bca0940f4e648b2/c1b63/kafka_metrics.png 1200w,
/static/a20d16a42c4e4ef06bca0940f4e648b2/3c492/kafka_metrics.png 1300w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Sample Use Cases&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Data Ingestion: Internally we have a Kafka deployment for ingesting data and back it up to Elasticsearch and Ceph S3 for analysis using Logstash and s3 Connector. We have 3 Kafka and 3 zookeeper Brokers of 10 Tb each backed by Persistent Volumes with a peak throughput of around 20k messages per second and a sustained rate of ~12k messages per second.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data Streaming: For credit monitoring we have a kafka deployment which ingests credit data as a producer in near-real time and are consumed and sent to the seldon model-serving layer for risk monitoring and fraud detection.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Monitoring for the Data Hub]]></title><description><![CDATA[Monitoring The Open Data Hub is a reference architecture running on OpenShift that incorporates a variety of open source projects to…]]></description><link>https://www.opendatahub.io/blog/2019-07-02-open-data-hub-monitoring/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2019-07-02-open-data-hub-monitoring/</guid><pubDate>Tue, 02 Jul 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Monitoring&lt;/h2&gt;
&lt;p&gt;The Open Data Hub is a reference architecture running on OpenShift that incorporates a variety of open source projects to function as a ML-as-a-service platform. Given how many different pieces there are in the platform, it is very difficult for devops engineers to get a clear picture of the health of the system. Moreover, it is not possible to make an informed decision regarding hardware requests or SLA’s without understanding how your system is performing. We are addressing this problem by using Prometheus and Grafana as the monitoring solution for the Open Data Hub.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/06a05e4be018ba058fbe205204826b93/3c492/cluster_metrics.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 41%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAC4jAAAuIwF4pT92AAABYUlEQVR42mWRS67bMAxFvYu+WB9LoiQqdhwnr0iATjrqoKPuoftfxCnkOAWKDg4o8vIjSkPNwrl14kH6j21T1lVRTdQa0ZbQbmt8+ZoQ8Twfd4Z1rjxvmXURtq2wLcJ1Fm5L4n6RF6twOQtLi8yaWNqLuds5s86ZKp5vz0+GyTmMdTyj8L02nPM467DG4Q6WVLjkssent24ddQo8tKFJOH2M3LetN7SM1vK7ZH7UiHiPt5Ye986SveeRA5tMiHW7FqzlizH8jJFfLdHihDl98LldGew4clYlThPGjGgplJwxpxMpBFqtTNYQvOMyzwTv6TVNlXOteDsiMZJFuK0rQ0+IfsK7vp7ZC8K+lqE/Rz+nEJGYiCHsMW/sfoGe64w9hhhu15WhNaWqUDVTilCqUA9KSXtcW95tbZmS0+6Xmg5daOdCyoFn/xSthXnRv+K7+U6Vo/gY+B6m/6KtkKQ3/Mof4InjRUVmFHkAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Cluster Monitoring Dashboard&quot;
        src=&quot;/static/06a05e4be018ba058fbe205204826b93/5a190/cluster_metrics.png&quot;
        srcset=&quot;/static/06a05e4be018ba058fbe205204826b93/772e8/cluster_metrics.png 200w,
/static/06a05e4be018ba058fbe205204826b93/e17e5/cluster_metrics.png 400w,
/static/06a05e4be018ba058fbe205204826b93/5a190/cluster_metrics.png 800w,
/static/06a05e4be018ba058fbe205204826b93/c1b63/cluster_metrics.png 1200w,
/static/06a05e4be018ba058fbe205204826b93/3c492/cluster_metrics.png 1300w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Prometheus&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 115px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/b9191029a9b5c20a32e1004eaf2ecd12/e907b/prometheus_logo.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 99.13043478260869%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD20lEQVR42m1VS2glRRTtOI4fFFFxdCEyC0Ww61a/PIPxA5KFILrQvLrVnTfJGwRFBAMqBh0ZFwYX4saNC92IoKL4Ie7EzwjOSPwg4yKIMDoQxrzXVf1LzCSZxLxPv5Jb3W8myUxBUUX3rVP3nHP7tuOUwzjOkAmCfbRfOOxdowIQGuE9hfCbRmgpZH8rAT9oyd8Ma3C/Mc6QPVeeuWiYWecyWlWNj2vB/lqfqJhz9YpZm/B6qeTHMh+66xOe2apX+pnP+yGyr3WN3UVnvtgLStnR2hLsrbUJz9CBSEKX9qFgJ5eld28ioZNInkcIW6Fg+XrdM6nkGy3B/QLU2bc7M4R3OpPDRiPrxBKMQpYkkm/HyL+j95mEP2IJCxGyX84GnlGCdVLJ89WALnWDXaCxz5/eOlQxSkBPC+gRtVTy72MfZlMf5ihGI7wRSf7KsuTv/mdjWU8JyDOfm5WAr54p6TvNwL0xlXwpkZwAc42sS+CZz99uCnZPKOEpy0CyhxPJHsgkf4guLGKBJLHSZBI+tYCJz6dJfIWsSwEUeLbI8FF6v9wYvY7WpRq7Payz2ywjhHmKITYKoU/nUsm3deC6TiL5sTV74wUKWkC0NMVvKDSetRr/PjKy/8zYwasKvdkRK1GZRChYb6teMWkAM06EEBII3UQa/kuCIztpwYwzNKg3GsfHxi4vAN2pgjbrFOdYj1gmEj5xyNWIqCLQg7zfqJLIi38G7hU7S8ruS8AU2QvmcNVs1r1SJtazLJGdIED7sMxSKwlzGiELBX98VymUwFRmWrjfasFOKQFzgyxJ00jCjwQYk8NFluxcWPPuNM+M7A/HCwP2jmZw39UaqwdpHwl4sjSnTZRjZJ87qc+Pl6Z01q39/MTpR4YPrI5Xro+Ed7MOhg/snGpy5KYkcK9t1tzRZQmpZSZYh0xKJX/VyQLvpc3SsRChvxJQycCCFvCTRvh571TIfg0Fm898+McaWNZjLCFPJ/jdTjOo3JogpJEsatCWTcBH9ui3o4kUxkQIR4v6hW2inUj+1fmgBPnL7clhm2XxxbDXlYCGFu6Luya6M0rA8+E4qysBX1KGMUK+EvD2Ys0ddQa1ZkElfGZBBbSpdW3UK22N8FpYc2ciZEcUukebgk1nkn/Yn6qaZZ93YwldkiussWfP98ZBnVlnkX1MAYmEblxI0BgEDrqSRv7+StEQepnkeTjOp3d+URe6dZlpKNh0hJBSR9k8VMlbAj5IpIctwZ6IJcznjaqxfRJhQUn+4EVgl/oFLD4Gt2jBp0Nk32gBG2RUSmYhqFDARwqhdvq5O6681C/gf0psnXUy4ltjAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Prometheus Logo&quot;
        src=&quot;/static/b9191029a9b5c20a32e1004eaf2ecd12/e907b/prometheus_logo.png&quot;
        srcset=&quot;/static/b9191029a9b5c20a32e1004eaf2ecd12/e907b/prometheus_logo.png 115w&quot;
        sizes=&quot;(max-width: 115px) 100vw, 115px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://prometheus.io/&quot;&gt;Prometheus&lt;/a&gt; is an open-source monitoring and alerting tool that was developed at SoundCloud to address the challenges posed by the micro-service oriented architecture they had developed. We were drawn to Prometheus as it offers scalability, easy configurability for targets and alerting, and is simple to run in a variety of different environments. This was especially important to us as any monitoring solution we use should be able to scale out to a large number of instances. In addition to scalably gathering metrics, Prometheus has a powerful query language (imaginatively called PromQL) that makes it easy for the layman to gain insight into the metrics via slicing and various transformations.&lt;/p&gt;
&lt;h2&gt;Grafana&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 115px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/6e29c3d79f9653ee277a30b1ce3049e3/e907b/grafana_logo.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 108.69565217391303%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAAACXBIWXMAAC4jAAAuIwF4pT92AAAE0ElEQVR42pVUe2xTVRi/yMCwsXZd1/WxrrO7Xe9pR2YiCUFNnEbFPwz+IVlIbztWYGw+mHMsgCywhQAikclDXmPsmSGjiDDAKDgyxiSoWYIhi39M+WcICkLvXe/70X7m3q6l+Er8ki/3nPOd73d+93e+72DYf1ikCpsNGDbrjzeIXCpAjEZJ1K2tQ1XVbFi4cA5UVmbp3oY98bdkbVHfmPrqjs3WYtMhXyesLAcIl8N0EK1LgiZj/8tG2rAsmvQtZ4M+gSIJlQqgeCyIfo+SxEfUcuLFB0G0hCZRNx1ES1Kk0sk06XuFDqJdNIm6ogG0hQoRK+kAuilV+yEW9MXVGr/OEFYtAKivACVcDnwIKVBXAcoK/+BjgJNBj0Gs9t2D1QsAasrjiXA5aABitV/brAEKVIC4TgdQzzSJ9tMk0RsNED9xIZ+qSRENEMOPAU5U+edOB9FNmkRABZBAa04igQ/5gCaJvvvVyAuAzcqUY7LB8yRFEi9Nk2hMA6UDRD2WKS4X9PVpASpAKJpWco0fYiTalL4wzbFHoBnjWUwIHRFr/CK93IOnaT6s8T5P1xBxzdU1PmDC3lN6YmVllsZGGz+o9TijYWIpFS57kwqjZxIz65oxYeI8v5o4mj7pQYPHQNeX3ebf9gLzVhnwdfizOmAk+QfRurLN/DteFdb7Adb7E9JaAqg6zzW6vmyRXlq1bq/wrvdKOoFvwJ/j3/eAvM4DzHv4DY15ZEaOWCO+CVoRMI34GLMWb2Ya8G1sIz4JLQSITR7uYSP+mi5bM77zkYYb3bvUllKAzTgwG9w9qV+52+CxyC2lAvdBaVfmTU41OecxG9wR2OoBqcV9a7zOnh1r9vh1hrFW1/HENjcwm10SfOgGdovrkxSgstX9KtvqkoW2kqe0+Z06ezY0OedpY7bFZWdbS37VcoQ2d62ewO1wboc9JcDucCrMjmIF2kuA3V48kAIUdrleFncW37u1sdSoM+zA5swwzdJBtztPwkE3iDuL9T7H1IPO29LeIuB2O3hut0OO73MC1+6YhH5rTqrexD1Fv0h77GsyLymlL9deNAhdLmB3O75IXvdeW0g8ZGehrwjinY4496ldhU4HSIdtK1IsuX1Fi7n99gn+sG0ff8jWzB+xvaCDH7DMFw/af4ajDuAP2B4ye4sqkn3cYcG5Dmu70GWdlHutCbnbBnKv9b50zLogXcg9JXlch20Z22F9neopydPlOGZtgAE78J2FIpy2g3zCdi7ZBTNt9dvH1hzlRMENOFkI6mAhyIOW+1x//rLM7kjt5Qcs1fKgRRSPWxLyZxaR77fcEfstGx9tnBGbG8xfLETMinjKDMoZcyIxVABCxPy9cMq8VThtqhU/L1gnRvKH1bMFCfWMOQ7nC4A9WbBKO+ixfk/Vl3jOjMQhkywOmRLCkElSvjSpcCkf4GI+wNf5AN8kx9I5k6pcMIF41rQ+3e+RjEc3MjORhw2LYCQP1GFjHMaMIA8bOOmiQUiMGAGuanOjJF0yxLSYMmw8roONY3P++hphqQV2NNsuX50/JV3JvSuPGPqky3lPy6OGRdLI/Frxcm6Tei13Cn40ANw0gPptblh/2UeSNfmvBuPZ9ui4yfhPB8IPuYR8PeeC+l3OVzCBzc2MafYnCty2582BJsYAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Grafana Logo&quot;
        src=&quot;/static/6e29c3d79f9653ee277a30b1ce3049e3/e907b/grafana_logo.png&quot;
        srcset=&quot;/static/6e29c3d79f9653ee277a30b1ce3049e3/e907b/grafana_logo.png 115w&quot;
        sizes=&quot;(max-width: 115px) 100vw, 115px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The front end we are using to visualize these metrics is &lt;a href=&quot;https://grafana.com/&quot;&gt;Grafana&lt;/a&gt;. Grafana has robust integrations in place with data sources such as Prometheus, Elasticsearch, InfluxDB, and Graphite. It also has support for a variety of different types of visualizations both out of the box and through the &lt;a href=&quot;https://grafana.com/docs/administration/cli/&quot;&gt;grafana-cli&lt;/a&gt;. The Grafana community has a large number of dashboards available &lt;a href=&quot;https://grafana.com/dashboards&quot;&gt;here&lt;/a&gt; which can easily be imported into any running instance of Grafana.&lt;/p&gt;
&lt;h2&gt;Additional Configuration&lt;/h2&gt;
&lt;p&gt;The monitoring deployment is configured out of the box to scrape ODH spark metrics as well as all pod level metrics that are being exposed on default ports. These metrics can be used to create Grafana dashboards for a quick picture of the health of your system. If your system requires any additional configuration for the alerting or custom endpoints to scrape metrics from, all you need to do is modify the prometheus.yml config map in  prometheus-objects.yaml to add a custom job. A custom job could look something like the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- job_name: &apos;My Custom Kafka Metrics&apos;
         metrics_path: &apos;/metrics&apos;
         scheme: http
         static_configs:
           - targets:
             - http://my-custom-kafka-endpoint.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/a20d16a42c4e4ef06bca0940f4e648b2/3c492/kafka.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 38.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAC4jAAAuIwF4pT92AAABkklEQVR42nWQS4sTQRRG+1e4mDGpR1dVuqur388EFDLxgT9tEHeDiEKC4GYwE/yHZ+iOi3Hh4nDu/e63ulF5zBiODc2pIP+aEr6klKeM8djSn2qKH55wnzD3+mOz5NUpED4nFN893am6Zj8D+b0nqs+B9lzQXHLKX9lSqi4Z01PPcKkpHj3hIaE6Z9S/525J9RTIv6VUjxnNOdCdS+o/geLBE5mDQu8V8UFh9wr9RjFn8Z1cSD9awocU/8lhD9ebe6cxbyX2bt41ei+JD5r0fUY0pJLKSlon6RNF4ySNlfSpotsoRq/ovV48eUWXXPMuEeTG0IUWfSMRr1bs2i3RrrdsO7MwtTHb1rLrHLvBsm0NU+0WxtoyLjZMnWOoDcFvmKaJNHGsXt8y9B2RWK2ZkeurF9YC8XfXUuOMI1YaJQRaCmIpMErgtMCZGBvHaKUw1hF5K/Du/2QbSTG/JZM0QdIViqHSjJWmCYK6CEzjSOIsG+eIqtxRBkMZ7D9Ui82L2VJmluIFITU4Y7DGsLq5RSvJM68VAUqrDgjJAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Kafka Dashboard&quot;
        src=&quot;/static/a20d16a42c4e4ef06bca0940f4e648b2/5a190/kafka.png&quot;
        srcset=&quot;/static/a20d16a42c4e4ef06bca0940f4e648b2/772e8/kafka.png 200w,
/static/a20d16a42c4e4ef06bca0940f4e648b2/e17e5/kafka.png 400w,
/static/a20d16a42c4e4ef06bca0940f4e648b2/5a190/kafka.png 800w,
/static/a20d16a42c4e4ef06bca0940f4e648b2/c1b63/kafka.png 1200w,
/static/a20d16a42c4e4ef06bca0940f4e648b2/3c492/kafka.png 1300w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub 0.3 Release Guide]]></title><description><![CDATA[What is included? Open Data Hub 0.3 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub…]]></description><link>https://www.opendatahub.io/releases/2019-06-27-odh-release-0.3-blog/</link><guid isPermaLink="false">https://www.opendatahub.io/releases/2019-06-27-odh-release-0.3-blog/</guid><pubDate>Thu, 27 Jun 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is included?&lt;/h2&gt;
&lt;p&gt;Open Data Hub 0.3 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub is a meta-operator that can be installed on OpenShift Container Platform 3.11 and 4. The following is a list of tools included in this release:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Open Data Hub Operator&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;Meta Operator Application management&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://seldon.io&quot;&gt;Seldon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.2.7&lt;/td&gt;
&lt;td&gt;Model Serving and Metrics Tool&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://jupyter.org/hub&quot;&gt;JupyterHub&lt;/a&gt; with GPU Support&lt;/td&gt;
&lt;td&gt;3.0.7&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.2.3&lt;/td&gt;
&lt;td&gt;Query &amp;#x26; ETL frameworks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TwoSigma &lt;a href=&quot;http://beakerx.com/&quot;&gt;BeakerX&lt;/a&gt; Integration&lt;/td&gt;
&lt;td&gt;1.4.0&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://prometheus.io/&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;System monitoring tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://grafana.com/&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;4.7&lt;/td&gt;
&lt;td&gt;System monitoring tools&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Seldon&lt;/h2&gt;
&lt;p&gt;Seldon is an open source framework that makes it easier to deploy AI/ML models on Kubernetes and OpenShift. The model can be created and trained using many tools such as Apache Spark, scikit-learn and TensorFlow. Seldon also provides metric for Prometheus scraping. Metrics can be custom model metrics or Seldon core system metrics.
To learn more about how to use Seldon as part of Open Data Hub 0.3 please visit &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-seldon.adoc&quot;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;JupyterHub with GPU&lt;/h2&gt;
&lt;p&gt;Open Data Hub release 0.3 will include a deployment of Jupyterhub with GPU support giving  users and Data Scientists  easier access to GPU processing.  Specific documentation will be provided shortly.&lt;/p&gt;
&lt;h2&gt;Apache Spark&lt;/h2&gt;
&lt;p&gt;Apache Spark was previously included in Open Data Hub release 0.2. In release 0.3 we include Prometheus and Grafana monitoring support for Spark metrics. For example notebooks using Apache Spark please refer to the included sample notebooks in the &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-seldon.adoc&quot;&gt;tutorial section&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;BeakerX&lt;/h2&gt;
&lt;p&gt;BeakerX is an extension to Jupyter Notebook that includes tools for plotting, creating tables and forms and many more.&lt;br&gt;
To learn more about how to use BeakerX as part of Open Data Hub 0.3 please visit &lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-seldon.adoc&quot;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Prometheus and Grafana&lt;/h2&gt;
&lt;p&gt;Prometheus and Grafana are widely used open source tools for monitoring clusters and applications on Kubernetes and OpenShift.
Prometheus provides monitoring and alerting tools, it scrapes metrics from all components that expose a REST interface supporting prometheus metrics. Grafana is a visualization tool that is the leader in time series analytics. Users can create Grafana dashboards that display metrics gathered by Prometheus into plots and graphs for analysis. Out of the box, Promotheus will scrap Apache Spark metrics withing Open Data Hub namespace.
To learn more about how to use Prometheus and Grafana as part of Open Data Hub 0.3 please visit
&lt;a href=&quot;https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-monitoring.adoc&quot;&gt;link&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Project Road Map for 2019]]></title><description><![CDATA[What's Next?  The Open Data Hub community has some exciting new features planned for 2019.  Driven by the release of OpenShift 4, we have…]]></description><link>https://www.opendatahub.io/blog/2019-04-29-project-road-map-for-2019/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2019-04-29-project-road-map-for-2019/</guid><pubDate>Mon, 29 Apr 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What&apos;s Next?&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/cdbcc456cae75be2820dbb23264fbb99/54bf4/roadmap_2019.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 41%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABNUlEQVR42o1RbW+CMBj0//+Zfd2yLH7QubEoLwoqQSJIS0UWmCKitHAL3SRzc8suueZJc730uevgn0jTFGEYgjEGIcSvuk5zcMFBCJHioihQ13UrOM9BEEDXdVjTqTT/07AxCTwf2yTFIctRcQFelpeG/gqGqiGiDJUQKI8n8FN53TA77LHcRfCyDVb7GJPQxcNLH8uIYMF8uGwFw7MxpguYzIW7XeN+1EdPVeDFFE7oSV1DaWi7Dm4nA2ixi9HagULn6BoKBpaKp5mGoTPBTe8Oj74J/XUJNXahBDN0tWcotoHhwmr58cNdhmNRXOS2YRFoQJC97eSKebaH4OJCczocITj/uXKDqq5bNqnN7TnGYwOmZcp2v2uqz2xrOVctO+fLr802YJQiJARJkiDP86uNXnv3DiNeYu2XgnroAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Open Data Hub Road Map - 2019&quot;
        src=&quot;/static/cdbcc456cae75be2820dbb23264fbb99/5a190/roadmap_2019.png&quot;
        srcset=&quot;/static/cdbcc456cae75be2820dbb23264fbb99/772e8/roadmap_2019.png 200w,
/static/cdbcc456cae75be2820dbb23264fbb99/e17e5/roadmap_2019.png 400w,
/static/cdbcc456cae75be2820dbb23264fbb99/5a190/roadmap_2019.png 800w,
/static/cdbcc456cae75be2820dbb23264fbb99/54bf4/roadmap_2019.png 1007w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Open Data Hub community has some exciting new features planned for 2019.  Driven by the release of OpenShift 4, we have officially moved to operators.  This will make deployment and management of the services easier for administrators.  Other key highlights on the road map for this year include added support for monitoring services with Prometheus and Grafana, as well as improved machine learning model lifecycle management with tools such as MLflow and Seldon-core.&lt;/p&gt;
&lt;h2&gt;February 2019&lt;/h2&gt;
&lt;h3&gt;Version 0.1 - Initial ODH Release&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Container Platform Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://jupyter.org/hub&quot;&gt;JupyterHub&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.9.4&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x, Minishift 1.3x&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.2.3&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x, Minishift 1.3x&lt;/td&gt;
&lt;td&gt;Query &amp;#x26; ETL frameworks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/ceph/cn&quot;&gt;Ceph-nano&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Latest master&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x, Minishift 1.3x&lt;/td&gt;
&lt;td&gt;Storage&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;a href=&quot;https://gitlab.com/opendatahub/jupyterhub-ansible&quot;&gt;initial launch of ODH&lt;/a&gt; includes an object store, an analytics engine for big data and a platform for spinning up data science notebooks.  This provides everything you need to get started with machine learning on OpenShift.&lt;/p&gt;
&lt;h2&gt;May 2019&lt;/h2&gt;
&lt;h3&gt;OCP 4 and Operator Support with Monitoring&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Container Platform Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Open Data Hub Operator&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Application management&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.seldon.io/open-source/&quot;&gt;Seldon-core&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.2.6+&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Model Serving&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://jupyter.org/hub&quot;&gt;JupyterHub&lt;/a&gt; with GPU Support&lt;/td&gt;
&lt;td&gt;0.9.4&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.2.3&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Query &amp;#x26; ETL frameworks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TwoSigma &lt;a href=&quot;http://beakerx.com/&quot;&gt;BeakerX&lt;/a&gt; Integration&lt;/td&gt;
&lt;td&gt;1.3.0&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Data science tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://prometheus.io/&quot;&gt;Prometheus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;System monitoring tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://grafana.com/&quot;&gt;Grafana&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;4.7&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;System monitoring tools&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The May release of ODH brings a re-design of the deployment to take advantage of &lt;a href=&quot;https://blog.openshift.com/introducing-the-operator-framework/&quot;&gt;Kubernetes operators&lt;/a&gt;!   Seldon-core will provide model-serving and model-monitoring capabilities. In order to better understand system usage and workloads, Prometheus and Grafana are also being targeted with pre-configured metrics and dashboards to monitor ODH.&lt;/p&gt;
&lt;p&gt;Also, JupyterHub will be improved to allow GPUs installed in OpenShift 3.1x to be usable within Jupyter notebooks.  When a user selects a notebook and specifies GPU workloads, the tasks will automatically run on a node that is GPU-enabled and available.&lt;/p&gt;
&lt;h2&gt;August 2019&lt;/h2&gt;
&lt;h3&gt;Data Engineering + Model Lifecycle&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Container Platform Version&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Open Data Hub Operator&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Application management&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://argoproj.github.io/argo/&quot;&gt;Argo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.2.1+&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Orchestration platforms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://mlflow.org/&quot;&gt;MLflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.9.1+&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Model experimentation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://strimzi.io/&quot;&gt;Kafka (Strimzi / AMQ Streams)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.11.2+&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Streaming and enrichment&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server&quot;&gt;Spark SQL Thrift Server&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.2.3+&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Query &amp;#x26; ETL frameworks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hive Metastore&lt;/td&gt;
&lt;td&gt;1.2.1+&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Metadata cataloging&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;http://gethue.com/&quot;&gt;Cloudera Hue&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;4.4+&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Data exploration&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://rook.io/docs/rook/v0.8/ceph-quickstart.html&quot;&gt;Ceph Rook&lt;/a&gt; Operator Integration&lt;/td&gt;
&lt;td&gt;0.9.3&lt;/td&gt;
&lt;td&gt;OpenShift 3.1x and 4.x&lt;/td&gt;
&lt;td&gt;Storage&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Version 0.3 of ODH includes added support for data engineers with Cloudera Hue, Argo, Kafka, and Spark SQL Thrift Server.  Argo is great for managing pipelines and workflows.  Ceph-nano is being replaced by Ceph Rook, the best way to deploy and manage Ceph Storage on OpenShift. For metadata and cataloging of data stored in the Ceph data lake, Hive Metastore will be added.  Spark SQL Thrift Server can be configured to enable SQL access to data stored in the Ceph data lake by leveraging Spark as the processing engine.  Hue will provide an interface for data analysts to query the data lake using metadata in Hive Metastore and the SQL capabilities of Spark SQL Thrift Server.&lt;/p&gt;
&lt;p&gt;For machine learning model lifecycles, MLflow is being added to allow model experimentation.&lt;/p&gt;
&lt;h2&gt;November 2019&lt;/h2&gt;
&lt;p&gt;We haven&apos;t thought that far out yet, but stay tuned!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub Overview]]></title><description><![CDATA[What is the Open Data Hub?  Built on OpenShift, Open Data Hub uses the leading Kubernetes platform to deliver a meta-project that integrates…]]></description><link>https://www.opendatahub.io/blog/2018-12-04-open-data-hub-overview/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2018-12-04-open-data-hub-overview/</guid><pubDate>Tue, 04 Dec 2018 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;What is the Open Data Hub?&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/9084423b13f90e1074daa3b83cfe247e/1cfc2/Open_Data_Hub_Deployment.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 77%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAADYklEQVR42o1U3YsTVxQ/ZvMxk0xmkp1kMzuZJJvsNMkuuyPupWmadrniB47suuLCNeAuBj8YcCVCxIKrpZdarbtiS0NLaNqiT30ZsLLiQ0sf8uJDaX0oxTfBP0D8B3yacvIlfeuFw733zLm/8zu/cxhoNpshxliQUurnnPtd153odrsBPIuiqAuCkIsL8YwWi+VCijKDvq7jBHqc9+MBAP4oVvJ/meRog1IBOOdBxtiE4zgBULLxRCIxDaDEMAkhJDw3p+U+PDCTkyQtecJS8s4qCZumHYpEIlMZVdXBkCfd+fe1vwvWQrvZDMGIHQI+d4jCt7akPzvrhZ87l+KY/cfvuX3v862TejRYunMyv+zw1bB3+FvVIzwBBAKjt7gjVh+Qc+47f34t+sPppWMPGdVe7R6uP/nmShYBb936dO7Oduuz3etX65XKhow+3rq7+cn27g0AmLUs64O1Wi3qMjbxH8BWqyU+ukiNHpuXfr3AJnm7LfNGQ0A5dnZ2st1ud9Z1XZED9Zd/+VItPftJ5wD+KmPiCGMMiI+wEZ7n7fM8z8d7A8HLelkdaAoxNE3TkoQQBdlwAB+WinHD/R0gAmFwMBiclyQpkQMQJttv5FyDC6apJU1z0TjLqumNU4sGAEidTie+t7eXBwDRsqxItVpFlu9KRl2SyaQGAEsAUAaAeJywrJLbH1soFstzi0vL9eNWkdnmPABM3759I+26LgGAlKqqeqVSkZEhTsx4bLBknEFki2dWrYq2bYfwjgnx2/C7r9fr+dEGd3diVPIYEC844FuMSZlMRi8QorRYS0Qfaoc6ptNpwzTNJPpQR5DlScMw0pFIaopSKmDiMSBw7nuzsSG/Zev16sEj9MAh+8SZlfUCISTQe/HSvPfdQ5QjZAKE7HY71FsliZcrpYPPaqXo68t68Ss20LCPNWQB176oz968uXIEAAxV10tWoTCF/te/P37w6Ou7FwHgo1wqNQOet++3zaP5fzbpuesfv1e4fzp87NDCVGrMcAS4fP9qxupsLxgAIpZsVyoybTSE51dY9umlM3GYJmGMxXl1HCe8ynm4VluLAiQlSqmEevaxhoL2G4POkeDYmFGjcBIwGA0bNTgPdtd1sQdB9Pf/B7iaAMhSAoAwAESGFh1av4L/u/4FBpw+U6mFRw8AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Open Data Hub&quot;
        src=&quot;/static/9084423b13f90e1074daa3b83cfe247e/5a190/Open_Data_Hub_Deployment.png&quot;
        srcset=&quot;/static/9084423b13f90e1074daa3b83cfe247e/772e8/Open_Data_Hub_Deployment.png 200w,
/static/9084423b13f90e1074daa3b83cfe247e/e17e5/Open_Data_Hub_Deployment.png 400w,
/static/9084423b13f90e1074daa3b83cfe247e/5a190/Open_Data_Hub_Deployment.png 800w,
/static/9084423b13f90e1074daa3b83cfe247e/1cfc2/Open_Data_Hub_Deployment.png 900w&quot;
        sizes=&quot;(max-width: 800px) 100vw, 800px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Built on OpenShift, Open Data Hub uses the leading Kubernetes platform to deliver a meta-project that integrates Open Source components into a practical service-oriented AI and ML solution.  Organizations and IT departments can deploy Open Data Hub as a centralized self-service solution for analytic and data science workloads.&lt;/p&gt;
&lt;h2&gt;Data Science and ETL Tools&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 250px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/c49ed1ccb60c1d8f19c0c74ba36d1252/63868/ds_tools.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 27%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABfklEQVR42mOwKOzl3JjnK/4yS9VgSn6cXXRuPR8DAwPDokXd3D3Ld6tdulQheGZ3mtzUhes099fXszBAAGN9fT3TqtBQZhDnQoa42LU8cZ1buSrsDPPr4wUeFxrHvSnQzr+Vb565odSPF6SofvEtvj3zylPezdA6/miixd7lS6b5MNT/ZwLJ2Wtp8fxnYGAEsa+EMrC9KRQKvJcvVnMnW8KEwdTUWlvB2MGcQdFTTVnXxtjR2lob6gqGtdO6dd6Vqu991mEQnzTnOtgiG11dQWM9nWJjAx1XAwMdV3czTX9Pc42uaUFyaf/rBeUYohwt5ZdGGnhNDDWxXxNr5F7pay4O0vgwzlj5UoST+N1kdd89kwuMWpftEd+5cxG3rpycoLG+Xquxvm61oZ5Okr6ubra+vn6htYG2PtgVyYX1QnuLPDI/FWhUfCjQd4O57naCidXdeGPvh9HGbqt6e+1nzFvkO33uXC2QnJWVFa+xsTEXKCxDGRjA4VhvzwAOXwDNqHjc8mJVdQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Open Data Hub - Data Science&quot;
        src=&quot;/static/c49ed1ccb60c1d8f19c0c74ba36d1252/63868/ds_tools.png&quot;
        srcset=&quot;/static/c49ed1ccb60c1d8f19c0c74ba36d1252/772e8/ds_tools.png 200w,
/static/c49ed1ccb60c1d8f19c0c74ba36d1252/63868/ds_tools.png 250w&quot;
        sizes=&quot;(max-width: 250px) 100vw, 250px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Data scientists can use familiar tools such as Jupyter notebooks for developing complex algorithms and models.  Frameworks such as numpy, scikit-learn, Tensorflow and more are available for use.&lt;/p&gt;
&lt;p&gt;For data engineers who need to query or process their data, Spark from radanalytics.io is available for use.  If JDBC/ODBC access is required for data stored in S3, Spark Thrift Server comes pre-configured and ready to go.&lt;/p&gt;
&lt;h2&gt;Streaming and Enriching Data&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 480px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/4a4b5419eec3dcfe17bbc03415b29bc2/e85cb/streaming_tools.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 13.999999999999998%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAw0lEQVR42k2PzWrCQBSFPxutKC5sgwlhhEB+yErpRhATnMy4SWpBKdRuuhOk2+JK2r1v4Ht06dvJ6Cw8cPguF86BA9AFHOAR4P/0pU/H5RM3PQANy5alY+/m3a8hpWxKKXsmtAA2wBZ4+dtNzqt5+AqsHIc18AYo13UzIYT2PG/q+74SQozDMHzPsuw3iqLvOI4PaZouTaFp7Vi29p+i/PkYPgPGfbvguiIIgm6SJG3jqqraWmu/ruuRUmqW53lZFMXgAnCDGenJWR8RAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Open Data Hub - Streaming and Enrichment&quot;
        src=&quot;/static/4a4b5419eec3dcfe17bbc03415b29bc2/e85cb/streaming_tools.png&quot;
        srcset=&quot;/static/4a4b5419eec3dcfe17bbc03415b29bc2/772e8/streaming_tools.png 200w,
/static/4a4b5419eec3dcfe17bbc03415b29bc2/e17e5/streaming_tools.png 400w,
/static/4a4b5419eec3dcfe17bbc03415b29bc2/e85cb/streaming_tools.png 480w&quot;
        sizes=&quot;(max-width: 480px) 100vw, 480px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Open Data Hub offers numerous services for ingesting data, including Kafka, Logstash, Fluentd and rsyslog.  At the heart of it all is the strimzi.io Kafka streaming platform, which allows data to be ingested and processed at scale.  Kafka applications and logstash micro-services can be used to enrich your inbound data before it lands in storage.&lt;/p&gt;
&lt;h2&gt;Storing Data&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 338px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/19582ae0143b76115d5903f30b44cf57/2b41d/storage_tools.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 13.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA1UlEQVR42k2NzUoCYQBFj6bSSsQf0CgRlcCFiDCfunD8JtQZfwLRAVuk4BPkTrQE6xWc93AnuCpa6sp9u9yFC9/AKEbwwuWes7oc+qaCKAeK5WZcUfWbbKWRylereYD12HO7WwY3+230CbjwekkAQeDaXj8QAnzAFRDh68F8TmtGKyeNiZDGq9AaL4pmDJEfLlXEB7HR2/Hy83vqgRRudxfoAR1AB+5tbwI1QOPnsR0JCRkuSD1ZKNXTuTs9k5Hy7xEsK8ByMed9Zf77bOYEHHZPOWd+AZcSKeIKjgHgAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Open Data Hub - Storage&quot;
        src=&quot;/static/19582ae0143b76115d5903f30b44cf57/2b41d/storage_tools.png&quot;
        srcset=&quot;/static/19582ae0143b76115d5903f30b44cf57/772e8/storage_tools.png 200w,
/static/19582ae0143b76115d5903f30b44cf57/2b41d/storage_tools.png 338w&quot;
        sizes=&quot;(max-width: 338px) 100vw, 338px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Data is the foundational piece of AI, and Red Hat Ceph Object Store is ideal for building your lake of unstructured data.  Ceph’s S3 API makes integration with popular big data tools simple and easy.&lt;/p&gt;
&lt;p&gt;If log analysis is more what you need, Open Data Hub provides a deployment of Elasticsearch with Kibana for visualizations.&lt;/p&gt;
&lt;h2&gt;Managing Data&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 138px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/ac03c84dd54e825824a68090b3b0b7b3/948cf/dm_tools.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 48.55072463768116%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACJUlEQVR42mNgYGBgmLJqP8/ZB+9ir3z4X3vi6kNbkFh9fT0TAwQwMjAwsEBpEGaGYhaoPBMSm4Fh7c6jYrfefjn16sff/y+fvfjz8Pv//+cfvG4Fyf3//x9mCAOSgciAEYN9++23/g////9/1V/y42O67a9HG5f+vfP7///j5287gORl1dWluLi4Eri5ucV5eHhE+fn5nXl5eS05ODjCQfJ8fHwmHBwckSoqKuxgA+9+/Pv69pUr/59Mb/r7ri3z350V8349/fTj//XXPyeD5IUEBafy8vI28/DwdHNzc3fz8PDM5+bmnsnJydnAx8MzkYeHp5+bm7uDl5erBOLCdz8Ov338+P+Lrat+/5hR++/hqaO/X7/7+P/s8+85IHl+AYEVfIKCHvx8fEcFBQXnsbOzO3Jzc7dxcnIGsbOzn+Lj5e3k5eWN4+bm7gIbeOjJd+dHDx78flkb+f9yfuDvrysm/r/2+f+J7uJibpC8iIiUIScnZycfH1+2qKiou4iIiCQfH183JyfHZnZ29hZBQUEPkAtlZGSE4KF5fdWC9JdVEf+vZHn+f96Ze3f55ZfKaDEND3R+fn4Bbm7uVm5e3lnCwsImSDGNACdCbRXf10Y8+98R+/9+WdgUcAyvWsWMpBiWVMAaQREgJCTEhyaPiPGJKYEy50oiVtytCD+8My8sC2wgZhJBdikTEo0JQkNDmWe6GPMvjjbj6wg15mcgDLClSTAAACJowM0l6oIhAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Open Data Hub - Manage Data&quot;
        src=&quot;/static/ac03c84dd54e825824a68090b3b0b7b3/948cf/dm_tools.png&quot;
        srcset=&quot;/static/ac03c84dd54e825824a68090b3b0b7b3/948cf/dm_tools.png 138w&quot;
        sizes=&quot;(max-width: 138px) 100vw, 138px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Design your data pipelines the Kubernetes way using Argo.  Jobs can be created to train models or transform data.&lt;/p&gt;
&lt;h2&gt;Monitoring Infrastructure&lt;/h2&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 161px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/6dc446d2b783bbdff78736c44bf8273c/a5ccf/monitor_tools.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 41.61490683229814%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB2UlEQVR42m2RT2gTQRjFXzZLl8akf/AgtFJUrPZQd1O0sfESEQ8qFsTtbEwhiU1lNLMZQgihnjq96EHx0EOF4MmjAal3wV68qhc9ViGaxsS0UGx66GVksxtsoHN5M9/8eLx5AwA+AJBS+mqm8aJGpt7XzPCXLSvMOnPv/vASAsq2NTHZJBOJTXJmsIeThPgd/UWmMu24IRvsVng3Fc3vWbqsWpemO4yAcli34+dutubPr/9Jjq/UE2cXeg1jUB39e894VSfhT86+mro22pzTW+2EkXOZWIfZ8Nj9J2PL7eVT+erD05HW45O5DeHO4UIu3CBGrkn0r5vz0dvfrUiqQfQfP+PG1aMS7qydmN17M/yuvT70bPf18ZVuDd363A6FUOqmvrYVv/j0NzFeNswLS0d0OAxAkxJ++TkYO/gwSOXH/hH3DzyuUqn4C4XCKGMseIOvat/uXx9/+8iajHAxsMq5RikNeGZBALMAroRUzKgqYgDm+lVcdmYAZgCMgXM+wBhPZrPZlE0pT+fyd1ixmLTpgyXbtk3OeLJcLjumGoC7ABIArL4+5AMaFhUF3DkDKACIdp7spHQSCsaCpUwm9LxYPEapCKTT6aFSqRQSQiheyq768L+KHv0HPOmhZS3GwA8AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;alt text&quot;
        title=&quot;Open Data Hub - Monitor&quot;
        src=&quot;/static/6dc446d2b783bbdff78736c44bf8273c/a5ccf/monitor_tools.png&quot;
        srcset=&quot;/static/6dc446d2b783bbdff78736c44bf8273c/a5ccf/monitor_tools.png 161w&quot;
        sizes=&quot;(max-width: 161px) 100vw, 161px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Open Data Hub comes standard with Prometheus and Grafana for monitoring the components.  DevOps teams have predefined dashboards to help manage the infrastructure.&lt;/p&gt;
&lt;h2&gt;Community&lt;/h2&gt;
&lt;p&gt;We are an open community that fosters collaboration between organizations, vendors, developers, and academics.  &lt;a href=&quot;https://gitlab.com/opendatahub&quot;&gt;Join us and contribute&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;Tools&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Available in Open Data Hub&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://ceph.com/ceph-storage/object-storage/&quot;&gt;Ceph Object Storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark.apache.org/&quot;&gt;Spark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://jupyter.org/hub&quot;&gt;JuypyterHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Coming Soon to Open Data Hub&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kafka (&lt;a href=&quot;https://www.redhat.com/en/about/videos/summit-2018-introducing-amq-streams-data-streaming-apache-kafka&quot;&gt;AMQ Streams&lt;/a&gt; and &lt;a href=&quot;http://strimzi.io/&quot;&gt;Strimzi&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://docs.confluent.io/current/connect/index.html&quot;&gt;Kafka Connect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html&quot;&gt;Spark Thrift Server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/logstash&quot;&gt;Logstash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.fluentd.org/&quot;&gt;Fluentd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.rsyslog.com/&quot;&gt;Rsyslog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/kibana&quot;&gt;Kibana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://prometheus.io/&quot;&gt;Prometheus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://grafana.com/&quot;&gt;Grafana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://argoproj.github.io/&quot;&gt;Argo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Open Data Hub at DevConf.US 2018]]></title><description><![CDATA[The Open Data Hub team had several announcements and talks at DevConf.US 2018!  Watch the recorded sessions to learn more about the project…]]></description><link>https://www.opendatahub.io/blog/2018-11-28-open-data-hub-at-devconf-us/</link><guid isPermaLink="false">https://www.opendatahub.io/blog/2018-11-28-open-data-hub-at-devconf-us/</guid><pubDate>Wed, 28 Nov 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;The Open Data Hub team had several announcements and talks at DevConf.US 2018!  Watch the recorded sessions to learn more about the project.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://youtu.be/O16ST02CO0E?t=4308&quot;&gt;Open Data Hub in Mass Open Cloud&lt;/a&gt; - Get an overview the Open Data Hub and how academia can use it in the Mass Open Cloud for artificial intelligence and machine learning research.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://youtu.be/b1QgLBOqLKM?t=14m34s&quot;&gt;Enabling data exploration with JupyterHub on OpenShift&lt;/a&gt; - See JupyterHub in action in the Open Data Hub.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://youtu.be/b1QgLBOqLKM?t=9288&quot;&gt;Building AI with Ceph + OpenShift&lt;/a&gt; - Watch a demonstration of loading data, creating a model and deploying the model to a serverless framework in the Open Data Hub.&lt;/p&gt;</content:encoded></item></channel></rss>